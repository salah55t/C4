import os
import time
import logging
import psycopg2
import numpy as np
import pandas as pd
from decouple import config
from binance.client import Client
from psycopg2.extras import RealDictCursor
from scipy.signal import find_peaks
from sklearn.cluster import DBSCAN
from typing import List, Dict, Optional, Tuple

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (Logging) ----------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('sr_scanner.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('SR_Scanner')

# ---------------------- ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© ----------------------
# Ù‡Ø°Ø§ Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª ÙŠØ³ØªØ®Ø¯Ù… Ù†ÙØ³ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø¨ÙˆØª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
try:
    API_KEY: str = config('BINANCE_API_KEY')
    API_SECRET: str = config('BINANCE_API_SECRET')
    DB_URL: str = config('DATABASE_URL')
except Exception as e:
    logger.critical(f"âŒ ÙØ´Ù„ Ø­Ø§Ø³Ù… ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {e}")
    exit(1)

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø«ÙˆØ§Ø¨Øª ----------------------
# ÙƒÙ…ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… ØªØ­Ù„ÙŠÙ„Ù‡Ø§
DATA_FETCH_DAYS_4H = 200
DATA_FETCH_DAYS_15M = 30

# Ù…Ø¹Ø§ÙŠÙŠØ± ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚Ù…Ù… ÙˆØ§Ù„Ù‚ÙŠØ¹Ø§Ù† (ÙŠÙ…ÙƒÙ† ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©)
# prominence: Ù…Ø¯Ù‰ Ø¨Ø±ÙˆØ² Ø§Ù„Ù‚Ù…Ø©/Ø§Ù„Ù‚Ø§Ø¹ Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ù…Ø§ Ø­ÙˆÙ„Ù‡
# width: Ø¹Ø±Ø¶ Ø§Ù„Ù‚Ù…Ø©/Ø§Ù„Ù‚Ø§Ø¹
PROMINENCE_4H = 0.015  # 1.5%
WIDTH_4H = 5

PROMINENCE_15M = 0.008 # 0.8%
WIDTH_15M = 10

# Ù…Ø¹Ø§ÙŠÙŠØ± ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª (Clustering)
# eps: Ø§Ù„Ù…Ø³Ø§ÙØ© Ø§Ù„Ù‚ØµÙˆÙ‰ Ø¨ÙŠÙ† Ù†Ù‚Ø·ØªÙŠÙ† Ù„ÙŠØªÙ… Ø§Ø¹ØªØ¨Ø§Ø±Ù‡Ù…Ø§ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© (Ù†Ø³Ø¨Ø© Ù…Ø¦ÙˆÙŠØ© Ù…Ù† Ø§Ù„Ø³Ø¹Ø±)
CLUSTER_EPS_PERCENT = 0.005 # 0.5%

# ---------------------- Ø¯ÙˆØ§Ù„ Binance ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ----------------------
def get_binance_client() -> Optional[Client]:
    """ÙŠÙ‚ÙˆÙ… Ø¨ØªÙ‡ÙŠØ¦Ø© ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„ Ù…Ø¹ Binance."""
    try:
        client = Client(API_KEY, API_SECRET)
        client.ping()
        logger.info("âœ… [Binance] ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Binance Ø¨Ù†Ø¬Ø§Ø­.")
        return client
    except Exception as e:
        logger.critical(f"âŒ [Binance] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª: {e}")
        return None

def fetch_historical_data(client: Client, symbol: str, interval: str, days: int) -> Optional[pd.DataFrame]:
    """Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ø¹Ù…Ù„Ø© Ù…Ø¹ÙŠÙ†Ø©."""
    try:
        start_str = (pd.to_datetime('today') - pd.Timedelta(days=days)).strftime('%Y-%m-%d')
        logger.info(f"â³ [Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª {symbol} Ø¹Ù„Ù‰ ÙØ±ÙŠÙ… {interval} Ù„Ø¢Ø®Ø± {days} ÙŠÙˆÙ…...")
        klines = client.get_historical_klines(symbol, interval, start_str)
        if not klines:
            logger.warning(f"âš ï¸ [Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol} Ø¹Ù„Ù‰ ÙØ±ÙŠÙ… {interval}.")
            return None
        
        df = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_volume', 'trades', 'taker_buy_base', 'taker_buy_quote', 'ignore'])
        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        logger.info(f"âœ… [Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] ØªÙ… Ø¬Ù„Ø¨ {len(df)} Ø´Ù…Ø¹Ø© Ø¨Ù†Ø¬Ø§Ø­.")
        return df[numeric_cols].dropna()
    except Exception as e:
        logger.error(f"âŒ [Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ù€ {symbol}: {e}")
        return None

def get_validated_symbols(client: Client, filename: str = 'crypto_list.txt') -> List[str]:
    """Ù‚Ø±Ø§Ø¡Ø© Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Øª ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†Ù‡Ø§ Ù…Ø¹ Binance."""
    logger.info(f"â„¹ï¸ [Ø§Ù„ØªØ­Ù‚Ù‚] Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ù…Ù† '{filename}' ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†Ù‡Ø§...")
    try:
        script_dir = os.path.dirname(__file__)
        file_path = os.path.join(script_dir, filename)
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_symbols = {line.strip().upper() for line in f if line.strip() and not line.startswith('#')}
        
        formatted = {f"{s}USDT" if not s.endswith('USDT') else s for s in raw_symbols}
        exchange_info = client.get_exchange_info()
        active = {s['symbol'] for s in exchange_info['symbols'] if s.get('quoteAsset') == 'USDT' and s.get('status') == 'TRADING'}
        
        validated = sorted(list(formatted.intersection(active)))
        logger.info(f"âœ… [Ø§Ù„ØªØ­Ù‚Ù‚] Ø³ÙŠØªÙ… ØªØ­Ù„ÙŠÙ„ {len(validated)} Ø¹Ù…Ù„Ø© Ù…Ø¹ØªÙ…Ø¯Ø©.")
        return validated
    except Exception as e:
        logger.error(f"âŒ [Ø§Ù„ØªØ­Ù‚Ù‚] Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ²: {e}", exc_info=True)
        return []

# ---------------------- Ø¯ÙˆØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ----------------------
def init_db() -> Optional[psycopg2.extensions.connection]:
    """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹."""
    logger.info("[Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„...")
    try:
        conn = psycopg2.connect(DB_URL, connect_timeout=10, cursor_factory=RealDictCursor)
        with conn.cursor() as cur:
            cur.execute("""
                CREATE TABLE IF NOT EXISTS support_resistance_levels (
                    id SERIAL PRIMARY KEY,
                    symbol TEXT NOT NULL,
                    level_price DOUBLE PRECISION NOT NULL,
                    level_type TEXT NOT NULL, -- 'support' or 'resistance'
                    timeframe TEXT NOT NULL, -- '15m', '4h', etc.
                    strength INTEGER NOT NULL, -- Number of touches
                    last_tested_at TIMESTAMP,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                    CONSTRAINT unique_level UNIQUE (symbol, level_price, timeframe, level_type)
                );
            """)
        conn.commit()
        logger.info("âœ… [Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø¬Ø¯ÙˆÙ„ 'support_resistance_levels' Ø¨Ù†Ø¬Ø§Ø­.")
        return conn
    except Exception as e:
        logger.critical(f"âŒ [Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø£Ùˆ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„: {e}")
        return None

def save_levels_to_db(conn: psycopg2.extensions.connection, symbol: str, levels: List[Dict]):
    """Ø­ÙØ¸ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
    if not levels:
        logger.info(f"â„¹ï¸ [{symbol}] Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙˆÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„ÙŠØªÙ… Ø­ÙØ¸Ù‡Ø§.")
        return

    logger.info(f"â³ [{symbol}] Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ {len(levels)} Ù…Ø³ØªÙˆÙ‰ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    try:
        with conn.cursor() as cur:
            # Ø­Ø°Ù Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø£ÙˆÙ„Ø§Ù‹ Ù„Ø¶Ù…Ø§Ù† ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            cur.execute("DELETE FROM support_resistance_levels WHERE symbol = %s;", (symbol,))
            logger.info(f"ğŸ—‘ï¸ [{symbol}] ØªÙ… Ø­Ø°Ù Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©.")

            # Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
            insert_query = """
                INSERT INTO support_resistance_levels 
                (symbol, level_price, level_type, timeframe, strength, last_tested_at) 
                VALUES (%s, %s, %s, %s, %s, %s)
                ON CONFLICT (symbol, level_price, timeframe, level_type) DO NOTHING; 
            """
            for level in levels:
                cur.execute(insert_query, (
                    symbol,
                    level['level_price'],
                    level['level_type'],
                    level['timeframe'],
                    level['strength'],
                    level['last_tested_at']
                ))
        conn.commit()
        logger.info(f"âœ… [{symbol}] ØªÙ… Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        logger.error(f"âŒ [{symbol}] Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        conn.rollback()


# ---------------------- Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª ----------------------
def find_and_cluster_levels(df: pd.DataFrame, prominence: float, width: int, cluster_eps_percent: float) -> Tuple[List[Dict], List[Dict]]:
    """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚Ù…Ù… ÙˆØ§Ù„Ù‚ÙŠØ¹Ø§Ù† ÙˆØªØ¬Ù…ÙŠØ¹Ù‡Ø§ Ù„ØªØ­Ø¯ÙŠØ¯ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø©."""
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚ÙŠØ¹Ø§Ù† (Ø§Ù„Ø¯Ø¹ÙˆÙ…)
    lows = df['low'].to_numpy()
    low_peaks_indices, _ = find_peaks(-lows, prominence=lows.mean() * prominence, width=width)
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚Ù…Ù… (Ø§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø§Øª)
    highs = df['high'].to_numpy()
    high_peaks_indices, _ = find_peaks(highs, prominence=highs.mean() * prominence, width=width)

    def cluster_and_strengthen(prices: np.ndarray, indices: np.ndarray, level_type: str) -> List[Dict]:
        if len(indices) == 0:
            return []
        
        points = prices[indices].reshape(-1, 1)
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø§Ù„Ù‚ØµÙˆÙ‰ Ù„Ù„ØªØ¬Ù…ÙŠØ¹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø³Ø¹Ø±
        eps_value = points.mean() * cluster_eps_percent
        
        db = DBSCAN(eps=eps_value, min_samples=2, metric='euclidean').fit(points)
        
        clustered_levels = []
        unique_labels = set(db.labels_)
        
        for label in unique_labels:
            if label == -1: # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙŠ Ù„Ø§ ØªÙ†ØªÙ…ÙŠ Ù„Ø£ÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© (Noise)
                continue
            
            class_member_mask = (db.labels_ == label)
            cluster_points_indices = indices[class_member_mask]
            
            if len(cluster_points_indices) > 0:
                cluster_prices = prices[cluster_points_indices]
                mean_price = cluster_prices.mean()
                strength = len(cluster_prices)
                last_tested_timestamp = df.index[cluster_points_indices[-1]]
                
                clustered_levels.append({
                    "level_price": float(mean_price),
                    "level_type": level_type,
                    "strength": int(strength),
                    "last_tested_at": last_tested_timestamp.to_pydatetime()
                })
        
        return clustered_levels

    support_levels = cluster_and_strengthen(lows, low_peaks_indices, 'support')
    resistance_levels = cluster_and_strengthen(highs, high_peaks_indices, 'resistance')
    
    return support_levels, resistance_levels

# ---------------------- Ø­Ù„Ù‚Ø© Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ----------------------
def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª."""
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ù…Ø­Ù„Ù„ Ø§Ù„Ø¯Ø¹ÙˆÙ… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø§Øª...")
    
    client = get_binance_client()
    if not client:
        return
        
    conn = init_db()
    if not conn:
        return

    symbols_to_scan = get_validated_symbols(client)
    if not symbols_to_scan:
        logger.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù…Ù„Ø§Øª Ù„ØªØ­Ù„ÙŠÙ„Ù‡Ø§. Ø³ÙŠØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ´ØºÙŠÙ„.")
        return

    logger.info(f"ğŸŒ€ Ø³ÙŠØªÙ… ØªØ­Ù„ÙŠÙ„ {len(symbols_to_scan)} Ø¹Ù…Ù„Ø©. Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù‚Ø¯ ØªØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ø·ÙˆÙŠÙ„Ø§Ù‹.")

    for i, symbol in enumerate(symbols_to_scan):
        logger.info(f"--- ({i+1}/{len(symbols_to_scan)}) Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„Ø©: {symbol} ---")
        all_symbol_levels = []

        # --- ØªØ­Ù„ÙŠÙ„ ÙØ±ÙŠÙ… 4 Ø³Ø§Ø¹Ø§Øª ---
        df_4h = fetch_historical_data(client, symbol, '4h', DATA_FETCH_DAYS_4H)
        if df_4h is not None and not df_4h.empty:
            supports_4h, resistances_4h = find_and_cluster_levels(df_4h, PROMINENCE_4H, WIDTH_4H, CLUSTER_EPS_PERCENT)
            for level in supports_4h + resistances_4h:
                level['timeframe'] = '4h'
            all_symbol_levels.extend(supports_4h)
            all_symbol_levels.extend(resistances_4h)
            logger.info(f"ğŸ” [{symbol}-4h] ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(supports_4h)} Ù…Ø³ØªÙˆÙ‰ Ø¯Ø¹Ù… Ùˆ {len(resistances_4h)} Ù…Ø³ØªÙˆÙ‰ Ù…Ù‚Ø§ÙˆÙ…Ø©.")
        else:
            logger.warning(f"âš ï¸ [{symbol}-4h] ØªØ¹Ø°Ø± Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ ØªØ­Ù„ÙŠÙ„Ù‡Ø§.")
        
        time.sleep(1) # Ø§Ø³ØªØ±Ø§Ø­Ø© Ù‚ØµÙŠØ±Ø© Ù„ØªØ¬Ù†Ø¨ Ø¥ØºØ±Ø§Ù‚ Ø§Ù„Ù€ API

        # --- ØªØ­Ù„ÙŠÙ„ ÙØ±ÙŠÙ… 15 Ø¯Ù‚ÙŠÙ‚Ø© ---
        df_15m = fetch_historical_data(client, symbol, '15m', DATA_FETCH_DAYS_15M)
        if df_15m is not None and not df_15m.empty:
            supports_15m, resistances_15m = find_and_cluster_levels(df_15m, PROMINENCE_15M, WIDTH_15M, CLUSTER_EPS_PERCENT)
            for level in supports_15m + resistances_15m:
                level['timeframe'] = '15m'
            all_symbol_levels.extend(supports_15m)
            all_symbol_levels.extend(resistances_15m)
            logger.info(f"ğŸ” [{symbol}-15m] ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(supports_15m)} Ù…Ø³ØªÙˆÙ‰ Ø¯Ø¹Ù… Ùˆ {len(resistances_15m)} Ù…Ø³ØªÙˆÙ‰ Ù…Ù‚Ø§ÙˆÙ…Ø©.")
        else:
            logger.warning(f"âš ï¸ [{symbol}-15m] ØªØ¹Ø°Ø± Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ ØªØ­Ù„ÙŠÙ„Ù‡Ø§.")
            
        # Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ© Ù„Ù„Ø¹Ù…Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
        if all_symbol_levels:
            save_levels_to_db(conn, symbol, all_symbol_levels)
        
        logger.info(f"--- âœ… Ø§Ù†ØªÙ‡Ù‰ ØªØ­Ù„ÙŠÙ„ {symbol} ---")
        time.sleep(2) # Ø§Ø³ØªØ±Ø§Ø­Ø© Ø£Ø·ÙˆÙ„ Ø¨ÙŠÙ† Ø§Ù„Ø¹Ù…Ù„Ø§Øª

    conn.close()
    logger.info("ğŸ‰ğŸ‰ğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© ØªØ­Ù„ÙŠÙ„ ÙˆØ­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø¨Ù†Ø¬Ø§Ø­! ğŸ‰ğŸ‰ğŸ‰")


if __name__ == "__main__":
    main()
