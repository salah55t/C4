import os
import time
import logging
import psycopg2
import numpy as np
import pandas as pd
from decouple import config
from binance.client import Client
from psycopg2.extras import RealDictCursor
from scipy.signal import find_peaks
from sklearn.cluster import DBSCAN
from typing import List, Dict, Optional, Tuple
import threading
import http.server
import socketserver

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (Logging) ----------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('sr_scanner_v3.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('SR_Scanner_V3')

# ---------------------- ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© ----------------------
try:
    API_KEY: str = config('BINANCE_API_KEY')
    API_SECRET: str = config('BINANCE_API_SECRET')
    DB_URL: str = config('DATABASE_URL')
except Exception as e:
    logger.critical(f"âŒ ÙØ´Ù„ Ø­Ø§Ø³Ù… ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {e}")
    exit(1)

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø«ÙˆØ§Ø¨Øª ----------------------
ANALYSIS_INTERVAL_HOURS = 4  # Ø§Ù„ÙØ§ØµÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ Ø¨ÙŠÙ† ÙƒÙ„ Ø¯ÙˆØ±Ø© ØªØ­Ù„ÙŠÙ„ (Ø¨Ø§Ù„Ø³Ø§Ø¹Ø§Øª)

# ÙƒÙ…ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
DATA_FETCH_DAYS_1D = 600
DATA_FETCH_DAYS_4H = 200
DATA_FETCH_DAYS_15M = 30

# Ù…Ø¹Ø§ÙŠÙŠØ± ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚Ù…Ù… ÙˆØ§Ù„Ù‚ÙŠØ¹Ø§Ù†
PROMINENCE_1D = 0.025
WIDTH_1D = 10
PROMINENCE_4H = 0.015
WIDTH_4H = 5
PROMINENCE_15M = 0.008
WIDTH_15M = 10

# Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªØ¬Ù…ÙŠØ¹ ÙˆØ§Ù„Ø¯Ù…Ø¬
CLUSTER_EPS_PERCENT = 0.005
CONFLUENCE_ZONE_PERCENT = 0.005

# Ù…Ø¹Ø§ÙŠÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø¨Ø±ÙˆÙØ§ÙŠÙ„ Ø§Ù„Ø­Ø¬Ù…
VOLUME_PROFILE_BINS = 100

# ---------------------- Ù‚Ø³Ù… Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨ (Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ù…Ù†ØµØ©) ----------------------
class WebServerHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        """Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ø·Ù„Ø¨Ø§Øª GET Ø¨ØµÙØ­Ø© HTML Ø¨Ø³ÙŠØ·Ø©."""
        self.send_response(200)
        self.send_header("Content-type", "text/html; charset=utf-8")
        self.end_headers()
        html_content = """
        <!DOCTYPE html>
        <html lang="ar" dir="rtl">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø§Ø³Ø­</title>
            <style>
                body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background-color: #f4f4f9; color: #333; display: flex; justify-content: center; align-items: center; height: 100vh; margin: 0; }
                .container { text-align: center; padding: 40px; background-color: white; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); }
                h1 { color: #0056b3; }
                p { font-size: 1.2rem; }
                .status { font-weight: bold; color: #28a745; }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>ğŸ“Š Ù…Ø§Ø³Ø­ Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø©</h1>
                <p>Ø§Ù„Ø®Ø¯Ù…Ø© <span class="status">ØªØ¹Ù…Ù„</span> ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©.</p>
                <p>ÙŠØªÙ… Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ.</p>
            </div>
        </body>
        </html>
        """
        self.wfile.write(html_content.encode('utf-8'))

def run_web_server():
    """ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ÙØ° Ø§Ù„Ù…Ø­Ø¯Ø¯ Ù…Ù† Ù‚Ø¨Ù„ Ø§Ù„Ù…Ù†ØµØ©."""
    PORT = int(os.environ.get("PORT", 8080))
    with socketserver.TCPServer(("", PORT), WebServerHandler) as httpd:
        logger.info(f"ğŸŒ Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨ ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ÙØ° {PORT}")
        httpd.serve_forever()

# ---------------------- Ø¯ÙˆØ§Ù„ Binance ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ----------------------
def get_binance_client() -> Optional[Client]:
    """ÙŠÙ‚ÙˆÙ… Ø¨ØªÙ‡ÙŠØ¦Ø© ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„ Ù…Ø¹ Binance."""
    try:
        client = Client(API_KEY, API_SECRET)
        client.ping()
        logger.info("âœ… [Binance] ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Binance Ø¨Ù†Ø¬Ø§Ø­.")
        return client
    except Exception as e:
        logger.critical(f"âŒ [Binance] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª: {e}")
        return None

def fetch_historical_data(client: Client, symbol: str, interval: str, days: int) -> Optional[pd.DataFrame]:
    """Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ø¹Ù…Ù„Ø© Ù…Ø¹ÙŠÙ†Ø©."""
    try:
        start_str = (pd.to_datetime('today') - pd.Timedelta(days=days)).strftime('%Y-%m-%d')
        logger.info(f"â³ [Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª {symbol} Ø¹Ù„Ù‰ ÙØ±ÙŠÙ… {interval} Ù„Ø¢Ø®Ø± {days} ÙŠÙˆÙ…...")
        klines = client.get_historical_klines(symbol, interval, start_str)
        if not klines:
            logger.warning(f"âš ï¸ [Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol} Ø¹Ù„Ù‰ ÙØ±ÙŠÙ… {interval}.")
            return None
        
        df = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_volume', 'trades', 'taker_buy_base', 'taker_buy_quote', 'ignore'])
        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        logger.info(f"âœ… [Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] ØªÙ… Ø¬Ù„Ø¨ {len(df)} Ø´Ù…Ø¹Ø© Ø¨Ù†Ø¬Ø§Ø­.")
        return df[numeric_cols].dropna()
    except Exception as e:
        logger.error(f"âŒ [Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ù€ {symbol}: {e}")
        return None

def get_validated_symbols(client: Client, filename: str = 'crypto_list.txt') -> List[str]:
    """Ù‚Ø±Ø§Ø¡Ø© Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Øª ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†Ù‡Ø§ Ù…Ø¹ Binance."""
    logger.info(f"â„¹ï¸ [Ø§Ù„ØªØ­Ù‚Ù‚] Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ù…Ù† '{filename}' ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†Ù‡Ø§...")
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, filename)
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_symbols = {line.strip().upper() for line in f if line.strip() and not line.startswith('#')}
        
        formatted = {f"{s}USDT" if not s.endswith('USDT') else s for s in raw_symbols}
        exchange_info = client.get_exchange_info()
        active = {s['symbol'] for s in exchange_info['symbols'] if s.get('quoteAsset') == 'USDT' and s.get('status') == 'TRADING'}
        
        validated = sorted(list(formatted.intersection(active)))
        logger.info(f"âœ… [Ø§Ù„ØªØ­Ù‚Ù‚] Ø³ÙŠØªÙ… ØªØ­Ù„ÙŠÙ„ {len(validated)} Ø¹Ù…Ù„Ø© Ù…Ø¹ØªÙ…Ø¯Ø©.")
        return validated
    except Exception as e:
        logger.error(f"âŒ [Ø§Ù„ØªØ­Ù‚Ù‚] Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ²: {e}", exc_info=True)
        return []

# ---------------------- Ø¯ÙˆØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ----------------------
def init_db() -> Optional[psycopg2.extensions.connection]:
    """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØ­Ø¯ÙŠØ« Ø¨Ù†ÙŠØ© Ø§Ù„Ø¬Ø¯ÙˆÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§."""
    logger.info("[Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„...")
    conn = None
    try:
        conn = psycopg2.connect(DB_URL, connect_timeout=10, cursor_factory=RealDictCursor)
        with conn.cursor() as cur:
            # Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†ÙˆØ¹ NUMERIC Ù„Ù„Ù‚ÙˆØ© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
            cur.execute("""
                CREATE TABLE IF NOT EXISTS support_resistance_levels (
                    id SERIAL PRIMARY KEY,
                    symbol TEXT NOT NULL,
                    level_price DOUBLE PRECISION NOT NULL,
                    level_type TEXT NOT NULL,
                    timeframe TEXT NOT NULL,
                    strength NUMERIC NOT NULL, -- Ø§Ø³ØªØ®Ø¯Ø§Ù… NUMERIC Ù„Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
                    last_tested_at TIMESTAMP,
                    details TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                    CONSTRAINT unique_level UNIQUE (symbol, level_price, timeframe, level_type)
                );
            """)
            conn.commit()

            # Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¹Ù…ÙˆØ¯ 'details' ÙˆØ¥Ø¶Ø§ÙØªÙ‡ (Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©)
            cur.execute("""
                SELECT 1 FROM information_schema.columns 
                WHERE table_name='support_resistance_levels' AND column_name='details';
            """)
            if cur.fetchone() is None:
                logger.info("[Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] Ø§Ù„Ø¹Ù…ÙˆØ¯ 'details' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ø¬Ø§Ø±ÙŠ Ø¥Ø¶Ø§ÙØªÙ‡...")
                cur.execute("ALTER TABLE support_resistance_levels ADD COLUMN details TEXT;")
                conn.commit()
                logger.info("âœ… [Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¹Ù…ÙˆØ¯ 'details' Ø¨Ù†Ø¬Ø§Ø­.")

            # ---- Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ----
            # Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†ÙˆØ¹ Ø¹Ù…ÙˆØ¯ 'strength' ÙˆØªØ­Ø¯ÙŠØ«Ù‡ Ø¥Ù„Ù‰ NUMERIC Ø¥Ø°Ø§ ÙƒØ§Ù† integer Ø£Ùˆ bigint
            cur.execute("""
                SELECT data_type FROM information_schema.columns 
                WHERE table_name = 'support_resistance_levels' AND column_name = 'strength';
            """)
            result = cur.fetchone()
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†ÙˆØ¹ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù‡Ùˆ Ø£Ø­Ø¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµØ­ÙŠØ­Ø© Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªØ³Ø¨Ø¨ Ø§Ù„Ø®Ø·Ø£
            if result and result['data_type'] in ('bigint', 'integer'):
                logger.info(f"[Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] Ø§Ù„Ø¹Ù…ÙˆØ¯ 'strength' Ù…Ù† Ù†ÙˆØ¹ {result['data_type']}. Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¥Ù„Ù‰ NUMERIC...")
                cur.execute("ALTER TABLE support_resistance_levels ALTER COLUMN strength TYPE NUMERIC USING strength::numeric;")
                conn.commit()
                logger.info("âœ… [Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] ØªÙ… ØªØ­Ø¯ÙŠØ« Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù…ÙˆØ¯ 'strength' Ø¥Ù„Ù‰ NUMERIC Ø¨Ù†Ø¬Ø§Ø­.")

        logger.info("âœ… [Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] ØªÙ… ØªÙ‡ÙŠØ¦Ø© ÙˆØªØ­Ø¯ÙŠØ« Ø¬Ø¯ÙˆÙ„ 'support_resistance_levels' Ø¨Ù†Ø¬Ø§Ø­.")
        return conn
    except Exception as e:
        logger.critical(f"âŒ [Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø£Ùˆ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„: {e}")
        if conn:
            conn.rollback()
        return None

def save_levels_to_db(conn: psycopg2.extensions.connection, symbol: str, levels: List[Dict]):
    """Ø­ÙØ¸ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
    if not levels:
        logger.info(f"â„¹ï¸ [{symbol}] Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙˆÙŠØ§Øª Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„ÙŠØªÙ… Ø­ÙØ¸Ù‡Ø§.")
        return
    logger.info(f"â³ [{symbol}] Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ {len(levels)} Ù…Ø³ØªÙˆÙ‰ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    try:
        with conn.cursor() as cur:
            # ØªÙ… Ø­Ø°Ù Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù„Ù„Ø¹Ù…Ù„Ø© Ù„Ø¶Ù…Ø§Ù† ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            cur.execute("DELETE FROM support_resistance_levels WHERE symbol = %s;", (symbol,))
            insert_query = """
                INSERT INTO support_resistance_levels 
                (symbol, level_price, level_type, timeframe, strength, last_tested_at, details) 
                VALUES (%s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (symbol, level_price, timeframe, level_type) DO NOTHING; 
            """
            for level in levels:
                cur.execute(insert_query, (
                    symbol, level.get('level_price'), level.get('level_type'),
                    level.get('timeframe'), level.get('strength'),
                    level.get('last_tested_at'), level.get('details')
                ))
        conn.commit()
        logger.info(f"âœ… [{symbol}] ØªÙ… Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        logger.error(f"âŒ [{symbol}] Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        conn.rollback()

# ---------------------- Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª ----------------------
def find_price_action_levels(df: pd.DataFrame, prominence: float, width: int, cluster_eps_percent: float) -> List[Dict]:
    """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚Ù…Ù… ÙˆØ§Ù„Ù‚ÙŠØ¹Ø§Ù† ÙˆØªØ¬Ù…ÙŠØ¹Ù‡Ø§ Ù„ØªØ­Ø¯ÙŠØ¯ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø©."""
    lows = df['low'].to_numpy()
    highs = df['high'].to_numpy()
    low_peaks_indices, _ = find_peaks(-lows, prominence=lows.mean() * prominence, width=width)
    high_peaks_indices, _ = find_peaks(highs, prominence=highs.mean() * prominence, width=width)

    def cluster_and_strengthen(prices: np.ndarray, indices: np.ndarray, level_type: str) -> List[Dict]:
        if len(indices) < 2: return []
        points = prices[indices].reshape(-1, 1)
        eps_value = points.mean() * cluster_eps_percent
        db = DBSCAN(eps=eps_value, min_samples=2).fit(points)
        clustered_levels = []
        for label in set(db.labels_):
            if label != -1:
                mask = (db.labels_ == label)
                cluster_indices = indices[mask]
                clustered_levels.append({
                    "level_price": float(prices[cluster_indices].mean()),
                    "level_type": level_type,
                    "strength": int(len(cluster_indices)),
                    "last_tested_at": df.index[cluster_indices[-1]].to_pydatetime()
                })
        return clustered_levels

    support_levels = cluster_and_strengthen(lows, low_peaks_indices, 'support')
    resistance_levels = cluster_and_strengthen(highs, high_peaks_indices, 'resistance')
    return support_levels + resistance_levels

def analyze_volume_profile(df: pd.DataFrame, bins: int) -> List[Dict]:
    """ØªØ­Ù„ÙŠÙ„ Ø¨Ø±ÙˆÙØ§ÙŠÙ„ Ø§Ù„Ø­Ø¬Ù… Ù„ØªØ­Ø¯ÙŠØ¯ Ù†Ù‚Ø·Ø© Ø§Ù„ØªØ­ÙƒÙ… (POC)."""
    price_min, price_max = df['low'].min(), df['high'].max()
    if price_min >= price_max:
        logger.warning("[Volume Profile] Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø³Ø¹Ø±ÙŠ ØºÙŠØ± ØµØ§Ù„Ø­. ÙŠØªÙ… Ø§Ù„ØªØ®Ø·ÙŠ.")
        return []

    price_bins = np.linspace(price_min, price_max, bins + 1)
    bin_centers = (price_bins[:-1] + price_bins[1:]) / 2
    volume_by_bin = np.zeros(bins)

    for _, row in df.iterrows():
        low_idx = np.searchsorted(price_bins, row['low'], side='right') - 1
        high_idx = np.searchsorted(price_bins, row['high'], side='left')
        
        low_idx = max(0, low_idx)
        high_idx = min(bins, high_idx)

        if high_idx > low_idx:
            num_bins_spanned = high_idx - low_idx
            volume_per_bin = row['volume'] / num_bins_spanned if num_bins_spanned > 0 else row['volume']
            for i in range(low_idx, high_idx):
                 volume_by_bin[i] += volume_per_bin

    if np.sum(volume_by_bin) == 0:
        logger.warning("[Volume Profile] Ù„Ù… ÙŠØªÙ… Ø­Ø³Ø§Ø¨ Ø£ÙŠ Ø­Ø¬Ù….")
        return []
    
    poc_index = np.argmax(volume_by_bin)
    poc_price = bin_centers[poc_index]
    poc_volume = volume_by_bin[poc_index]
    
    # ---- Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ----
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… float Ù„Ù„Ù‚ÙˆØ© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø¬Ù… Ù„ØªØ¬Ù†Ø¨ ØªØ¬Ø§ÙˆØ² Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµØ­ÙŠØ­Ø©
    return [{"level_price": float(poc_price), "level_type": 'poc', "strength": float(poc_volume), "last_tested_at": None}]


def find_confluence_zones(levels: List[Dict], confluence_percent: float) -> Tuple[List[Dict], List[Dict]]:
    """ØªØ­Ø¯ÙŠØ¯ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØªÙˆØ§ÙÙ‚ (Confluence) Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…ØªÙ‚Ø§Ø±Ø¨Ø©."""
    if not levels: return [], []
    levels.sort(key=lambda x: x['level_price'])
    
    tf_weights = {'1d': 3, '4h': 2, '15m': 1}
    type_weights = {'poc': 2.5, 'support': 1.5, 'resistance': 1.5, 'hvn': 1, 'confluence': 4}

    confluence_zones = []
    used_indices = set()
    
    for i in range(len(levels)):
        if i in used_indices: continue
        
        current_zone_levels = [levels[i]]
        current_zone_indices = {i}
        
        for j in range(i + 1, len(levels)):
            if j in used_indices: continue
            
            price_i = levels[i]['level_price']
            price_j = levels[j]['level_price']

            if (abs(price_j - price_i) / price_i) <= confluence_percent:
                current_zone_levels.append(levels[j])
                current_zone_indices.add(j)

        if len(current_zone_levels) > 1:
            used_indices.update(current_zone_indices)
            
            avg_price = sum(l['level_price'] * l['strength'] for l in current_zone_levels) / sum(l['strength'] for l in current_zone_levels)
            total_strength = 0
            for l in current_zone_levels:
                tf_w = tf_weights.get(l['timeframe'], 1)
                type_w = type_weights.get(l['level_type'], 1)
                total_strength += l['strength'] * tf_w * type_w

            timeframes = sorted(list(set(l['timeframe'] for l in current_zone_levels)))
            details = sorted(list(set(l['level_type'] for l in current_zone_levels)))
            last_tested = max((l['last_tested_at'] for l in current_zone_levels if l['last_tested_at']), default=None)

            confluence_zones.append({
                "level_price": avg_price,
                "level_type": 'confluence',
                # ---- Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ----
                # Ø¶Ù…Ø§Ù† Ø£Ù† Ø§Ù„Ù‚ÙˆØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù‡ÙŠ float Ù„ØªÙƒÙˆÙ† Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø®Ø±Ù‰
                "strength": float(total_strength), 
                "timeframe": ",".join(timeframes),
                "details": ",".join(details),
                "last_tested_at": last_tested
            })

    remaining_levels = [level for i, level in enumerate(levels) if i not in used_indices]
    logger.info(f"ğŸ¤ [Confluence] ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(confluence_zones)} Ù…Ù†Ø·Ù‚Ø© ØªÙˆØ§ÙÙ‚ Ùˆ {len(remaining_levels)} Ù…Ø³ØªÙˆÙ‰ ÙØ±Ø¯ÙŠ Ù…ØªØ¨Ù‚ÙŠ.")
    return confluence_zones, remaining_levels

# ---------------------- Ø­Ù„Ù‚Ø© Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„ ----------------------
def run_full_analysis():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ù†Ø·Ù‚ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„."""
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ù…Ø­Ù„Ù„ Ø§Ù„Ø¯Ø¹ÙˆÙ… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø§Øª...")
    
    client = get_binance_client()
    if not client: return
        
    conn = init_db()
    if not conn: return

    symbols_to_scan = get_validated_symbols(client, 'crypto_list.txt')
    if not symbols_to_scan:
        logger.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù…Ù„Ø§Øª Ù„ØªØ­Ù„ÙŠÙ„Ù‡Ø§. Ø³ÙŠØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ´ØºÙŠÙ„.")
        conn.close()
        return

    logger.info(f"ğŸŒ€ Ø³ÙŠØªÙ… ØªØ­Ù„ÙŠÙ„ {len(symbols_to_scan)} Ø¹Ù…Ù„Ø©.")

    timeframes_config = {
        '1d':  {'days': DATA_FETCH_DAYS_1D,  'prominence': PROMINENCE_1D,  'width': WIDTH_1D},
        '4h':  {'days': DATA_FETCH_DAYS_4H,  'prominence': PROMINENCE_4H,  'width': WIDTH_4H},
        '15m': {'days': DATA_FETCH_DAYS_15M, 'prominence': PROMINENCE_15M, 'width': WIDTH_15M}
    }

    for i, symbol in enumerate(symbols_to_scan):
        logger.info(f"--- ({i+1}/{len(symbols_to_scan)}) Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„Ø©: {symbol} ---")
        raw_levels = []

        for tf, config in timeframes_config.items():
            df = fetch_historical_data(client, symbol, tf, config['days'])
            if df is not None and not df.empty:
                pa_levels = find_price_action_levels(df, config['prominence'], config['width'], CLUSTER_EPS_PERCENT)
                vol_levels = analyze_volume_profile(df, bins=VOLUME_PROFILE_BINS)
                
                for level in pa_levels + vol_levels:
                    level['timeframe'] = tf
                raw_levels.extend(pa_levels + vol_levels)
            else:
                logger.warning(f"âš ï¸ [{symbol}-{tf}] ØªØ¹Ø°Ø± Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
            time.sleep(1) 
            
        if raw_levels:
            confluence_zones, remaining_singles = find_confluence_zones(raw_levels, CONFLUENCE_ZONE_PERCENT)
            final_levels = confluence_zones + remaining_singles
            save_levels_to_db(conn, symbol, final_levels)
        else:
            logger.info(f"â„¹ï¸ [{symbol}] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ø³ØªÙˆÙŠØ§Øª Ø£ÙˆÙ„ÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„Ù‡Ø§.")
        
        logger.info(f"--- âœ… Ø§Ù†ØªÙ‡Ù‰ ØªØ­Ù„ÙŠÙ„ {symbol} ---")
        time.sleep(2)

    conn.close()
    logger.info("ğŸ‰ğŸ‰ğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Øª! ğŸ‰ğŸ‰ğŸ‰")

def analysis_scheduler():
    """ØªÙ‚ÙˆÙ… Ø¨Ø¬Ø¯ÙˆÙ„Ø© ÙˆØªØ´ØºÙŠÙ„ Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ."""
    while True:
        try:
            run_full_analysis()
        except Exception as e:
            logger.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ ÙÙŠ Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {e}", exc_info=True)
        
        sleep_duration_seconds = ANALYSIS_INTERVAL_HOURS * 60 * 60
        logger.info(f"ğŸ‘ Ø§ÙƒØªÙ…Ù„Øª Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„. Ø³ÙŠØªÙ… Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù…Ø¯Ø© {ANALYSIS_INTERVAL_HOURS} Ø³Ø§Ø¹Ø§Øª.")
        time.sleep(sleep_duration_seconds)

# ---------------------- Ù†Ù‚Ø·Ø© Ø§Ù†Ø·Ù„Ø§Ù‚ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ----------------------
if __name__ == "__main__":
    # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ´ØºÙŠÙ„ Ø®ÙŠØ· Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨
    web_server_thread = threading.Thread(target=run_web_server)
    web_server_thread.daemon = True
    web_server_thread.start()

    # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ´ØºÙŠÙ„ Ø®ÙŠØ· Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„
    analysis_thread = threading.Thread(target=analysis_scheduler)
    analysis_thread.daemon = True
    analysis_thread.start()

    # Ø¥Ø¨Ù‚Ø§Ø¡ Ø§Ù„Ø®ÙŠØ· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø­ÙŠÙ‹Ø§ Ù„Ù„Ø³Ù…Ø§Ø­ Ù„Ù„Ø®ÙŠÙˆØ· Ø§Ù„Ø£Ø®Ø±Ù‰ Ø¨Ø§Ù„Ø¹Ù…Ù„
    try:
        while True:
            time.sleep(3600)
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ØªÙ… Ø·Ù„Ø¨ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬. ÙˆØ¯Ø§Ø¹Ø§Ù‹!")
