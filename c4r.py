import os
import time
import logging
import psycopg2
import numpy as np
import pandas as pd
import datetime as dt
from decouple import config
from binance.client import Client
from psycopg2.extras import RealDictCursor, execute_values
from scipy.signal import find_peaks
from sklearn.cluster import DBSCAN
from typing import List, Dict, Optional, Tuple
import threading
import http.server
import socketserver
from concurrent.futures import ThreadPoolExecutor, as_completed

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (Logging) ----------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('sr_scanner_scalping_edition.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('SR_Scanner_Scalping')

# ---------------------- ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© ----------------------
try:
    API_KEY: str = config('BINANCE_API_KEY')
    API_SECRET: str = config('BINANCE_API_SECRET')
    DB_URL: str = config('DATABASE_URL')
except Exception as e:
    logger.critical(f"âŒ ÙØ´Ù„ Ø­Ø§Ø³Ù… ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {e}")
    exit(1)

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø«ÙˆØ§Ø¨Øª (Ù†Ø³Ø®Ø© Ø§Ù„Ø³ÙƒØ§Ù„Ø¨ÙŠÙ†Ø¬) ----------------------
ANALYSIS_INTERVAL_MINUTES = 15
MAX_WORKERS = 10
API_RETRY_ATTEMPTS = 3
API_RETRY_DELAY = 5

DATA_FETCH_DAYS_1H = 30
DATA_FETCH_DAYS_15M = 7
DATA_FETCH_DAYS_5M = 3

ATR_PROMINENCE_MULTIPLIER_1H = 0.8
ATR_PROMINENCE_MULTIPLIER_15M = 0.6
ATR_PROMINENCE_MULTIPLIER_5M = 0.5
ATR_PERIOD = 14
ATR_SHORT_PERIOD = 7
ATR_LONG_PERIOD = 28

WIDTH_1H = 8
WIDTH_15M = 5
WIDTH_5M = 3

VOLUME_CONFIRMATION_ENABLED = True
VOLUME_AVG_PERIOD = 20
VOLUME_SPIKE_FACTOR = 1.6

CLUSTER_EPS_PERCENT = 0.0015
CONFLUENCE_ZONE_PERCENT = 0.002
VOLUME_PROFILE_BINS = 100

# ---------------------- Ù‚Ø³Ù… Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨ ----------------------
class WebServerHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.send_header("Content-type", "text/html; charset=utf-8")
        self.end_headers()
        html_content = """
        <!DOCTYPE html><html lang="ar" dir="rtl"><head><meta charset="UTF-8"><title>Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø§Ø³Ø­</title>
        <style>body{font-family: 'Segoe UI', sans-serif; background-color: #1a1a1a; color: #f0f0f0; display: flex; justify-content: center; align-items: center; height: 100vh; margin: 0;} .container{text-align: center; padding: 40px; background-color: #2b2b2b; border-radius: 10px; box-shadow: 0 4px 15px rgba(0,0,0,0.5); border: 1px solid #00aaff;} h1{color: #00aaff;} .status{font-weight: bold; color: #28a745;}</style>
        </head><body><div class="container"><h1>âš¡ï¸ Ù…Ø§Ø³Ø­ Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© - Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø³ÙƒØ§Ù„Ø¨ÙŠÙ†Ø¬ Ù…Ø¹ ÙÙŠØ¨ÙˆÙ†Ø§ØªØ´ÙŠ âš¡ï¸</h1><h2>(ÙÙŠØ¨ÙˆÙ†Ø§ØªØ´ÙŠ Ø¹Ù„Ù‰ ÙØ±ÙŠÙ… 15 Ø¯Ù‚ÙŠÙ‚Ø©)</h2><p>Ø§Ù„Ø®Ø¯Ù…Ø© <span class="status">ØªØ¹Ù…Ù„</span>.</p><p>ÙŠØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ« ÙƒÙ„ 15 Ø¯Ù‚ÙŠÙ‚Ø©.</p></div></body></html>
        """
        self.wfile.write(html_content.encode('utf-8'))

def run_web_server():
    PORT = int(os.environ.get("PORT", 8080))
    with socketserver.TCPServer(("", PORT), WebServerHandler) as httpd:
        logger.info(f"ğŸŒ Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨ ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ÙØ° {PORT}")
        httpd.serve_forever()

# ---------------------- Ø¯ÙˆØ§Ù„ Binance ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ----------------------
def get_binance_client() -> Optional[Client]:
    try:
        client = Client(API_KEY, API_SECRET)
        client.ping()
        logger.info("âœ… [Binance] ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Binance Ø¨Ù†Ø¬Ø§Ø­.")
        return client
    except Exception as e:
        logger.critical(f"âŒ [Binance] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª: {e}")
        return None

def fetch_historical_data_with_retry(client: Client, symbol: str, interval: str, days: int) -> Optional[pd.DataFrame]:
    for attempt in range(API_RETRY_ATTEMPTS):
        try:
            start_str = (pd.to_datetime('today') - pd.Timedelta(days=days)).strftime('%Y-%m-%d')
            klines = client.get_historical_klines(symbol, interval, start_str)
            if not klines:
                logger.warning(f"âš ï¸ [{symbol}] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ ÙØ±ÙŠÙ… {interval}.")
                return None
            df = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_volume', 'trades', 'taker_buy_base', 'taker_buy_quote', 'ignore'])
            numeric_cols = ['open', 'high', 'low', 'close', 'volume']
            for col in numeric_cols: df[col] = pd.to_numeric(df[col], errors='coerce')
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms').dt.tz_localize('UTC')
            df.set_index('timestamp', inplace=True)
            return df[numeric_cols].dropna()
        except Exception as e:
            logger.error(f"âŒ [{symbol}] Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}/{API_RETRY_ATTEMPTS}): {e}")
            if attempt < API_RETRY_ATTEMPTS - 1: time.sleep(API_RETRY_DELAY)
    logger.critical(f"âŒ [{symbol}] ÙØ´Ù„ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ {API_RETRY_ATTEMPTS} Ù…Ø­Ø§ÙˆÙ„Ø§Øª.")
    return None

def get_validated_symbols(client: Client, filename: str = 'crypto_list.txt') -> List[str]:
    logger.info(f"â„¹ï¸ [Ø§Ù„ØªØ­Ù‚Ù‚] Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ù…Ù† '{filename}' ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†Ù‡Ø§...")
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, filename)
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_symbols = {line.strip().upper() for line in f if line.strip() and not line.startswith('#')}
        formatted = {f"{s}USDT" if not s.endswith('USDT') else s for s in raw_symbols}
        exchange_info = client.get_exchange_info()
        active = {s['symbol'] for s in exchange_info['symbols'] if s.get('quoteAsset') == 'USDT' and s.get('status') == 'TRADING'}
        validated = sorted(list(formatted.intersection(active)))
        logger.info(f"âœ… [Ø§Ù„ØªØ­Ù‚Ù‚] Ø³ÙŠØªÙ… ØªØ­Ù„ÙŠÙ„ {len(validated)} Ø¹Ù…Ù„Ø© Ù…Ø¹ØªÙ…Ø¯Ø©.")
        return validated
    except Exception as e:
        logger.error(f"âŒ [Ø§Ù„ØªØ­Ù‚Ù‚] Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ²: {e}", exc_info=True)
        return []

# ---------------------- Ø¯ÙˆØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ----------------------
def init_db() -> Optional[psycopg2.extensions.connection]:
    logger.info("[Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„...")
    conn = None
    try:
        conn = psycopg2.connect(DB_URL, connect_timeout=10, cursor_factory=RealDictCursor)
        with conn.cursor() as cur:
            cur.execute("""
                CREATE TABLE IF NOT EXISTS support_resistance_levels (
                    id SERIAL PRIMARY KEY,
                    symbol TEXT NOT NULL,
                    level_price DOUBLE PRECISION NOT NULL,
                    level_type TEXT NOT NULL,
                    timeframe TEXT NOT NULL,
                    strength NUMERIC NOT NULL,
                    score NUMERIC DEFAULT 0,
                    last_tested_at TIMESTAMP WITH TIME ZONE,
                    details TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                    CONSTRAINT unique_level UNIQUE (symbol, level_price, timeframe, level_type, details)
                );
            """)
            cur.execute("SELECT 1 FROM information_schema.columns WHERE table_name='support_resistance_levels' AND column_name='score'")
            if not cur.fetchone():
                logger.info("[DB] Ø¹Ù…ÙˆØ¯ 'score' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø³ÙŠØªÙ… Ø¥Ø¶Ø§ÙØªÙ‡...")
                cur.execute("ALTER TABLE support_resistance_levels ADD COLUMN score NUMERIC DEFAULT 0;")
                logger.info("âœ… [DB] ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø¹Ù…ÙˆØ¯ 'score' Ø¨Ù†Ø¬Ø§Ø­.")

            conn.commit()
        logger.info("âœ… [Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] ØªÙ… ØªÙ‡ÙŠØ¦Ø© ÙˆØªØ­Ø¯ÙŠØ« Ø¬Ø¯ÙˆÙ„ 'support_resistance_levels' Ø¨Ù†Ø¬Ø§Ø­.")
        return conn
    except Exception as e:
        logger.critical(f"âŒ [Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø£Ùˆ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„: {e}")
        if conn: conn.rollback()
        return None

def save_levels_to_db_batch(conn: psycopg2.extensions.connection, all_final_levels: List[Dict]):
    if not all_final_levels:
        logger.info("â„¹ï¸ [DB] Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙˆÙŠØ§Øª Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„ÙŠØªÙ… Ø­ÙØ¸Ù‡Ø§.")
        return
    logger.info(f"â³ [DB] Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ {len(all_final_levels)} Ù…Ø³ØªÙˆÙ‰ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    try:
        with conn.cursor() as cur:
            symbols_processed = list(set(level['symbol'] for level in all_final_levels))
            cur.execute("DELETE FROM support_resistance_levels WHERE symbol = ANY(%s);", (symbols_processed,))
            logger.info(f"[DB] ØªÙ… Ø­Ø°Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù„Ù€ {len(symbols_processed)} Ø¹Ù…Ù„Ø©.")
            
            insert_query = """
                INSERT INTO support_resistance_levels 
                (symbol, level_price, level_type, timeframe, strength, score, last_tested_at, details) 
                VALUES %s ON CONFLICT (symbol, level_price, timeframe, level_type, details) DO NOTHING;
            """
            values_to_insert = [
                (level.get('symbol'), level.get('level_price'), level.get('level_type'), 
                 level.get('timeframe'), level.get('strength'), level.get('score', 0), 
                 level.get('last_tested_at'), level.get('details')) 
                for level in all_final_levels
            ]
            execute_values(cur, insert_query, values_to_insert)
        conn.commit()
        logger.info(f"âœ… [DB] ØªÙ… Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ù…Ø¬Ù…Ø¹.")
    except Exception as e:
        logger.error(f"âŒ [DB] Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ù…Ø¬Ù…Ø¹ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}", exc_info=True)
        conn.rollback()


# ---------------------- Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª ----------------------

def calculate_level_score(level: Dict) -> int:
    score = 0
    score += float(level.get('strength', 1)) * 10
    last_tested = level.get('last_tested_at')
    if last_tested:
        if isinstance(last_tested, dt.datetime) and last_tested.tzinfo is None:
             last_tested = last_tested.replace(tzinfo=dt.timezone.utc)
        days_since_tested = (dt.datetime.now(dt.timezone.utc) - last_tested).days
        if days_since_tested < 2: score += 30
        elif days_since_tested < 7: score += 15
        elif days_since_tested < 30: score += 5
    if level.get('level_type') == 'confluence':
        num_timeframes = len(level.get('timeframe', '').split(','))
        num_details = len(level.get('details', '').split(','))
        score += (num_timeframes + num_details) * 20
        if 'poc' in level.get('details', ''): score += 25
    if level.get('level_type') == 'poc':
        score += 15
    if 'fib' in level.get('level_type', ''):
        score += 5
        if 'Golden Level' in level.get('details', ''):
            score += 20
    return int(score)

def calculate_atr(df: pd.DataFrame, period: int) -> float:
    if df.empty or len(df) < period: return 0
    high_low = df['high'] - df['low']
    high_close = np.abs(df['high'] - df['close'].shift())
    low_close = np.abs(df['low'] - df['close'].shift())
    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
    atr = tr.ewm(alpha=1/period, adjust=False).mean()
    return atr.iloc[-1] if not atr.empty else 0

def find_price_action_levels(df: pd.DataFrame, atr_value: float, prominence_multiplier: float, width: int, cluster_eps_percent: float) -> List[Dict]:
    lows = df['low'].to_numpy()
    highs = df['high'].to_numpy()
    dynamic_prominence = atr_value * prominence_multiplier
    if dynamic_prominence == 0:
        logger.warning("[Peaks] Ù‚ÙŠÙ…Ø© ATR ØªØ³Ø§ÙˆÙŠ ØµÙØ±ØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚ÙŠÙ…Ø© Ø¨Ø±ÙˆØ² Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ØµØºÙŠØ±Ø©.")
        dynamic_prominence = highs.mean() * 0.01 
    low_peaks_indices, _ = find_peaks(-lows, prominence=dynamic_prominence, width=width)
    high_peaks_indices, _ = find_peaks(highs, prominence=dynamic_prominence, width=width)
    if VOLUME_CONFIRMATION_ENABLED and not df.empty:
        df['volume_avg'] = df['volume'].rolling(window=VOLUME_AVG_PERIOD, min_periods=1).mean()
        confirmed_low_indices = [idx for idx in low_peaks_indices if df['volume'].iloc[idx] >= df['volume_avg'].iloc[idx] * VOLUME_SPIKE_FACTOR]
        confirmed_high_indices = [idx for idx in high_peaks_indices if df['volume'].iloc[idx] >= df['volume_avg'].iloc[idx] * VOLUME_SPIKE_FACTOR]
        low_peaks_indices, high_peaks_indices = np.array(confirmed_low_indices), np.array(confirmed_high_indices)
    def cluster_and_strengthen(prices: np.ndarray, indices: np.ndarray, level_type: str) -> List[Dict]:
        if len(indices) < 2: return []
        points = prices[indices].reshape(-1, 1)
        eps_value = points.mean() * cluster_eps_percent
        if eps_value == 0: return []
        db = DBSCAN(eps=eps_value, min_samples=2).fit(points)
        clustered_levels = []
        for label in set(db.labels_):
            if label != -1:
                mask = (db.labels_ == label)
                cluster_indices = indices[mask]
                clustered_levels.append({
                    "level_price": float(prices[cluster_indices].mean()),
                    "level_type": level_type,
                    "strength": int(len(cluster_indices)),
                    "last_tested_at": df.index[cluster_indices[-1]].to_pydatetime()
                })
        return clustered_levels
    support_levels = cluster_and_strengthen(lows, low_peaks_indices, 'support')
    resistance_levels = cluster_and_strengthen(highs, high_peaks_indices, 'resistance')
    return support_levels + resistance_levels

def analyze_volume_profile(df: pd.DataFrame, bins: int) -> List[Dict]:
    price_min, price_max = df['low'].min(), df['high'].max()
    if price_min >= price_max: return []
    price_bins = np.linspace(price_min, price_max, bins + 1)
    bin_centers = (price_bins[:-1] + price_bins[1:]) / 2
    volume_by_bin = np.zeros(bins)
    for _, row in df.iterrows():
        low_idx = np.searchsorted(price_bins, row['low'], side='right') - 1
        high_idx = np.searchsorted(price_bins, row['high'], side='left')
        low_idx, high_idx = max(0, low_idx), min(bins, high_idx)
        if high_idx > low_idx:
            volume_per_bin = row['volume'] / (high_idx - low_idx)
            for i in range(low_idx, high_idx): volume_by_bin[i] += volume_per_bin
    if np.sum(volume_by_bin) == 0: return []
    poc_index = np.argmax(volume_by_bin)
    return [{"level_price": float(bin_centers[poc_index]), "level_type": 'poc', "strength": float(volume_by_bin[poc_index]), "last_tested_at": None}]

def calculate_fibonacci_levels(df: pd.DataFrame) -> List[Dict]:
    if df.empty: return []
    max_high = df['high'].max()
    min_low = df['low'].min()
    diff = max_high - min_low
    if diff <= 0: return []
    fib_ratios = [0.236, 0.382, 0.5, 0.618, 0.786]
    all_fib_levels = []
    # Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¯Ø¹ÙˆÙ…: 0 Ø¹Ù†Ø¯ Ø£Ø¹Ù„Ù‰ Ù‚Ù…Ø©, 1 Ø¹Ù†Ø¯ Ø£Ø¯Ù†Ù‰ Ù‚Ø§Ø¹
    for ratio in fib_ratios:
        level_price = max_high - (diff * ratio)
        details = f"Fib Support {ratio*100:.1f}%" + (" (Golden Level)" if ratio == 0.618 else "")
        all_fib_levels.append({
            "level_price": float(level_price), "level_type": 'fib_support',
            "strength": 20 if ratio == 0.618 else 5, "details": details, "last_tested_at": None
        })
    # Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø§Øª: 0 Ø¹Ù†Ø¯ Ø£Ø¯Ù†Ù‰ Ù‚Ø§Ø¹, 1 Ø¹Ù†Ø¯ Ø£Ø¹Ù„Ù‰ Ù‚Ù…Ø©
    for ratio in fib_ratios:
        level_price = min_low + (diff * ratio)
        details = f"Fib Resistance {ratio*100:.1f}%" + (" (Golden Level)" if ratio == 0.618 else "")
        all_fib_levels.append({
            "level_price": float(level_price), "level_type": 'fib_resistance',
            "strength": 20 if ratio == 0.618 else 5, "details": details, "last_tested_at": None
        })
    return all_fib_levels

def find_confluence_zones(levels: List[Dict], confluence_percent: float) -> Tuple[List[Dict], List[Dict]]:
    if not levels: return [], []
    levels.sort(key=lambda x: x['level_price'])
    tf_weights = {'1h': 3, '15m': 2, '5m': 1} 
    type_weights = {'poc': 2.5, 'support': 1.5, 'resistance': 1.5, 'fib_support': 1.2, 'fib_resistance': 1.2}
    confluence_zones, used_indices = [], set()
    for i in range(len(levels)):
        if i in used_indices: continue
        current_zone_levels, current_zone_indices = [levels[i]], {i}
        for j in range(i + 1, len(levels)):
            if j in used_indices: continue
            price_i, price_j = levels[i]['level_price'], levels[j]['level_price']
            if price_i > 0 and (abs(price_j - price_i) / price_i) <= confluence_percent:
                current_zone_levels.append(levels[j])
                current_zone_indices.add(j)
        if len(current_zone_levels) > 1:
            used_indices.update(current_zone_indices)
            total_strength_for_avg = sum(l['strength'] for l in current_zone_levels)
            if total_strength_for_avg == 0: continue
            avg_price = sum(l['level_price'] * l['strength'] for l in current_zone_levels) / total_strength_for_avg
            total_strength = sum(l['strength'] * tf_weights.get(l.get('timeframe'), 1) * type_weights.get(l.get('level_type'), 1) for l in current_zone_levels)
            timeframes = sorted(list(set(str(l['timeframe']) for l in current_zone_levels)))
            details = sorted(list(set(l['level_type'] for l in current_zone_levels)))
            last_tested = max((l['last_tested_at'] for l in current_zone_levels if l['last_tested_at']), default=None)
            confluence_zones.append({
                "level_price": avg_price, "level_type": 'confluence', "strength": float(total_strength), 
                "timeframe": ",".join(timeframes), "details": ",".join(details), "last_tested_at": last_tested
            })
    remaining_levels = [level for i, level in enumerate(levels) if i not in used_indices]
    return confluence_zones, remaining_levels

# ---------------------- Ø­Ù„Ù‚Ø© Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„ ----------------------

def analyze_single_symbol(symbol: str, client: Client) -> List[Dict]:
    logger.info(f"--- Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ (Ø³ÙƒØ§Ù„Ø¨ÙŠÙ†Ø¬) Ù„Ù„Ø¹Ù…Ù„Ø©: {symbol} ---")
    raw_levels = []
    
    # --- Ø§Ù„ØªØ¹Ø¯ÙŠÙ„: Ø­Ø³Ø§Ø¨ ÙÙŠØ¨ÙˆÙ†Ø§ØªØ´ÙŠ Ø£ÙˆÙ„Ø§Ù‹ ÙˆØ¨Ø´ÙƒÙ„ Ø­ØµØ±ÙŠ Ø¹Ù„Ù‰ ÙØ±ÙŠÙ… 15 Ø¯Ù‚ÙŠÙ‚Ø© ---
    df_15m = fetch_historical_data_with_retry(client, symbol, '15m', DATA_FETCH_DAYS_15M)
    if df_15m is not None and not df_15m.empty:
        logger.info(f"==> [{symbol}] Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙŠØ§Øª ÙÙŠØ¨ÙˆÙ†Ø§ØªØ´ÙŠ Ø¹Ù„Ù‰ ÙØ±ÙŠÙ… 15 Ø¯Ù‚ÙŠÙ‚Ø©...")
        fib_levels = calculate_fibonacci_levels(df_15m)
        for level in fib_levels:
            level['timeframe'] = '15m'
        raw_levels.extend(fib_levels)
    else:
        logger.warning(f"âš ï¸ [{symbol}] ØªØ¹Ø°Ø± Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª 15 Ø¯Ù‚ÙŠÙ‚Ø© Ù„Ø­Ø³Ø§Ø¨ ÙÙŠØ¨ÙˆÙ†Ø§ØªØ´ÙŠ.")

    # --- Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø§Ø¯ÙŠ Ù„Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ (Ø¨Ø¯ÙˆÙ† ÙÙŠØ¨ÙˆÙ†Ø§ØªØ´ÙŠ) ---
    timeframes_config = {
        '1h':  {'days': DATA_FETCH_DAYS_1H,  'prominence_multiplier': ATR_PROMINENCE_MULTIPLIER_1H,  'width': WIDTH_1H},
        '15m': {'days': DATA_FETCH_DAYS_15M, 'prominence_multiplier': ATR_PROMINENCE_MULTIPLIER_15M, 'width': WIDTH_15M},
        '5m':  {'days': DATA_FETCH_DAYS_5M,  'prominence_multiplier': ATR_PROMINENCE_MULTIPLIER_5M,  'width': WIDTH_5M}
    }

    for tf, config in timeframes_config.items():
        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª 15 Ø¯Ù‚ÙŠÙ‚Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ø£Ùˆ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©
        df = df_15m if tf == '15m' else fetch_historical_data_with_retry(client, symbol, tf, config['days'])
        
        if df is not None and not df.empty:
            atr_standard = calculate_atr(df, period=ATR_PERIOD)
            atr_short = calculate_atr(df, period=ATR_SHORT_PERIOD)
            atr_long = calculate_atr(df, period=ATR_LONG_PERIOD)
            dynamic_prominence_multiplier = config['prominence_multiplier']
            if atr_long > 0 and atr_short > atr_long * 1.25: dynamic_prominence_multiplier *= 1.2
            elif atr_long > 0 and atr_short < atr_long * 0.8: dynamic_prominence_multiplier *= 0.8

            pa_levels = find_price_action_levels(df, atr_standard, dynamic_prominence_multiplier, config['width'], CLUSTER_EPS_PERCENT)
            vol_levels = analyze_volume_profile(df, bins=VOLUME_PROFILE_BINS)
            
            all_new_levels = pa_levels + vol_levels
            for level in all_new_levels:
                level['timeframe'] = tf
            raw_levels.extend(all_new_levels)
        else:
            logger.warning(f"âš ï¸ [{symbol}-{tf}] ØªØ¹Ø°Ø± Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø³ÙŠØªÙ… Ø§Ù„ØªØ®Ø·ÙŠ.")
        
    if not raw_levels:
        logger.info(f"â„¹ï¸ [{symbol}] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ø³ØªÙˆÙŠØ§Øª Ø£ÙˆÙ„ÙŠØ©.")
        return []

    confluence_zones, remaining_singles = find_confluence_zones(raw_levels, CONFLUENCE_ZONE_PERCENT)
    final_levels = confluence_zones + remaining_singles
    
    for level in final_levels:
        level['symbol'] = symbol
        level['score'] = calculate_level_score(level)
        
    logger.info(f"--- âœ… Ø§Ù†ØªÙ‡Ù‰ ØªØ­Ù„ÙŠÙ„ {symbol}ØŒ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(final_levels)} Ù…Ø³ØªÙˆÙ‰ Ù†Ù‡Ø§Ø¦ÙŠ. ---")
    return final_levels

def run_full_analysis():
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ù…Ø­Ù„Ù„ Ø§Ù„Ø³ÙƒØ§Ù„Ø¨ÙŠÙ†Ø¬...")
    
    client = get_binance_client()
    if not client: return
    conn = init_db()
    if not conn: return
    symbols_to_scan = get_validated_symbols(client, 'crypto_list.txt')
    if not symbols_to_scan:
        logger.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù…Ù„Ø§Øª Ù„ØªØ­Ù„ÙŠÙ„Ù‡Ø§. Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©.")
        conn.close()
        return

    logger.info(f"ğŸŒ€ Ø³ÙŠØªÙ… ØªØ­Ù„ÙŠÙ„ {len(symbols_to_scan)} Ø¹Ù…Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… {MAX_WORKERS} Ø®ÙŠØ·Ø§Ù‹ Ù…ØªÙˆØ§Ø²ÙŠØ§Ù‹.")
    all_final_levels = []
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_symbol = {executor.submit(analyze_single_symbol, symbol, client): symbol for symbol in symbols_to_scan}
        for i, future in enumerate(as_completed(future_to_symbol)):
            symbol = future_to_symbol[future]
            try:
                symbol_levels = future.result()
                if symbol_levels: all_final_levels.extend(symbol_levels)
                logger.info(f"ğŸ”„ ({i+1}/{len(symbols_to_scan)}) ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†ØªØ§Ø¦Ø¬ {symbol}.")
            except Exception as e:
                logger.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ {symbol}: {e}", exc_info=True)

    if all_final_levels:
        all_final_levels.sort(key=lambda x: x.get('score', 0), reverse=True)
        save_levels_to_db_batch(conn, all_final_levels)
    else:
        logger.info("â„¹ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ø³ØªÙˆÙŠØ§Øª ÙÙŠ Ø£ÙŠ Ø¹Ù…Ù„Ø© Ø®Ù„Ø§Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ±Ø©.")

    conn.close()
    logger.info("ğŸ‰ğŸ‰ğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ø¯ÙˆØ±Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙƒØ§Ù„Ø¨ÙŠÙ†Ø¬! ğŸ‰ğŸ‰ğŸ‰")

def analysis_scheduler():
    while True:
        try:
            run_full_analysis()
        except Exception as e:
            logger.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ ÙÙŠ Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {e}", exc_info=True)
        
        sleep_duration_seconds = ANALYSIS_INTERVAL_MINUTES * 60
        logger.info(f"ğŸ‘ Ø§ÙƒØªÙ…Ù„Øª Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„. Ø³ÙŠØªÙ… Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù…Ø¯Ø© {ANALYSIS_INTERVAL_MINUTES} Ø¯Ù‚ÙŠÙ‚Ø©.")
        time.sleep(sleep_duration_seconds)

# ---------------------- Ù†Ù‚Ø·Ø© Ø§Ù†Ø·Ù„Ø§Ù‚ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ----------------------
if __name__ == "__main__":
    web_server_thread = threading.Thread(target=run_web_server, daemon=True)
    web_server_thread.start()
    analysis_thread = threading.Thread(target=analysis_scheduler, daemon=True)
    analysis_thread.start()
    try:
        while True: time.sleep(3600)
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ØªÙ… Ø·Ù„Ø¨ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬. ÙˆØ¯Ø§Ø¹Ø§Ù‹!")
