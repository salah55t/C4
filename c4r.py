import os
import time
import logging
import psycopg2
import numpy as np
import pandas as pd
from decouple import config
from binance.client import Client
from psycopg2.extras import RealDictCursor
from scipy.signal import find_peaks
from sklearn.cluster import DBSCAN
from typing import List, Dict, Optional, Tuple

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (Logging) ----------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('sr_scanner_v3.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('SR_Scanner_V3')

# ---------------------- ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© ----------------------
try:
    API_KEY: str = config('BINANCE_API_KEY')
    API_SECRET: str = config('BINANCE_API_SECRET')
    DB_URL: str = config('DATABASE_URL')
except Exception as e:
    logger.critical(f"âŒ ÙØ´Ù„ Ø­Ø§Ø³Ù… ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {e}")
    exit(1)

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø«ÙˆØ§Ø¨Øª ----------------------
# ÙƒÙ…ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
DATA_FETCH_DAYS_1D = 600
DATA_FETCH_DAYS_4H = 200
DATA_FETCH_DAYS_15M = 30

# Ù…Ø¹Ø§ÙŠÙŠØ± ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚Ù…Ù… ÙˆØ§Ù„Ù‚ÙŠØ¹Ø§Ù†
PROMINENCE_1D = 0.025
WIDTH_1D = 10
PROMINENCE_4H = 0.015
WIDTH_4H = 5
PROMINENCE_15M = 0.008
WIDTH_15M = 10

# Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªØ¬Ù…ÙŠØ¹ ÙˆØ§Ù„Ø¯Ù…Ø¬
CLUSTER_EPS_PERCENT = 0.005 # Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙ‚Ø§Ø±Ø¨ Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù‚Ù…Ù…/Ø§Ù„Ù‚ÙŠØ¹Ø§Ù†
CONFLUENCE_ZONE_PERCENT = 0.005 # Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙ‚Ø§Ø±Ø¨ Ù„Ø¯Ù…Ø¬ Ù…Ø³ØªÙˆÙŠØ§Øª Ù…Ù† ÙØ±ÙŠÙ…Ø§Øª Ù…Ø®ØªÙ„ÙØ© (0.5%)

# Ù…Ø¹Ø§ÙŠÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø¨Ø±ÙˆÙØ§ÙŠÙ„ Ø§Ù„Ø­Ø¬Ù…
VOLUME_PROFILE_BINS = 100

# ---------------------- Ø¯ÙˆØ§Ù„ Binance ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ----------------------
def get_binance_client() -> Optional[Client]:
    """ÙŠÙ‚ÙˆÙ… Ø¨ØªÙ‡ÙŠØ¦Ø© ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„ Ù…Ø¹ Binance."""
    try:
        client = Client(API_KEY, API_SECRET)
        client.ping()
        logger.info("âœ… [Binance] ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Binance Ø¨Ù†Ø¬Ø§Ø­.")
        return client
    except Exception as e:
        logger.critical(f"âŒ [Binance] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª: {e}")
        return None

def fetch_historical_data(client: Client, symbol: str, interval: str, days: int) -> Optional[pd.DataFrame]:
    """Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ø¹Ù…Ù„Ø© Ù…Ø¹ÙŠÙ†Ø©."""
    try:
        start_str = (pd.to_datetime('today') - pd.Timedelta(days=days)).strftime('%Y-%m-%d')
        logger.info(f"â³ [Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª {symbol} Ø¹Ù„Ù‰ ÙØ±ÙŠÙ… {interval} Ù„Ø¢Ø®Ø± {days} ÙŠÙˆÙ…...")
        klines = client.get_historical_klines(symbol, interval, start_str)
        if not klines:
            logger.warning(f"âš ï¸ [Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol} Ø¹Ù„Ù‰ ÙØ±ÙŠÙ… {interval}.")
            return None
        
        df = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_volume', 'trades', 'taker_buy_base', 'taker_buy_quote', 'ignore'])
        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        logger.info(f"âœ… [Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] ØªÙ… Ø¬Ù„Ø¨ {len(df)} Ø´Ù…Ø¹Ø© Ø¨Ù†Ø¬Ø§Ø­.")
        return df[numeric_cols].dropna()
    except Exception as e:
        logger.error(f"âŒ [Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ù€ {symbol}: {e}")
        return None

def get_validated_symbols(client: Client, filename: str = 'crypto_list.txt') -> List[str]:
    """Ù‚Ø±Ø§Ø¡Ø© Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Øª ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†Ù‡Ø§ Ù…Ø¹ Binance."""
    logger.info(f"â„¹ï¸ [Ø§Ù„ØªØ­Ù‚Ù‚] Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ù…Ù† '{filename}' ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†Ù‡Ø§...")
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, filename)
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_symbols = {line.strip().upper() for line in f if line.strip() and not line.startswith('#')}
        
        formatted = {f"{s}USDT" if not s.endswith('USDT') else s for s in raw_symbols}
        exchange_info = client.get_exchange_info()
        active = {s['symbol'] for s in exchange_info['symbols'] if s.get('quoteAsset') == 'USDT' and s.get('status') == 'TRADING'}
        
        validated = sorted(list(formatted.intersection(active)))
        logger.info(f"âœ… [Ø§Ù„ØªØ­Ù‚Ù‚] Ø³ÙŠØªÙ… ØªØ­Ù„ÙŠÙ„ {len(validated)} Ø¹Ù…Ù„Ø© Ù…Ø¹ØªÙ…Ø¯Ø©.")
        return validated
    except Exception as e:
        logger.error(f"âŒ [Ø§Ù„ØªØ­Ù‚Ù‚] Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ²: {e}", exc_info=True)
        return []

# ---------------------- Ø¯ÙˆØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ----------------------
def init_db() -> Optional[psycopg2.extensions.connection]:
    """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¥Ù†Ø´Ø§Ø¡/ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¬Ø¯ÙˆÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§."""
    logger.info("[Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„...")
    conn = None
    try:
        conn = psycopg2.connect(DB_URL, connect_timeout=10, cursor_factory=RealDictCursor)
        with conn.cursor() as cur:
            # Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§
            # The original CREATE TABLE statement is correct, it includes the 'details' column.
            cur.execute("""
                CREATE TABLE IF NOT EXISTS support_resistance_levels (
                    id SERIAL PRIMARY KEY,
                    symbol TEXT NOT NULL,
                    level_price DOUBLE PRECISION NOT NULL,
                    level_type TEXT NOT NULL, -- 'support','resistance','poc','hvn','confluence'
                    timeframe TEXT NOT NULL, -- '15m', '4h', '1d', '15m,4h' etc. for confluence
                    strength BIGINT NOT NULL, -- Weighted strength score
                    last_tested_at TIMESTAMP,
                    details TEXT, -- Contributing level types for confluence, e.g., 'poc,support'
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                    CONSTRAINT unique_level UNIQUE (symbol, level_price, timeframe, level_type)
                );
            """)
            conn.commit()

            # Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¹Ù…ÙˆØ¯ 'details' ÙˆØ¥Ø¶Ø§ÙØªÙ‡ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙÙ‚ÙˆØ¯Ù‹Ø§ (Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù…Ù† Ø§Ù„Ø¬Ø¯ÙˆÙ„)
            cur.execute("""
                SELECT 1 FROM information_schema.columns 
                WHERE table_name='support_resistance_levels' AND column_name='details';
            """)
            if cur.fetchone() is None:
                logger.info("[Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] Ø§Ù„Ø¹Ù…ÙˆØ¯ 'details' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ø¬Ø§Ø±ÙŠ Ø¥Ø¶Ø§ÙØªÙ‡ Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¬Ø¯ÙˆÙ„...")
                cur.execute("ALTER TABLE support_resistance_levels ADD COLUMN details TEXT;")
                conn.commit()
                logger.info("âœ… [Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¹Ù…ÙˆØ¯ 'details' Ø¨Ù†Ø¬Ø§Ø­.")

        logger.info("âœ… [Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø¬Ø¯ÙˆÙ„ 'support_resistance_levels' ÙˆØ§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­Ø¯ÙŠØ«Ù‡ Ø¨Ù†Ø¬Ø§Ø­.")
        return conn
    except Exception as e:
        logger.critical(f"âŒ [Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø£Ùˆ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„: {e}")
        # It's important to rollback on failure
        if conn:
            conn.rollback()
        return None

def save_levels_to_db(conn: psycopg2.extensions.connection, symbol: str, levels: List[Dict]):
    """Ø­ÙØ¸ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ù…ÙØµÙÙ‘Ø§Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
    if not levels:
        logger.info(f"â„¹ï¸ [{symbol}] Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙˆÙŠØ§Øª Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„ÙŠØªÙ… Ø­ÙØ¸Ù‡Ø§.")
        return

    logger.info(f"â³ [{symbol}] Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ {len(levels)} Ù…Ø³ØªÙˆÙ‰ Ù…ÙØµÙÙ‘Ù‰ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    try:
        with conn.cursor() as cur:
            cur.execute("DELETE FROM support_resistance_levels WHERE symbol = %s;", (symbol,))
            
            insert_query = """
                INSERT INTO support_resistance_levels 
                (symbol, level_price, level_type, timeframe, strength, last_tested_at, details) 
                VALUES (%s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (symbol, level_price, timeframe, level_type) DO NOTHING; 
            """
            for level in levels:
                cur.execute(insert_query, (
                    symbol, level.get('level_price'), level.get('level_type'),
                    level.get('timeframe'), level.get('strength'),
                    level.get('last_tested_at'), level.get('details')
                ))
        conn.commit()
        logger.info(f"âœ… [{symbol}] ØªÙ… Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…ÙØµÙÙ‘Ø§Ø© Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        logger.error(f"âŒ [{symbol}] Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        conn.rollback()

# ---------------------- Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª ----------------------

def find_price_action_levels(df: pd.DataFrame, prominence: float, width: int, cluster_eps_percent: float) -> List[Dict]:
    """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚Ù…Ù… ÙˆØ§Ù„Ù‚ÙŠØ¹Ø§Ù† ÙˆØªØ¬Ù…ÙŠØ¹Ù‡Ø§ Ù„ØªØ­Ø¯ÙŠØ¯ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø©."""
    lows = df['low'].to_numpy()
    highs = df['high'].to_numpy()
    low_peaks_indices, _ = find_peaks(-lows, prominence=lows.mean() * prominence, width=width)
    high_peaks_indices, _ = find_peaks(highs, prominence=highs.mean() * prominence, width=width)

    def cluster_and_strengthen(prices: np.ndarray, indices: np.ndarray, level_type: str) -> List[Dict]:
        if len(indices) < 2: return []
        points = prices[indices].reshape(-1, 1)
        eps_value = points.mean() * cluster_eps_percent
        db = DBSCAN(eps=eps_value, min_samples=2).fit(points)
        clustered_levels = []
        for label in set(db.labels_):
            if label != -1:
                mask = (db.labels_ == label)
                cluster_indices = indices[mask]
                clustered_levels.append({
                    "level_price": float(prices[cluster_indices].mean()),
                    "level_type": level_type,
                    "strength": int(len(cluster_indices)),
                    "last_tested_at": df.index[cluster_indices[-1]].to_pydatetime()
                })
        return clustered_levels

    support_levels = cluster_and_strengthen(lows, low_peaks_indices, 'support')
    resistance_levels = cluster_and_strengthen(highs, high_peaks_indices, 'resistance')
    return support_levels + resistance_levels

def analyze_volume_profile(df: pd.DataFrame, bins: int) -> List[Dict]:
    """
    ØªØ­Ù„ÙŠÙ„ Ø¨Ø±ÙˆÙØ§ÙŠÙ„ Ø§Ù„Ø­Ø¬Ù… Ù„ØªØ­Ø¯ÙŠØ¯ Ù†Ù‚Ø·Ø© Ø§Ù„ØªØ­ÙƒÙ… (POC). (Ø¥ØµØ¯Ø§Ø± Ù…ØµØ­Ø­)
    """
    price_min, price_max = df['low'].min(), df['high'].max()
    if price_min >= price_max:
        logger.warning("[Volume Profile] Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø³Ø¹Ø±ÙŠ ØºÙŠØ± ØµØ§Ù„Ø­. ÙŠØªÙ… Ø§Ù„ØªØ®Ø·ÙŠ.")
        return []

    # Ø¥Ù†Ø´Ø§Ø¡ "Ø­Ø¯ÙˆØ¯" Ø§Ù„Ø³Ù„Ø§Øª ÙˆØ­Ø³Ø§Ø¨ "Ù…Ø±Ø§ÙƒØ²" Ø§Ù„Ø³Ù„Ø§Øª
    price_bins = np.linspace(price_min, price_max, bins + 1)
    bin_centers = (price_bins[:-1] + price_bins[1:]) / 2
    volume_by_bin = np.zeros(bins)

    for _, row in df.iterrows():
        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø³Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªØºØ·ÙŠÙ‡Ø§ Ø§Ù„Ø´Ù…Ø¹Ø©
        low_idx = np.searchsorted(price_bins, row['low']) - 1
        high_idx = np.searchsorted(price_bins, row['high']) -1

        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø¶Ù…Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„ØµØ­ÙŠØ­Ø©
        low_idx = max(0, low_idx)
        high_idx = min(bins - 1, high_idx)
        
        if high_idx >= low_idx:
            num_bins_spanned = (high_idx - low_idx) + 1
            volume_per_bin = row['volume'] / num_bins_spanned
            # ØªÙˆØ²ÙŠØ¹ Ø­Ø¬Ù… Ø§Ù„ØªØ¯Ø§ÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ù„Ø§Øª Ø§Ù„ØªÙŠ Ù…Ø±Øª Ø¨Ù‡Ø§ Ø§Ù„Ø´Ù…Ø¹Ø©
            for i in range(low_idx, high_idx + 1):
                volume_by_bin[i] += volume_per_bin
    
    if np.sum(volume_by_bin) == 0:
        logger.warning("[Volume Profile] Ù„Ù… ÙŠØªÙ… Ø­Ø³Ø§Ø¨ Ø£ÙŠ Ø­Ø¬Ù….")
        return []

    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø³Ù„Ø© Ø°Ø§Øª Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø£Ø¹Ù„Ù‰ (POC)
    poc_index = np.argmax(volume_by_bin)
    poc_price = bin_centers[poc_index]
    poc_volume = volume_by_bin[poc_index]
    
    return [{
        "level_price": float(poc_price),
        "level_type": 'poc',
        "strength": int(poc_volume),
        "last_tested_at": None
    }]


def find_confluence_zones(levels: List[Dict], confluence_percent: float) -> Tuple[List[Dict], List[Dict]]:
    """
    ØªØ­Ø¯ÙŠØ¯ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØªÙˆØ§ÙÙ‚ (Confluence) Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…ØªÙ‚Ø§Ø±Ø¨Ø©.
    """
    if not levels: return [], []
    levels.sort(key=lambda x: x['level_price'])
    
    tf_weights = {'1d': 3, '4h': 2, '15m': 1}
    type_weights = {'poc': 2.5, 'support': 1.5, 'resistance': 1.5, 'hvn': 1, 'confluence': 4}

    confluence_zones = []
    used_indices = set()
    
    for i in range(len(levels)):
        if i in used_indices: continue
        
        current_zone_levels = [levels[i]]
        current_zone_indices = {i}
        
        for j in range(i + 1, len(levels)):
            if j in used_indices: continue
            
            price_i = levels[i]['level_price']
            price_j = levels[j]['level_price']

            if (abs(price_j - price_i) / price_i) <= confluence_percent:
                current_zone_levels.append(levels[j])
                current_zone_indices.add(j)

        if len(current_zone_levels) > 1:
            used_indices.update(current_zone_indices)
            
            avg_price = sum(l['level_price'] * l['strength'] for l in current_zone_levels) / sum(l['strength'] for l in current_zone_levels)
            total_strength = 0
            for l in current_zone_levels:
                tf_w = tf_weights.get(l['timeframe'], 1)
                type_w = type_weights.get(l['level_type'], 1)
                total_strength += l['strength'] * tf_w * type_w

            timeframes = sorted(list(set(l['timeframe'] for l in current_zone_levels)))
            details = sorted(list(set(l['level_type'] for l in current_zone_levels)))
            last_tested = max((l['last_tested_at'] for l in current_zone_levels if l['last_tested_at']), default=None)

            confluence_zones.append({
                "level_price": avg_price,
                "level_type": 'confluence',
                "strength": int(total_strength),
                "timeframe": ",".join(timeframes),
                "details": ",".join(details),
                "last_tested_at": last_tested
            })

    remaining_levels = [level for i, level in enumerate(levels) if i not in used_indices]
    
    logger.info(f"ğŸ¤ [Confluence] ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(confluence_zones)} Ù…Ù†Ø·Ù‚Ø© ØªÙˆØ§ÙÙ‚ Ùˆ {len(remaining_levels)} Ù…Ø³ØªÙˆÙ‰ ÙØ±Ø¯ÙŠ Ù…ØªØ¨Ù‚ÙŠ.")
    return confluence_zones, remaining_levels


# ---------------------- Ø­Ù„Ù‚Ø© Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ----------------------
def main():
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ù…Ø­Ù„Ù„ Ø§Ù„Ø¯Ø¹ÙˆÙ… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø§Øª (Ø§Ù„Ø¥ØµØ¯Ø§Ø± 3.1 Ù…Ø¹ Confluence Ù…ØµØ­Ø­)...")
    
    client = get_binance_client()
    if not client: return
        
    conn = init_db()
    if not conn: return

    symbols_to_scan = get_validated_symbols(client, 'crypto_list.txt')
    if not symbols_to_scan:
        logger.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù…Ù„Ø§Øª Ù„ØªØ­Ù„ÙŠÙ„Ù‡Ø§. Ø³ÙŠØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ´ØºÙŠÙ„.")
        return

    logger.info(f"ğŸŒ€ Ø³ÙŠØªÙ… ØªØ­Ù„ÙŠÙ„ {len(symbols_to_scan)} Ø¹Ù…Ù„Ø©.")

    timeframes_config = {
        '1d':  {'days': DATA_FETCH_DAYS_1D,  'prominence': PROMINENCE_1D,  'width': WIDTH_1D},
        '4h':  {'days': DATA_FETCH_DAYS_4H,  'prominence': PROMINENCE_4H,  'width': WIDTH_4H},
        '15m': {'days': DATA_FETCH_DAYS_15M, 'prominence': PROMINENCE_15M, 'width': WIDTH_15M}
    }

    for i, symbol in enumerate(symbols_to_scan):
        logger.info(f"--- ({i+1}/{len(symbols_to_scan)}) Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„Ø©: {symbol} ---")
        raw_levels = []

        for tf, config in timeframes_config.items():
            df = fetch_historical_data(client, symbol, tf, config['days'])
            if df is not None and not df.empty:
                pa_levels = find_price_action_levels(df, config['prominence'], config['width'], CLUSTER_EPS_PERCENT)
                vol_levels = analyze_volume_profile(df, bins=VOLUME_PROFILE_BINS)
                
                for level in pa_levels + vol_levels:
                    level['timeframe'] = tf
                raw_levels.extend(pa_levels + vol_levels)
            else:
                logger.warning(f"âš ï¸ [{symbol}-{tf}] ØªØ¹Ø°Ø± Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
            time.sleep(1) 
            
        if raw_levels:
            confluence_zones, remaining_singles = find_confluence_zones(raw_levels, CONFLUENCE_ZONE_PERCENT)
            final_levels = confluence_zones + remaining_singles
            save_levels_to_db(conn, symbol, final_levels)
        else:
            logger.info(f"â„¹ï¸ [{symbol}] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ø³ØªÙˆÙŠØ§Øª Ø£ÙˆÙ„ÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„Ù‡Ø§.")
        
        logger.info(f"--- âœ… Ø§Ù†ØªÙ‡Ù‰ ØªØ­Ù„ÙŠÙ„ {symbol} ---")
        time.sleep(2)

    conn.close()
    logger.info("ğŸ‰ğŸ‰ğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© ØªØ­Ù„ÙŠÙ„ ÙˆØ­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø¨Ù†Ø¬Ø§Ø­! ğŸ‰ğŸ‰ğŸ‰")


if __name__ == "__main__":
    main()
