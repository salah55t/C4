import time
import os
import json
import logging
import requests
import numpy as np
import pandas as pd
import psycopg2
import pickle
import lightgbm as lgb
from psycopg2 import sql
from psycopg2.extras import RealDictCursor
from binance.client import Client
from datetime import datetime, timedelta
from decouple import config
from typing import List, Dict, Optional, Any, Tuple
from sklearn.model_selection import TimeSeriesSplit # Ø³Ù†Ø³ØªØ®Ø¯Ù…Ù‡Ø§ Ø¨Ø¹Ù†Ø§ÙŠØ©
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
from flask import Flask
from threading import Thread

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (Logging) ----------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ml_model_trainer_v5.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('MLTrainer_V5')

# ---------------------- ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© ----------------------
try:
    API_KEY: str = config('BINANCE_API_KEY')
    API_SECRET: str = config('BINANCE_API_SECRET')
    DB_URL: str = config('DATABASE_URL')
    TELEGRAM_TOKEN: Optional[str] = config('TELEGRAM_BOT_TOKEN', default=None)
    CHAT_ID: Optional[str] = config('TELEGRAM_CHAT_ID', default=None)
except Exception as e:
     logger.critical(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {e}")
     exit(1)

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø«ÙˆØ§Ø¨Øª ÙˆØ§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© ----------------------
BASE_ML_MODEL_NAME: str = 'LightGBM_Scalping_V6_Enhanced' # ØªØ­Ø¯ÙŠØ« Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
SIGNAL_GENERATION_TIMEFRAME: str = '15m'
DATA_LOOKBACK_DAYS_FOR_TRAINING: int = 180 # Ø²ÙŠØ§Ø¯Ø© ÙØªØ±Ø© Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
BTC_SYMBOL = 'BTCUSDT'

# Indicator & Feature Parameters (Ù…Ø­Ø¯Ø«Ø© Ù…Ø¹ Ù…ÙŠØ²Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© ÙˆÙ†ÙˆØ§ÙØ° Ù…ØªØ¹Ø¯Ø¯Ø©)
RSI_PERIODS: List[int] = [14, 21] # ÙØªØ±Ø§Øª RSI Ù…ØªØ¹Ø¯Ø¯Ø©
MACD_FAST, MACD_SLOW, MACD_SIGNAL = 12, 26, 9
ATR_PERIOD: int = 14
EMA_TREND_PERIODS: Dict[str, int] = {'fast': 50, 'slow': 200, 'mid': 100} # EMA Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ø§ØªØ¬Ø§Ù‡
VOL_MA_PERIOD: int = 30 # Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…ØªØ­Ø±Ùƒ
CORR_PERIODS: List[int] = [30, 60] # ÙØªØ±Ø§Øª Ø§Ø±ØªØ¨Ø§Ø· Ù…ØªØ¹Ø¯Ø¯Ø©
ROLLING_VOLATILITY_PERIODS: List[int] = [10, 20, 50] # ÙØªØ±Ø§Øª ØªÙ‚Ù„Ø¨Ø§Øª Ù…ØªØ¯Ø­Ø±Ø¬Ø©

# Triple-Barrier Method Parameters
TP_ATR_MULTIPLIER: float = 2.0
SL_ATR_MULTIPLIER: float = 1.5
MAX_HOLD_PERIOD: int = 48 # Ø²ÙŠØ§Ø¯Ø© ÙØªØ±Ø© Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø§Ù„Ù‚ØµÙˆÙ‰ (48 * 15 Ø¯Ù‚ÙŠÙ‚Ø© = 12 Ø³Ø§Ø¹Ø©)

# Global variables
conn: Optional[psycopg2.extensions.connection] = None
client: Optional[Client] = None
btc_data_cache: Optional[pd.DataFrame] = None

# --- Ø¯ÙˆØ§Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ ---
def init_db():
    global conn
    try:
        conn = psycopg2.connect(DB_URL, cursor_factory=RealDictCursor)
        with conn.cursor() as cur:
            cur.execute("""
                CREATE TABLE IF NOT EXISTS ml_models (
                    id SERIAL PRIMARY KEY, model_name TEXT NOT NULL UNIQUE,
                    model_data BYTEA NOT NULL, trained_at TIMESTAMP DEFAULT NOW(), metrics JSONB );
            """)
        conn.commit()
        logger.info("âœ… [DB] ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        logger.critical(f"âŒ [DB] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}"); exit(1)

def get_binance_client():
    global client
    try:
        client = Client(API_KEY, API_SECRET)
        client.ping()
        logger.info("âœ… [Binance] ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Binance Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        logger.critical(f"âŒ [Binance] ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ Binance: {e}"); exit(1)

def get_validated_symbols(filename: str = 'crypto_list.txt') -> List[str]:
    if not client:
        logger.error("âŒ [Validation] Ø¹Ù…ÙŠÙ„ Binance Ù„Ù… ÙŠØªÙ… ØªÙ‡ÙŠØ¦ØªÙ‡.")
        return []
    try:
        script_dir = os.path.dirname(__file__)
        file_path = os.path.join(script_dir, filename)
        with open(file_path, 'r', encoding='utf-8') as f:
            symbols = {s.strip().upper() for s in f if s.strip() and not s.startswith('#')}
        formatted = {f"{s}USDT" if not s.endswith('USDT') else s for s in symbols}
        info = client.get_exchange_info()
        active = {s['symbol'] for s in info['symbols'] if s['status'] == 'TRADING' and s['quoteAsset'] == 'USDT'}
        validated = sorted(list(formatted.intersection(active)))
        logger.info(f"âœ… [Validation] ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(validated)} Ø¹Ù…Ù„Ø© ØµØ§Ù„Ø­Ø© Ù„Ù„ØªØ¯Ø§ÙˆÙ„.")
        return validated
    except FileNotFoundError:
        logger.error(f"âŒ [Validation] Ù…Ù„Ù Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Øª '{filename}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")
        return []
    except Exception as e:
        logger.error(f"âŒ [Validation] Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ²: {e}"); return []

# --- Ø¯ÙˆØ§Ù„ Ø¬Ù„Ø¨ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
def fetch_historical_data(symbol: str, interval: str, days: int) -> Optional[pd.DataFrame]:
    try:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… datetime.now() Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† utcnow() Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© Ù…Ø¹ binance
        start_str = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d %H:%M:%S")
        logger.info(f"â„¹ï¸ [Data] Fetching {symbol} data from {start_str} with interval {interval}")
        klines = client.get_historical_klines(symbol, interval, start_str)
        if not klines:
            logger.warning(f"âš ï¸ [Data] No klines data returned for {symbol}.")
            return None
        df = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_volume', 'trades', 'taker_buy_base', 'taker_buy_quote', 'ignore'])
        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols: df[col] = pd.to_numeric(df[col], errors='coerce')
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹
        return df[numeric_cols].dropna()
    except Exception as e:
        logger.error(f"âŒ [Data] Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol}: {e}", exc_info=True)
        return None

def fetch_and_cache_btc_data():
    global btc_data_cache
    logger.info("â„¹ï¸ [BTC Data] Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† ÙˆØªØ®Ø²ÙŠÙ†Ù‡Ø§...")
    btc_data_cache = fetch_historical_data(BTC_SYMBOL, SIGNAL_GENERATION_TIMEFRAME, DATA_LOOKBACK_DAYS_FOR_TRAINING)
    if btc_data_cache is None:
        logger.critical("âŒ [BTC Data] ÙØ´Ù„ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ†."); exit(1)
    # Ø¥Ø¶Ø§ÙØ© Ù…ÙŠØ²Ø© Ø¹ÙˆØ§Ø¦Ø¯ Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† Ù…Ø¨Ø§Ø´Ø±Ø© Ù‡Ù†Ø§
    btc_data_cache['btc_log_returns'] = np.log(btc_data_cache['close'] / btc_data_cache['close'].shift(1))
    btc_data_cache.dropna(inplace=True) # ØªÙ†Ø¸ÙŠÙ Ø£ÙŠ NaNs Ø¨Ø¹Ø¯ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¹ÙˆØ§Ø¦Ø¯

def calculate_features(df: pd.DataFrame, btc_df: pd.DataFrame) -> pd.DataFrame:
    df_calc = df.copy()

    # --- Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø³Ø¹Ø± ÙˆØ§Ù„Ø­Ø¬Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---
    df_calc['log_return_1_period'] = np.log(df_calc['close'] / df_calc['close'].shift(1))
    df_calc['log_return_2_period'] = np.log(df_calc['close'] / df_calc['close'].shift(2))
    df_calc['log_return_5_period'] = np.log(df_calc['close'] / df_calc['close'].shift(5))
    df_calc['volume_change'] = df_calc['volume'].diff() / df_calc['volume'].shift(1)
    df_calc['relative_volume'] = df_calc['volume'] / (df_calc['volume'].rolling(window=VOL_MA_PERIOD, min_periods=1).mean() + 1e-9)

    # --- Ù…ÙŠØ²Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙ†ÙŠ ---
    # ATR
    high_low = df_calc['high'] - df_calc['low']
    high_close = (df_calc['high'] - df_calc['close'].shift()).abs()
    low_close = (df_calc['low'] - df_calc['close'].shift()).abs()
    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
    df_calc['atr'] = tr.ewm(span=ATR_PERIOD, adjust=False).mean()

    # RSI (ÙØªØ±Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©)
    for period in RSI_PERIODS:
        delta = df_calc['close'].diff()
        gain = delta.clip(lower=0).ewm(com=period - 1, adjust=False).mean()
        loss = -delta.clip(upper=0).ewm(com=period - 1, adjust=False).mean()
        # ØªØ¬Ù†Ø¨ Ø§Ù„Ù‚Ø³Ù…Ø© Ø¹Ù„Ù‰ ØµÙØ± Ø£Ùˆ Ù‚ÙŠÙ… ØµØºÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
        rs = gain / loss.replace(0, 1e-9)
        df_calc[f'rsi_{period}'] = 100 - (100 / (1 + rs))

    # MACD
    ema_fast = df_calc['close'].ewm(span=MACD_FAST, adjust=False).mean()
    ema_slow = df_calc['close'].ewm(span=MACD_SLOW, adjust=False).mean()
    macd_line = ema_fast - ema_slow
    macd_signal_line = macd_line.ewm(span=MACD_SIGNAL, adjust=False).mean()
    df_calc['macd_hist'] = macd_line - macd_signal_line

    # EMA Crossings / Deviations
    df_calc['ema_fast_trend'] = df_calc['close'].ewm(span=EMA_TREND_PERIODS['fast'], adjust=False).mean()
    df_calc['ema_mid_trend'] = df_calc['close'].ewm(span=EMA_TREND_PERIODS['mid'], adjust=False).mean()
    df_calc['ema_slow_trend'] = df_calc['close'].ewm(span=EMA_TREND_PERIODS['slow'], adjust=False).mean()

    df_calc['price_vs_ema_fast'] = (df_calc['close'] / df_calc['ema_fast_trend']) - 1
    df_calc['price_vs_ema_mid'] = (df_calc['close'] / df_calc['ema_mid_trend']) - 1
    df_calc['price_vs_ema_slow'] = (df_calc['close'] / df_calc['ema_slow_trend']) - 1
    df_calc['ema_fast_vs_mid'] = (df_calc['ema_fast_trend'] / df_calc['ema_mid_trend']) - 1
    df_calc['ema_mid_vs_slow'] = (df_calc['ema_mid_trend'] / df_calc['ema_slow_trend']) - 1


    # --- Ù…ÙŠØ²Ø§Øª Ø§Ù„ØªÙ‚Ù„Ø¨ (Volatility Features) ---
    for period in ROLLING_VOLATILITY_PERIODS:
        df_calc[f'rolling_vol_{period}'] = df_calc['log_return_1_period'].rolling(window=period).std()

    # --- Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ø£ØµÙˆÙ„ (Ù…Ø¹ BTC) ---
    # Ø¯Ù…Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª BTC Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ù…Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ø²Ù…Ù†ÙŠ
    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„ÙÙ‡Ø±Ø³ Ù…ØªØ³Ù‚ Ø¨ÙŠÙ† df_calc Ùˆ btc_df
    temp_df = pd.merge(df_calc[['log_return_1_period']], btc_df[['btc_log_returns']], left_index=True, right_index=True, how='left')
    
    for period in CORR_PERIODS:
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ù…ØªØ¯Ø­Ø±Ø¬.fillna(0) Ù…Ù‡Ù… Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ÙØªØ±Ø§Øª Ø§Ù„ØªÙŠ Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙÙŠÙ‡Ø§ Ø§Ø±ØªØ¨Ø§Ø·
        df_calc[f'btc_correlation_{period}'] = temp_df['log_return_1_period'].rolling(window=period).corr(temp_df['btc_log_returns']).fillna(0)

    # --- Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙˆÙ‚Øª ---
    df_calc['hour_of_day'] = df_calc.index.hour.astype('category')
    df_calc['day_of_week'] = df_calc.index.dayofweek.astype('category')
    df_calc['day_of_month'] = df_calc.index.day.astype('category')
    df_calc['month'] = df_calc.index.month.astype('category')

    # Ù…ÙŠØ²Ø§Øª Ø¯ÙˆØ±ÙŠØ© Ù„Ù„ÙˆÙ‚Øª (Sine/Cosine)
    df_calc['hour_sin'] = np.sin(2 * np.pi * df_calc.index.hour / 24)
    df_calc['hour_cos'] = np.cos(2 * np.pi * df_calc.index.hour / 24)
    df_calc['day_of_week_sin'] = np.sin(2 * np.pi * df_calc.index.dayofweek / 7)
    df_calc['day_of_week_cos'] = np.cos(2 * np.pi * df_calc.index.dayofweek / 7)


    # ØªÙ†Ø¸ÙŠÙ Ø£ÙŠ Ø£Ø¹Ù…Ø¯Ø© Ù…Ø¤Ù‚ØªØ© Ø§Ø³ØªØ®Ø¯Ù…Øª Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙŠØ²Ø§Øª
    df_calc = df_calc.drop(columns=[col for col in ['ema_fast_trend', 'ema_mid_trend', 'ema_slow_trend'] if col in df_calc.columns], errors='ignore')

    return df_calc.dropna() # Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ ØµÙÙˆÙ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ NaN Ø¨Ø¹Ø¯ Ø­Ø³Ø§Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª

def get_triple_barrier_labels(prices: pd.Series, atr: pd.Series) -> pd.Series:
    labels = pd.Series(0, index=prices.index, dtype=int) # ØªØ£ÙƒØ¯ Ù…Ù† Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª int
    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† prices Ùˆ atr Ù„Ù‡Ø§ Ù†ÙØ³ Ø§Ù„ÙÙ‡Ø±Ø³
    prices = prices.copy()
    atr = atr.copy()

    # Ø§Ø³ØªØ®Ø¯Ø§Ù… to_numpy() Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
    prices_np = prices.to_numpy()
    atr_np = atr.to_numpy()

    for i in tqdm(range(len(prices) - MAX_HOLD_PERIOD), desc="Labeling", leave=False):
        entry_price = prices_np[i]
        current_atr = atr_np[i]

        if np.isnan(current_atr) or current_atr <= 0: # Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ATR = 0
            continue

        upper_barrier = entry_price + (current_atr * TP_ATR_MULTIPLIER)
        lower_barrier = entry_price - (current_atr * SL_ATR_MULTIPLIER)

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ø®ØªØ±Ø§Ù‚ Ø§Ù„Ø­Ø§Ø¬Ø² Ø£Ùˆ Ø§Ù†ØªÙ‡Ø§Ø¡ ÙØªØ±Ø© Ø§Ù„Ø§Ø­ØªÙØ§Ø¸
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø§ÙØ°Ø© Ø¹Ù„Ù‰ numpy array Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
        window_prices = prices_np[i + 1 : i + 1 + MAX_HOLD_PERIOD]

        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ø®ØªØ±Ø§Ù‚ Ø§Ù„Ø­Ø§Ø¬Ø² Ø§Ù„Ø¹Ù„ÙˆÙŠ (Ø±Ø¨Ø­)
        if (window_prices >= upper_barrier).any():
            labels.iloc[i] = 1 # Profit
            continue
        
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ø®ØªØ±Ø§Ù‚ Ø§Ù„Ø­Ø§Ø¬Ø² Ø§Ù„Ø³ÙÙ„ÙŠ (Ø®Ø³Ø§Ø±Ø©)
        if (window_prices <= lower_barrier).any():
            labels.iloc[i] = -1 # Loss
            continue
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ø®ØªØ±Ø§Ù‚ Ø£ÙŠ Ø­Ø§Ø¬Ø² Ø®Ù„Ø§Ù„ MAX_HOLD_PERIODØŒ ÙŠØ¨Ù‚Ù‰ 0 (Ù„Ø§ Ø´ÙŠØ¡)
        # Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¹ÙŠÙŠÙ†Ù‡ ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©

    return labels

def prepare_data_for_ml(df: pd.DataFrame, btc_df: pd.DataFrame, symbol: str) -> Optional[Tuple[pd.DataFrame, pd.Series, List[str], List[str]]]:
    logger.info(f"â„¹ï¸ [ML Prep] Preparing data for {symbol}...")
    df_featured = calculate_features(df, btc_df)

    # Ù‚Ø¨Ù„ Ø­Ø³Ø§Ø¨ Ø§Ù„Ù‡Ø¯ÙØŒ ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† 'atr' Ù„ÙŠØ³ ÙØ§Ø±ØºÙ‹Ø§
    if 'atr' not in df_featured.columns or df_featured['atr'].isnull().all():
        logger.warning(f"âš ï¸ [ML Prep] 'atr' feature is missing or all NaN for {symbol}. Cannot generate labels.")
        return None

    # ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© Ø§Ù„Ø­ÙˆØ§Ø¬Ø² Ø§Ù„Ø«Ù„Ø§Ø«ÙŠØ© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù‡Ø¯Ù (-1, 0, 1)
    df_featured['target'] = get_triple_barrier_labels(df_featured['close'], df_featured['atr'])

    # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ© ÙˆØ§Ù„ÙØ¦ÙˆÙŠØ©
    # Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø°Ù‡ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠÙ‹Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø­Ø³ÙˆØ¨Ø©
    
    numerical_features = [
        'log_return_1_period', 'log_return_2_period', 'log_return_5_period',
        'volume_change', 'relative_volume', 'atr', 'macd_hist',
        'price_vs_ema_fast', 'price_vs_ema_mid', 'price_vs_ema_slow',
        'ema_fast_vs_mid', 'ema_mid_vs_slow',
        'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos'
    ]
    # Ø¥Ø¶Ø§ÙØ© Ù…ÙŠØ²Ø§Øª RSI Ùˆ Rolling Volatility Ùˆ BTC Correlation Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠÙ‹Ø§
    for period in RSI_PERIODS:
        numerical_features.append(f'rsi_{period}')
    for period in ROLLING_VOLATILITY_PERIODS:
        numerical_features.append(f'rolling_vol_{period}')
    for period in CORR_PERIODS:
        numerical_features.append(f'btc_correlation_{period}')

    categorical_features_for_lgbm = [
        'hour_of_day', 'day_of_week', 'day_of_month', 'month'
    ]

    # ØªØµÙÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯Ù‡Ø§ Ø¨Ø¹Ø¯ Ø¹Ù…Ù„ÙŠØ© Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙŠØ²Ø§Øª ÙˆØ¥Ø²Ø§Ù„Ø© NaNs
    final_feature_columns = [col for col in numerical_features + categorical_features_for_lgbm if col in df_featured.columns]

    df_cleaned = df_featured.dropna(subset=final_feature_columns + ['target']).copy()

    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„ØªÙŠ Ù„Ø§ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹ÙŠÙ†Ø§Øª ÙƒØ§ÙÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ
    for cat_col in categorical_features_for_lgbm:
        if cat_col in df_cleaned.columns:
            # Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø¥Ù„Ù‰ Ù†ÙˆØ¹ 'category' Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† ÙƒØ°Ù„Ùƒ Ø¨Ø§Ù„ÙØ¹Ù„
            df_cleaned[cat_col] = df_cleaned[cat_col].astype('category')
            # Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ ÙØ¦Ø§Øª Ù„Ø§ ØªØ¸Ù‡Ø± ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ
            df_cleaned[cat_col] = df_cleaned[cat_col].cat.remove_unused_categories()

    if df_cleaned.empty or df_cleaned['target'].nunique() < 2:
        logger.warning(f"âš ï¸ [ML Prep] Data for {symbol} is empty or has less than 2 target classes after feature engineering. Skipping.")
        return None

    # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ù‚ÙŠÙ… Ø§Ù„Ù‡Ø¯Ù Ø¥Ù„Ù‰ 0, 1, 2 Ø¥Ø°Ø§ ÙƒØ§Ù†Øª -1, 0, 1ØŒ Ù„Ø£Ù† LightGBM ÙŠÙØ¶Ù„ 0-indexed Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯
    # Mapping: -1 -> 0 (Loss), 0 -> 1 (Neutral), 1 -> 2 (Profit)
    df_cleaned['target_mapped'] = df_cleaned['target'].map({-1: 0, 0: 1, 1: 2})
    
    # ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ© Ù…Ø¹Ø±ÙØ© Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ Ù„Ù€ LightGBM
    # LightGBM ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ 'category' dtype Ù…Ø¨Ø§Ø´Ø±Ø©
    for col in categorical_features_for_lgbm:
        if col in df_cleaned.columns:
            df_cleaned[col] = df_cleaned[col].astype('category')

    X = df_cleaned[final_feature_columns]
    y = df_cleaned['target_mapped'] # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ù…Ø¹Ø¯Ù„

    logger.info(f"ğŸ“Š [ML Prep] Target distribution for {symbol} (mapped: 0,1,2):\n{y.value_counts(normalize=True)}")
    
    return X, y, numerical_features, categorical_features_for_lgbm # ØªÙ…Ø±ÙŠØ± Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ù†ÙØµÙ„Ø©

# --- Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª LightGBM ---
def train_with_walk_forward_validation(X: pd.DataFrame, y: pd.Series, numerical_features: List[str], categorical_features: List[str]) -> Tuple[Optional[Any], Optional[Any], Optional[Dict[str, Any]]]:
    logger.info("â„¹ï¸ [ML Train] Starting training with Walk-Forward Validation...")
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… TimeSeriesSplit Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ Ù…Ø¹ Ø¹Ø¯Ø¯ ÙƒØ§ÙÙ Ù…Ù† Ø§Ù„Ø§Ù†Ù‚Ø³Ø§Ù…Ø§Øª.
    # n_splits=5 ÙŠØ¹Ù†ÙŠ 5 Ø£Ø¶Ø¹Ø§ÙØŒ ÙƒÙ„ Ø¶Ø¹Ù ÙŠØ³ØªØ®Ø¯Ù… Ø¬Ø²Ø¡Ù‹Ø§ Ø£ÙƒØ¨Ø± Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨.
    # max_train_size ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ­Ø¯ Ù…Ù† Ø­Ø¬Ù… Ù†Ø§ÙØ°Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    tscv = TimeSeriesSplit(n_splits=5) 
    
    final_model, final_scaler = None, None
    fold_metrics = [] # Ù„ØªØ®Ø²ÙŠÙ† Ù…Ù‚Ø§ÙŠÙŠØ³ ÙƒÙ„ Ø¶Ø¹Ù

    # Ù…Ø¹Ù„Ù…Ø§Øª LightGBM Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø© Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ø¯Ø§Ø¡ "Ù‚ÙŠØ§Ø³ÙŠ"
    lgbm_params = {
        'objective': 'multiclass', # Ù„Ø£Ù†Ù‡ Ù„Ø¯ÙŠÙ†Ø§ 3 ÙØ¦Ø§Øª (-1, 0, 1) Ø§Ù„ØªÙŠ ØªÙ… ØªØ¹ÙŠÙŠÙ†Ù‡Ø§ Ø¥Ù„Ù‰ (0, 1, 2)
        'num_class': 3,            # 3 ÙØ¦Ø§Øª
        'metric': 'multi_logloss', # Ù…Ù‚ÙŠØ§Ø³ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯
        'boosting_type': 'gbdt',
        'num_leaves': 63,          # Ù‚ÙŠÙ…Ø© Ø¬ÙŠØ¯Ø©ØŒ ØªØ³Ù…Ø­ Ø¨ØªØ¹Ù‚ÙŠØ¯ Ù…Ø¹Ù‚ÙˆÙ„
        'max_depth': -1,           # Ø§Ù„Ø³Ù…Ø§Ø­ Ù„Ù€ num_leaves Ø¨Ø§Ù„ØªØ­ÙƒÙ…
        'learning_rate': 0.02,     # Ù…Ø¹Ø¯Ù„ ØªØ¹Ù„Ù… ØµØºÙŠØ±
        'feature_fraction': 0.7,   # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ù…Ù† 70% Ù…Ù† Ø§Ù„Ù…ÙŠØ²Ø§Øª
        'bagging_fraction': 0.7,   # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ù…Ù† 70% Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        'bagging_freq': 1,         # ØªÙƒØ±Ø§Ø± Ø§Ù„Ù€ bagging
        'lambda_l1': 0.1,          # ØªÙ‚ÙŠÙŠØ¯ L1
        'lambda_l2': 0.1,          # ØªÙ‚ÙŠÙŠØ¯ L2
        'min_child_samples': 200,  # Ø¹Ø¯Ø¯ ÙƒØ¨ÙŠØ± ÙÙŠ Ø§Ù„Ø¹Ù‚Ø¯Ø© Ø§Ù„Ø·Ø±ÙÙŠØ© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªØ¬Ø§ÙˆØ²
        'verbose': -1,             # Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
        'n_jobs': -1,              # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ÙˆÙ‰
        'seed': 42,                # Ù„Ù„ØªØ­Ù‚Ù‚
        'is_unbalance': True       # Ù…Ù‡Ù… Ù„Ù„ÙØ¦Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØªÙˆØ§Ø²Ù†Ø©
    }

    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ© Ù„Ù€ LightGBM (ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† ÙƒÙ€ 'category' dtype)
    lgbm_categorical_features = [col for col in categorical_features if col in X.columns]
    
    for i, (train_index, test_index) in enumerate(tscv.split(X)):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¹ÙŠÙ†Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© ÙØ±Ø¹ÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨
        if X_train.empty or X_test.empty or y_train.empty or y_test.empty:
            logger.warning(f"âš ï¸ [ML Train] Fold {i+1} has empty train/test sets. Skipping.")
            continue
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø£ÙƒØ«Ø± Ù…Ù† ÙØ¦Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙŠ y_train Ùˆ y_test
        if y_train.nunique() < 2 or y_test.nunique() < 2:
            logger.warning(f"âš ï¸ [ML Train] Fold {i+1} train/test target has less than 2 classes. Skipping.")
            continue

        # ØªØ·Ø¨ÙŠÙ‚ StandardScaler ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
        scaler = StandardScaler()
        # Fit scaler only on numerical features of the training set
        scaler.fit(X_train[numerical_features])
        
        # Transform both train and test sets, retaining original column names
        X_train_scaled = X_train.copy()
        X_test_scaled = X_test.copy()

        X_train_scaled[numerical_features] = scaler.transform(X_train[numerical_features])
        X_test_scaled[numerical_features] = scaler.transform(X_test[numerical_features])
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ© Ø¥Ù„Ù‰ Ù†ÙˆØ¹ 'category' Ø¨Ø¹Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© (Ø¥Ù† Ù„Ù… ØªÙƒÙ† ÙƒØ°Ù„Ùƒ)
        for col in lgbm_categorical_features:
            if col in X_train_scaled.columns:
                X_train_scaled[col] = X_train_scaled[col].astype('category')
            if col in X_test_scaled.columns:
                X_test_scaled[col] = X_test_scaled[col].astype('category')

        # ØªØ¯Ø±ÙŠØ¨ LightGBM
        model = lgb.train(
            lgbm_params,
            lgb.Dataset(X_train_scaled, y_train, categorical_feature=lgbm_categorical_features),
            num_boost_round=2000, # Ø¹Ø¯Ø¯ ÙƒØ¨ÙŠØ± Ù„Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„ØªÙˆÙ‚Ù Ø§Ù„Ù…Ø¨ÙƒØ±
            valid_sets=[lgb.Dataset(X_test_scaled, y_test, categorical_feature=lgbm_categorical_features)],
            callbacks=[lgb.early_stopping(stopping_rounds=150, verbose=False)], # ØªÙˆÙ‚Ù Ù…Ø¨ÙƒØ±
            # feature_name = list(X.columns) # Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©
        )
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ… Ù„Ù„Ø¶Ø¹Ù Ø§Ù„Ø­Ø§Ù„ÙŠ
        y_pred = model.predict(X_test_scaled, num_iteration=model.best_iteration)
        y_pred_labels = np.argmax(y_pred, axis=1) # Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ù…Ù† Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª

        # Ù…Ù‚Ø§ÙŠÙŠØ³ Ù„ÙƒÙ„ Ø¶Ø¹Ù
        acc = accuracy_score(y_test, y_pred_labels)
        # Precision Ùˆ Recall Ùˆ F1 Ù„ÙƒÙ„ ÙØ¦Ø© Ø¹Ù„Ù‰ Ø­Ø¯Ø©
        # Ù†Ø±ÙƒØ² Ø¹Ù„Ù‰ ÙØ¦Ø© Ø§Ù„Ø±Ø¨Ø­ (mapped to 2) Ø£Ùˆ ÙØ¦Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø© (mapped to 0)
        precision_profit = precision_score(y_test, y_pred_labels, labels=[2], average='macro', zero_division=0)
        recall_profit = recall_score(y_test, y_pred_labels, labels=[2], average='macro', zero_division=0)
        f1_profit = f1_score(y_test, y_pred_labels, labels=[2], average='macro', zero_division=0)
        
        # ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¶Ø§ÙØ© Ù…Ù‚Ø§ÙŠÙŠØ³ Ù„ÙØ¦Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª
        # precision_loss = precision_score(y_test, y_pred_labels, labels=[0], average='macro', zero_division=0)

        fold_metrics.append({
            'accuracy': acc,
            'precision_profit': precision_profit,
            'recall_profit': recall_profit,
            'f1_profit': f1_profit,
            'best_iteration': model.best_iteration
        })

        logger.info(f"--- Fold {i+1}: Accuracy: {acc:.4f}, Precision (Profit): {precision_profit:.4f}, Recall (Profit): {recall_profit:.4f}, F1 (Profit): {f1_profit:.4f}, Best Iteration: {model.best_iteration}")
        
        final_model, final_scaler = model, scaler # Ø­ÙØ¸ Ø¢Ø®Ø± Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…Ø¯Ø±Ø¨

    if not final_model or not final_scaler:
        logger.error("âŒ [ML Train] Training failed, no model was created or all folds skipped.")
        return None, None, None

    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ù„Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¹Ø¨Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¶Ø¹Ø§Ù
    avg_accuracy = np.mean([m['accuracy'] for m in fold_metrics])
    avg_precision_profit = np.mean([m['precision_profit'] for m in fold_metrics])
    avg_recall_profit = np.mean([m['recall_profit'] for m in fold_metrics])
    avg_f1_profit = np.mean([m['f1_profit'] for m in fold_metrics])

    final_metrics = {
        'avg_accuracy': avg_accuracy,
        'avg_precision_profit': avg_precision_profit,
        'avg_recall_profit': avg_recall_profit,
        'avg_f1_profit': avg_f1_profit,
        'num_samples_trained': len(X),
        'num_folds': len(fold_metrics)
    }

    metrics_log_str = ', '.join([f"{k}: {v:.4f}" for k, v in final_metrics.items() if isinstance(v, (int, float))])
    logger.info(f"ğŸ“Š [ML Train] Average Walk-Forward Performance: {metrics_log_str}")
    
    # Ù…Ù„Ø§Ø­Ø¸Ø©: ÙŠØªÙ… Ø­ÙØ¸ Ø¢Ø®Ø± Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…Ø¯Ø±Ø¨. Ù„Ù†Ù‡Ø¬ Ø£ÙƒØ«Ø± Ù‚ÙˆØ©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù†Ù‡Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙƒØ¨Ø±
    # Ø£Ùˆ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ØŒ ÙˆÙ„ÙƒÙ† Ù‡Ø°Ø§ ÙŠØ¹Ù‚Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©. Ù„Ù‡Ø°Ø§ Ø§Ù„Ù†Ù‡Ø¬ "Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ"ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø¢Ø®Ø± Ù†Ù…ÙˆØ°Ø¬ ÙƒÙ€ "Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬"
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø£Ø¯Ø§Ø¤Ù‡ Ù‡Ùˆ Ø§Ù„Ø£ÙØ¶Ù„ Ø¹Ø¨Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¶Ø¹Ø§Ù.
    # Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ Ø­Ù‚Ù‚ Ø£ÙØ¶Ù„ Ù…Ù‚ÙŠØ§Ø³ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¨ÙŠÙ† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¶Ø¹Ø§Ù.
    # Ù„Ù„ØªØ¨Ø³ÙŠØ·ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø¢Ø®Ø± Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¯Ø±Ø¨ Ù‡Ù†Ø§.

    return final_model, final_scaler, final_metrics

def save_ml_model_to_db(model_bundle: Dict[str, Any], model_name: str, metrics: Dict[str, Any]):
    logger.info(f"â„¹ï¸ [DB Save] Saving model bundle '{model_name}'...")
    try:
        model_binary = pickle.dumps(model_bundle)
        metrics_json = json.dumps(metrics)
        with conn.cursor() as db_cur:
            db_cur.execute("""
                INSERT INTO ml_models (model_name, model_data, trained_at, metrics) 
                VALUES (%s, %s, NOW(), %s) ON CONFLICT (model_name) DO UPDATE SET 
                model_data = EXCLUDED.model_data, trained_at = NOW(), metrics = EXCLUDED.metrics;
            """, (model_name, model_binary, metrics_json))
        conn.commit()
        logger.info(f"âœ… [DB Save] Model bundle '{model_name}' saved successfully.")
    except Exception as e:
        logger.error(f"âŒ [DB Save] Error saving model bundle: {e}", exc_info=True); conn.rollback()

def send_telegram_message(text: str):
    if not TELEGRAM_TOKEN or not CHAT_ID:
        logger.warning("âš ï¸ [Telegram] TELEGRAM_TOKEN or CHAT_ID not configured. Skipping Telegram message.")
        return
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    try:
        response = requests.post(url, json={'chat_id': CHAT_ID, 'text': text, 'parse_mode': 'Markdown'}, timeout=10)
        response.raise_for_status() # ÙŠØ«ÙŠØ± Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ù„Ø£Ø®Ø·Ø§Ø¡ HTTP
        logger.info("âœ… [Telegram] Telegram message sent successfully.")
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ [Telegram] Failed to send Telegram message: {e}", exc_info=True)
    except Exception as e:
        logger.error(f"âŒ [Telegram] An unexpected error occurred while sending Telegram message: {e}", exc_info=True)


# --- Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø¹Ù…Ù„ ÙÙŠ Ø®ÙŠØ· Ù…Ù†ÙØµÙ„ ---
def run_training_job():
    logger.info(f"ğŸš€ Starting ADVANCED ML model training job ({BASE_ML_MODEL_NAME})...")
    init_db()
    get_binance_client()
    fetch_and_cache_btc_data()
    symbols_to_train = get_validated_symbols(filename='crypto_list.txt')
    if not symbols_to_train:
        logger.critical("âŒ [Main] No valid symbols found. Exiting training job.")
        if conn: conn.close()
        return
        
    send_telegram_message(f"ğŸš€ *{BASE_ML_MODEL_NAME} Training Started*\nWill train models for {len(symbols_to_train)} symbols.")
    
    successful_models, failed_models = 0, 0
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… tqdm Ù„Ø¹Ø±Ø¶ Ø´Ø±ÙŠØ· ØªÙ‚Ø¯Ù… Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø±Ù…ÙˆØ²
    for symbol in tqdm(symbols_to_train, desc="Overall Symbol Training"):
        logger.info(f"\n--- â³ [Main] Starting model training for {symbol} ---")
        try:
            df_hist = fetch_historical_data(symbol, SIGNAL_GENERATION_TIMEFRAME, DATA_LOOKBACK_DAYS_FOR_TRAINING)
            if df_hist is None or df_hist.empty:
                logger.warning(f"âš ï¸ [Main] No historical data for {symbol}, skipping."); failed_models += 1; continue
            
            # ØªÙ…Ø±ÙŠØ± Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ© ÙˆØ§Ù„ÙØ¦ÙˆÙŠØ© Ø¥Ù„Ù‰ prepare_data_for_ml
            prepared_data = prepare_data_for_ml(df_hist, btc_data_cache, symbol)
            if prepared_data is None:
                failed_models += 1; continue
            X, y, numerical_features, categorical_features = prepared_data
            
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† y ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚ÙŠÙ… ØµØ§Ù„Ø­Ø©
            if y.isnull().any() or y.nunique() < 2:
                logger.warning(f"âš ï¸ [Main] Target for {symbol} contains NaN or has less than 2 unique classes after preparation. Skipping.")
                failed_models += 1
                continue

            # ØªÙ…Ø±ÙŠØ± Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            training_result = train_with_walk_forward_validation(X, y, numerical_features, categorical_features)
            if not all(res is not None for res in training_result): # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ù„ÙŠØ³Øª None
                 logger.warning(f"âš ï¸ [Main] Training for {symbol} resulted in None values. Skipping."); failed_models += 1; continue
            
            final_model, final_scaler, model_metrics = training_result
            
            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ø±Ø¨Ø­ (precision_profit) ÙˆÙ„ÙŠØ³ ÙÙ‚Ø· Ø§Ù„Ø¯Ù‚Ø©
            # ÙŠÙ…ÙƒÙ† ØªØ¹Ø¯ÙŠÙ„ Ù‡Ø°Ø§ Ø§Ù„Ø¹ØªØ¨Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¯Ù‰ Ø¹Ø¯ÙˆØ§Ù†ÙŠØ© Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØªÙƒ
            if final_model and final_scaler and model_metrics.get('avg_precision_profit', 0) > 0.40: # Ø¹ØªØ¨Ø© Ø£Ø¹Ù„Ù‰ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ
                model_bundle = {
                    'model': final_model,
                    'scaler': final_scaler,
                    'feature_names': list(X.columns), # Ø­ÙØ¸ Ø£Ø³Ù…Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªÙ†Ø§Ø³Ù‚
                    'numerical_features': numerical_features,
                    'categorical_features': categorical_features
                }
                model_name = f"{BASE_ML_MODEL_NAME}_{symbol}"
                save_ml_model_to_db(model_bundle, model_name, model_metrics)
                successful_models += 1
                send_telegram_message(f"âœ… *Model Trained for {symbol}*\n_Avg Precision (Profit): {model_metrics['avg_precision_profit']:.4f}_")
            else:
                logger.warning(f"âš ï¸ [Main] Model for {symbol} did not meet performance criteria (Avg Precision Profit < 0.40). Discarding."); failed_models += 1
        except Exception as e:
            logger.critical(f"âŒ [Main] A fatal error occurred for {symbol}: {e}", exc_info=True); failed_models += 1
        time.sleep(1) # ØªØ£Ø®ÙŠØ± Ù‚ØµÙŠØ± Ø¨ÙŠÙ† ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø±Ù…ÙˆØ²

    completion_message = (f"âœ… *{BASE_ML_MODEL_NAME} Training Finished*\n"
                        f"- Successfully trained: {successful_models} models\n"
                        f"- Failed/Discarded: {failed_models} models\n"
                        f"- Total symbols: {len(symbols_to_train)}")
    send_telegram_message(completion_message)
    logger.info(completion_message)

    if conn:
        conn.close()
        logger.info("ğŸ‘‹ [Main] Database connection closed.")
    logger.info("ğŸ‘‹ [Main] ML training job finished.")

# --- Ø¥Ø¶Ø§ÙØ© Ø®Ø§Ø¯Ù… Flask Ù„Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Render ---
app = Flask(__name__)

@app.route('/')
def health_check():
    """Endpoint for Render health checks."""
    return "ML Trainer service is running and healthy.", 200

if __name__ == "__main__":
    # Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙŠ Ø®ÙŠØ· Ù…Ù†ÙØµÙ„ Ø­ØªÙ‰ Ù„Ø§ ØªÙ…Ù†Ø¹ Ø§Ù„Ø®Ø§Ø¯Ù… Ù…Ù† Ø§Ù„Ø¹Ù…Ù„
    training_thread = Thread(target=run_training_job)
    training_thread.daemon = True # Ø³ÙŠØªÙ… Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø®ÙŠØ· Ø¹Ù†Ø¯ Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    training_thread.start()
    
    # ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨
    port = int(os.environ.get("PORT", 10000))
    logger.info(f"ğŸŒ Starting web server on port {port} to keep the service alive...")
    # debug=True Ù„Ø§ ÙŠÙ†ØµØ­ Ø¨Ù‡ ÙÙŠ Ø¨ÙŠØ¦Ø§Øª Ø§Ù„Ø¥Ù†ØªØ§Ø¬ØŒ Ù‚Ù… Ø¨Ø¥Ø²Ø§Ù„ØªÙ‡ Ø¹Ù†Ø¯ Ø§Ù„Ù†Ø´Ø± Ø§Ù„ÙØ¹Ù„ÙŠ
    app.run(host='0.0.0.0', port=port)