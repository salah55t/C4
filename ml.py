import time
import os
import json
import logging
import requests
import numpy as np
import pandas as pd
import pandas_ta as ta # <-- Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
import psycopg2
import pickle
import lightgbm as lgb
from psycopg2.extras import RealDictCursor
from binance.client import Client
from datetime import datetime, timedelta
from decouple import config
from typing import List, Dict, Optional, Any, Tuple
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
from flask import Flask
from threading import Thread
from multiprocessing import Pool, cpu_count, Manager

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (Logging) ----------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ml_trainer_optimized.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('OptimizedCryptoMLTrainer')

# ---------------------- ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© ----------------------
try:
    API_KEY: str = config('BINANCE_API_KEY')
    API_SECRET: str = config('BINANCE_API_SECRET')
    DB_URL: str = config('DATABASE_URL')
    TELEGRAM_TOKEN: Optional[str] = config('TELEGRAM_BOT_TOKEN', default=None)
    CHAT_ID: Optional[str] = config('TELEGRAM_CHAT_ID', default=None)
except Exception as e:
     logger.critical(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {e}")
     exit(1)

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø«ÙˆØ§Ø¨Øª (Ù…Ø¹Ø¯Ù„Ø© Ù„Ù„Ø³Ø±Ø¹Ø©) ----------------------
BASE_ML_MODEL_NAME: str = 'LightGBM_Crypto_Predictor_V9_Optimized'
SIGNAL_TIMEFRAME: str = '15m'
DATA_LOOKBACK_DAYS: int = 180 # <-- ØªÙ‚Ù„ÙŠÙ„ ÙØªØ±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¬Ù„Ø¨ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
BTC_SYMBOL = 'BTCUSDT'

# --- Ù…Ø¹Ù„Ù…Ø§Øª Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø­Ø§Ø¬Ø² Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ ---
TP_ATR_MULTIPLIER: float = 1.8
SL_ATR_MULTIPLIER: float = 1.2
MAX_HOLD_PERIOD: int = 24

# --- Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù„Ù…ÙŠØ© Ù…Ø´ØªØ±ÙƒØ© Ø¨ÙŠÙ† Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª ---
# ÙŠØªÙ… ØªÙ…Ø±ÙŠØ±Ù‡Ø§ Ø¥Ù„Ù‰ ÙƒÙ„ Ø¹Ù…Ù„ÙŠØ© Ù„ØªØ¬Ù†Ø¨ Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ù†Ø´Ø§Ø¦Ù‡Ø§
manager = Manager()
btc_data_cache = manager.dict()

# --- Ø¯ÙˆØ§Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ ---
# Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø³ÙŠØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¤Ù‡Ø§ Ø¯Ø§Ø®Ù„ ÙƒÙ„ Ø¹Ù…Ù„ÙŠØ© Ù…ØªÙˆØ§Ø²ÙŠØ©
def get_db_connection():
    """Ø¥Ù†Ø´Ø§Ø¡ Ø§ØªØµØ§Ù„ Ø¬Ø¯ÙŠØ¯ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ÙƒÙ„ Ø¹Ù…Ù„ÙŠØ©."""
    return psycopg2.connect(DB_URL, cursor_factory=RealDictCursor)

def get_binance_client():
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù…ÙŠÙ„ Binance Ø¬Ø¯ÙŠØ¯ Ù„ÙƒÙ„ Ø¹Ù…Ù„ÙŠØ©."""
    return Client(API_KEY, API_SECRET)


# --- Ø¯ÙˆØ§Ù„ Ø¬Ù„Ø¨ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
def fetch_historical_data(client, symbol: str, interval: str, days: int) -> Optional[pd.DataFrame]:
    """Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù…Ù† Binance."""
    try:
        start_str = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d %H:%M:%S")
        klines = client.get_historical_klines(symbol, interval, start_str)
        if not klines: return None
        
        cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
        df = pd.DataFrame(klines, columns=cols + ['_'] * 6)
        
        for col in cols[1:]: df[col] = pd.to_numeric(df[col], errors='coerce')
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        return df[cols[1:]].dropna()
    except Exception as e:
        logger.error(f"âŒ [Data Fetch] Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª {symbol}: {e}")
        return None

def fetch_and_cache_btc_data_global():
    """Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª BTC Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙˆØªØ®Ø²ÙŠÙ†Ù‡Ø§ ÙÙŠ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ù…Ø´ØªØ±Ùƒ."""
    logger.info("â„¹ï¸ [BTC Data] Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† ÙˆØªØ®Ø²ÙŠÙ†Ù‡Ø§...")
    client = get_binance_client()
    btc_df = fetch_historical_data(client, BTC_SYMBOL, SIGNAL_TIMEFRAME, DATA_LOOKBACK_DAYS)
    if btc_df is None:
        logger.critical("âŒ [BTC Data] ÙØ´Ù„ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ†."); exit(1)
        
    btc_df['btc_log_return'] = np.log(btc_df['close'] / btc_df['close'].shift(1))
    btc_data_cache['df'] = btc_df.dropna()
    logger.info("âœ… [BTC Data] ØªÙ… ØªØ®Ø²ÙŠÙ† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† Ø¨Ù†Ø¬Ø§Ø­.")

def engineer_features(df: pd.DataFrame, btc_df: pd.DataFrame) -> pd.DataFrame:
    """
    Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… pandas-ta Ù„Ù„Ø³Ø±Ø¹Ø©.
    """
    df.ta.atr(length=14, append=True) # ÙŠØ­Ø³Ø¨ ATR ÙˆÙŠØ¶ÙŠÙ Ø¹Ù…ÙˆØ¯ 'ATRr_14'
    df.ta.rsi(length=14, append=True) # ÙŠØ­Ø³Ø¨ RSI ÙˆÙŠØ¶ÙŠÙ Ø¹Ù…ÙˆØ¯ 'RSI_14'
    df.ta.macd(fast=12, slow=26, signal=9, append=True) # ÙŠØ­Ø³Ø¨ MACD ÙˆÙŠØ¶ÙŠÙ 3 Ø£Ø¹Ù…Ø¯Ø©
    
    # Ø­Ø³Ø§Ø¨ Ù…ÙŠØ²Ø§Øª Ù…Ø®ØµØµØ© Ø£Ø®Ø±Ù‰
    df['log_return'] = np.log(df['close'] / df['close'].shift(1))
    df['relative_volume'] = df['volume'] / (df['volume'].rolling(window=30, min_periods=1).mean() + 1e-9)
    
    # Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ù…Ø¹ Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ†
    merged = df.join(btc_df['btc_log_return'], how='left')
    df['btc_correlation'] = merged['log_return'].rolling(window=50).corr(merged['btc_log_return']).fillna(0)

    # Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙˆÙ‚Øª
    df['hour'] = df.index.hour
    df['day_of_week'] = df.index.dayofweek
    
    # Ø¥Ø¹Ø§Ø¯Ø© ØªØ³Ù…ÙŠØ© Ø£Ø¹Ù…Ø¯Ø© pandas-ta Ù„Ø£Ø³Ù…Ø§Ø¡ Ø£Ø¨Ø³Ø·
    df.rename(columns={'ATRr_14': 'atr', 'RSI_14': 'rsi', 'MACDh_12_26_9': 'macd_hist'}, inplace=True)
    
    return df.replace([np.inf, -np.inf], np.nan).dropna()


def get_vectorized_labels(prices: pd.Series, atr: pd.Series) -> pd.Series:
    """
    *** Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…ÙˆØ¬Ù‡Ø© (Vectorized) ÙˆØ§Ù„Ø£Ø³Ø±Ø¹ Ø¨ÙƒØ«ÙŠØ± Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù. ***
    """
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø­ÙˆØ§Ø¬Ø²
    upper_barrier = prices + (atr * TP_ATR_MULTIPLIER)
    lower_barrier = prices - (atr * SL_ATR_MULTIPLIER)
    
    # Ø­Ø³Ø§Ø¨ Ø£Ù‚ØµÙ‰ ÙˆØ£Ø¯Ù†Ù‰ Ø³Ø¹Ø± ÙÙŠ Ø§Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ù„ÙƒÙ„ Ù†Ù‚Ø·Ø© Ø²Ù…Ù†ÙŠØ©
    future_highs = prices.shift(-1).rolling(window=MAX_HOLD_PERIOD, min_periods=1).max()
    future_lows = prices.shift(-1).rolling(window=MAX_HOLD_PERIOD, min_periods=1).min()
    
    # ØªØ­Ø¯ÙŠØ¯ Ù…ØªÙ‰ ØªÙ… Ù„Ù…Ø³ ÙƒÙ„ Ø­Ø§Ø¬Ø²
    profit_hit = future_highs >= upper_barrier
    loss_hit = future_lows <= lower_barrier
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    labels = pd.Series(0, index=prices.index, dtype=int)
    labels.loc[profit_hit & ~loss_hit] = 1  # Ø±Ø¨Ø­ ÙÙ‚Ø·
    labels.loc[~profit_hit & loss_hit] = -1 # Ø®Ø³Ø§Ø±Ø© ÙÙ‚Ø·
    
    # Ù„Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ÙŠØªÙ… ÙÙŠÙ‡Ø§ Ù„Ù…Ø³ Ø§Ù„Ø­Ø§Ø¬Ø²ÙŠÙ†ØŒ Ù†Ø®ØªØ§Ø± Ø§Ù„Ø£Ø³Ø¨Ù‚ (Ù‡Ø°Ù‡ Ù†Ø³Ø®Ø© Ù…Ø¨Ø³Ø·Ø© ÙˆØ³Ø±ÙŠØ¹Ø©)
    # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ù…Ù†Ø·Ù‚ Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ù‹Ø§ Ù‡Ù†Ø§ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±ØŒ Ù„ÙƒÙ† Ù‡Ø°Ø§ ÙŠÙƒÙÙŠ Ù„Ù„Ø³Ø±Ø¹Ø©.
    both_hit = profit_hit & loss_hit
    # ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø¨Ø³Ø·Ø©ØŒ Ù†Ø¹Ø·ÙŠ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø®Ø³Ø§Ø±Ø© ÙÙŠ Ø­Ø§Ù„Ø© Ù„Ù…Ø³ Ø§Ù„Ø­Ø§Ø¬Ø²ÙŠÙ†
    labels.loc[both_hit] = -1 
    
    return labels

def train_model(X: pd.DataFrame, y: pd.Series) -> Tuple[Optional[Any], Optional[Any], Optional[Dict[str, Any]]]:
    """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù…Ø¹Ù„Ù…Ø§Øª Ø®ÙÙŠÙØ© Ù„Ù„Ø³Ø±Ø¹Ø©."""
    # Ù…Ø¹Ù„Ù…Ø§Øª Ù…Ø®ÙÙØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø³Ø±ÙŠØ¹
    lgbm_params = {
        'objective': 'multiclass', 'num_class': 3, 'metric': 'multi_logloss',
        'boosting_type': 'gbdt', 
        'n_estimators': 500,      # <-- ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø´Ø¬Ø§Ø±
        'learning_rate': 0.05,    # <-- Ø²ÙŠØ§Ø¯Ø© Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
        'num_leaves': 31,         # <-- ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚
        'seed': 42, 'n_jobs': 1,  # n_jobs=1 Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
        'verbose': -1,
    }

    model = lgb.LGBMClassifier(**lgbm_params)
    
    # Ù„Ø§ Ù†Ø³ØªØ®Ø¯Ù… TimeSeriesSplit Ù‡Ù†Ø§ Ù„ØªØ¨Ø³ÙŠØ· ÙˆØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
    # ÙŠØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ ÙƒØ§Ù…Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ù„Ø±Ù…Ø² Ø§Ù„ÙˆØ§Ø­Ø¯
    # Ù‡Ø°Ø§ Ù…Ù‚Ø¨ÙˆÙ„ Ù„Ø£Ù†Ù†Ø§ Ù†Ø¯Ø±Ø¨ Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ù…Ù†ÙØµÙ„Ø§Ù‹ Ù„ÙƒÙ„ Ø¹Ù…Ù„Ø©
    
    numerical_features = X.select_dtypes(include=np.number).columns.tolist()
    scaler = StandardScaler()
    X.loc[:, numerical_features] = scaler.fit_transform(X[numerical_features])
    
    categorical_features = ['hour', 'day_of_week']
    for col in categorical_features:
        if col in X.columns: X[col] = X[col].astype('category')
            
    model.fit(X, y, categorical_feature=categorical_features)
    
    # Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù†ÙØ³Ù‡Ø§ (Ù„Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø³Ø±ÙŠØ¹ ÙÙ‚Ø·)
    y_pred = model.predict(X)
    accuracy = accuracy_score(y, y_pred)
    precision_profit = precision_score(y, y_pred, labels=[2], average='macro', zero_division=0)
    
    metrics = {
        'in_sample_accuracy': accuracy,
        'precision_for_profit_class': precision_profit,
        'num_samples': len(X)
    }
    
    return model, scaler, metrics

# --- Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… ØªØ´ØºÙŠÙ„Ù‡Ø§ Ù„ÙƒÙ„ Ø¹Ù…Ù„ÙŠØ© ---
def process_symbol(symbol: str):
    """
    Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙŠ ØªÙ‚ÙˆÙ… Ø¨ÙƒØ§Ù…Ù„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ø±Ù…Ø² ÙˆØ§Ø­Ø¯.
    Ø³ÙŠØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¤Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²Ù.
    """
    try:
        logger.info(f"âš™ï¸ [Process] Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© {symbol}...")
        client = get_binance_client()
        conn = get_db_connection()
        
        # Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ù…Ù„Ø© ÙˆØ§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ†
        hist_data = fetch_historical_data(client, symbol, SIGNAL_TIMEFRAME, DATA_LOOKBACK_DAYS)
        if hist_data is None or hist_data.empty:
            logger.warning(f"âš ï¸ [{symbol}] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ§Ø±ÙŠØ®ÙŠØ©.")
            return (symbol, 'No Data', None)

        btc_df_from_cache = btc_data_cache.get('df')
        if btc_df_from_cache is None:
             logger.error(f"âŒ [{symbol}] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª BTC ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©.")
             return (symbol, 'BTC Data Missing', None)
             
        # Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª
        df_featured = engineer_features(hist_data, btc_df_from_cache)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©)
        df_featured['target'] = get_vectorized_labels(df_featured['close'], df_featured['atr'])
        df_featured['target_mapped'] = df_featured['target'].map({-1: 0, 0: 1, 1: 2})
        
        feature_columns = [col for col in df_featured.columns if col in ['atr', 'rsi', 'macd_hist', 'log_return', 'relative_volume', 'btc_correlation', 'hour', 'day_of_week']]
        
        df_cleaned = df_featured.dropna(subset=feature_columns + ['target_mapped'])
        if df_cleaned.empty or df_cleaned['target_mapped'].nunique() < 3:
            logger.warning(f"âš ï¸ [{symbol}] Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ.")
            return (symbol, 'Insufficient Data', None)

        X = df_cleaned[feature_columns]
        y = df_cleaned['target_mapped']

        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model, scaler, metrics = train_model(X, y)
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ Ø­Ù‚Ù‚ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        if model and metrics and metrics.get('precision_for_profit_class', 0) > 0.50:
            model_bundle = {'model': model, 'scaler': scaler, 'feature_names': list(X.columns)}
            model_name = f"{BASE_ML_MODEL_NAME}_{symbol}"
            
            # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            model_binary = pickle.dumps(model_bundle)
            metrics_json = json.dumps(metrics)
            with conn.cursor() as cur:
                cur.execute("""
                    INSERT INTO ml_models (model_name, model_data, metrics) VALUES (%s, %s, %s)
                    ON CONFLICT (model_name) DO UPDATE SET model_data = EXCLUDED.model_data,
                    trained_at = NOW(), metrics = EXCLUDED.metrics;
                """, (model_name, model_binary, metrics_json))
            conn.commit()
            
            logger.info(f"âœ… [{symbol}] ØªÙ… ØªØ¯Ø±ÙŠØ¨ ÙˆØ­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­.")
            return (symbol, 'Success', metrics)
        else:
            logger.warning(f"âš ï¸ [{symbol}] Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù… ÙŠØ­Ù‚Ù‚ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨.")
            return (symbol, 'Low Performance', metrics)
            
    except Exception as e:
        logger.critical(f"âŒ [{symbol}] Ø®Ø·Ø£ ÙØ§Ø¯Ø­ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ù…Ø²: {e}", exc_info=True)
        return (symbol, 'Error', None)
    finally:
        if 'conn' in locals() and conn:
            conn.close()

def send_telegram_notification(text: str):
    """Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø¹Ø§Ø± Ø¥Ù„Ù‰ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…."""
    if not TELEGRAM_TOKEN or not CHAT_ID: return
    try:
        requests.post(f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage",
                      json={'chat_id': CHAT_ID, 'text': text, 'parse_mode': 'Markdown'}, timeout=10)
    except Exception as e:
        logger.error(f"âŒ [Telegram] ÙØ´Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¥Ø´Ø¹Ø§Ø±: {e}")

# --- Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ØªØ¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ© ---
def parallel_training_job():
    logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø­Ø³Ù†Ø© ({BASE_ML_MODEL_NAME})...")
    
    # Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª BTC Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡
    fetch_and_cache_btc_data_global()
    
    try:
        with open('crypto_list.txt', 'r', encoding='utf-8') as f:
            symbols = {s.strip().upper() + 'USDT' for s in f if s.strip()}
    except FileNotFoundError:
        logger.critical("âŒ [Main] Ù…Ù„Ù 'crypto_list.txt' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯."); return

    send_telegram_notification(f"ğŸš€ *Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù…Ø­Ø³Ù† Ù„Ù€ {len(symbols)} Ø¹Ù…Ù„Ø©*...")
    
    # ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª (Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„ Ø§Ù„Ø£Ù†ÙˆÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©)
    num_processes = cpu_count()
    logger.info(f"ğŸ–¥ï¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… {num_processes} Ø¹Ù…Ù„ÙŠØ§Øª Ù…ØªÙˆØ§Ø²ÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨.")
    
    results = []
    with Pool(processes=num_processes) as pool:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… tqdm Ù„Ø¥Ø¸Ù‡Ø§Ø± Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù… Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
        with tqdm(total=len(symbols), desc="Training Symbols") as pbar:
            for result in pool.imap_unordered(process_symbol, symbols):
                results.append(result)
                pbar.update()

    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    successful = sum(1 for r in results if r[1] == 'Success')
    failed = len(symbols) - successful
    
    summary_msg = (f"ğŸ *Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø­Ø³Ù†Ø©*\n"
                   f"- Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù†Ø§Ø¬Ø­Ø©: {successful}\n"
                   f"- Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙØ§Ø´Ù„Ø©/Ø§Ù„Ù…ØªØ¬Ø§Ù‡ÙÙ„Ø©: {failed}")
    send_telegram_notification(summary_msg)
    logger.info(summary_msg)

# --- Ø®Ø§Ø¯Ù… Flask Ù„Ù„Ø¨Ù‚Ø§Ø¡ Ù†Ø´Ø·Ù‹Ø§ Ø¹Ù„Ù‰ Render ---
app = Flask(__name__)
@app.route('/')
def health_check():
    return "Ø®Ø¯Ù…Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†Ø© ØªØ¹Ù…Ù„.", 200

if __name__ == "__main__":
    # Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙŠ Ø®ÙŠØ· Ù…Ù†ÙØµÙ„
    train_thread = Thread(target=parallel_training_job)
    train_thread.daemon = True
    train_thread.start()
    
    port = int(os.environ.get("PORT", 10000))
    logger.info(f"ğŸŒ ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ÙØ° {port}...")
    app.run(host='0.0.0.0', port=port)
