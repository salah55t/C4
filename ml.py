import time
import os
import json
import logging
import requests
import numpy as np
import pandas as pd
import psycopg2
import pickle
import lightgbm as lgb
from psycopg2 import sql
from psycopg2.extras import RealDictCursor
from binance.client import Client
from datetime import datetime, timedelta
from decouple import config
from typing import List, Dict, Optional, Any, Tuple
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, precision_score
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from tqdm import tqdm
from flask import Flask
from threading import Thread

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (Logging) ----------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ml_trainer_advanced.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('AdvancedCryptoMLTrainer')

# ---------------------- ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© ----------------------
try:
    API_KEY: str = config('BINANCE_API_KEY')
    API_SECRET: str = config('BINANCE_API_SECRET')
    DB_URL: str = config('DATABASE_URL')
    TELEGRAM_TOKEN: Optional[str] = config('TELEGRAM_BOT_TOKEN', default=None)
    CHAT_ID: Optional[str] = config('TELEGRAM_CHAT_ID', default=None)
except Exception as e:
     logger.critical(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {e}")
     exit(1)

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø«ÙˆØ§Ø¨Øª ÙˆØ§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© ----------------------
BASE_ML_MODEL_NAME: str = 'LightGBM_Crypto_Predictor_V8_Advanced'
SIGNAL_TIMEFRAME: str = '15m'
DATA_LOOKBACK_DAYS: int = 250
BTC_SYMBOL = 'BTCUSDT'

# --- Ù…Ø¹Ù„Ù…Ø§Øª Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª (Ù…Ø¹ Ø¥Ø¶Ø§ÙØ§Øª Ø¬Ø¯ÙŠØ¯Ø©) ---
RSI_PERIODS: List[int] = [14, 28]
MACD_PARAMS: Dict[str, int] = {'fast': 12, 'slow': 26, 'signal': 9}
ATR_PERIODS: Dict[str, int] = {'short': 14, 'long': 50}
EMA_PERIODS: Dict[str, int] = {'fast': 50, 'slow': 200}
VOLUME_MA_PERIOD: int = 30
BTC_CORR_PERIOD: int = 50

# --- Ù…Ø¹Ù„Ù…Ø§Øª Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø­Ø§Ø¬Ø² Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ (Ù…Ø¹Ø¯Ù„Ø©) ---
TP_ATR_MULTIPLIER: float = 1.8  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù‡Ø¯Ù Ù„Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„ÙØ±Øµ
SL_ATR_MULTIPLIER: float = 1.2
MAX_HOLD_PERIOD: int = 24 # ØªÙ‚Ù„ÙŠÙ„ ÙØªØ±Ø© Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¥Ù„Ù‰ 6 Ø³Ø§Ø¹Ø§Øª (24 * 15m)

# --- Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù„Ù…ÙŠØ© ---
conn: Optional[psycopg2.extensions.connection] = None
client: Optional[Client] = None
btc_data_cache: Optional[pd.DataFrame] = None

# --- Ø¯ÙˆØ§Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ ---
def initialize_database():
    """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
    global conn
    try:
        conn = psycopg2.connect(DB_URL, cursor_factory=RealDictCursor)
        with conn.cursor() as cur:
            cur.execute("""
                CREATE TABLE IF NOT EXISTS ml_models (
                    id SERIAL PRIMARY KEY, model_name TEXT NOT NULL UNIQUE,
                    model_data BYTEA NOT NULL, trained_at TIMESTAMP DEFAULT NOW(), metrics JSONB );
            """)
        conn.commit()
        logger.info("âœ… [DB] ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        logger.critical(f"âŒ [DB] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}"); exit(1)

def initialize_binance_client():
    """ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ Binance."""
    global client
    try:
        client = Client(API_KEY, API_SECRET)
        client.ping()
        logger.info("âœ… [Binance] ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Binance Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        logger.critical(f"âŒ [Binance] ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ Binance: {e}"); exit(1)

# --- Ø¯ÙˆØ§Ù„ Ø¬Ù„Ø¨ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
def fetch_historical_data(symbol: str, interval: str, days: int) -> Optional[pd.DataFrame]:
    """Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù…Ù† Binance Ù„Ø±Ù…Ø² Ù…Ø¹ÙŠÙ†."""
    try:
        start_str = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d %H:%M:%S")
        logger.info(f"â„¹ï¸ [Data] Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª {symbol} Ù…Ù† ØªØ§Ø±ÙŠØ® {start_str}")
        # Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ø§Ù„ØªÙŠ ØªØ´Ù…Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Taker
        klines = client.get_historical_klines(symbol, interval, start_str)
        if not klines:
            logger.warning(f"âš ï¸ [Data] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol}.")
            return None
        
        cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time',
                'quote_volume', 'trades', 'taker_buy_base', 'taker_buy_quote', 'ignore']
        df = pd.DataFrame(klines, columns=cols)
        
        numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'quote_volume', 'taker_buy_base', 'taker_buy_quote']
        for col in numeric_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce')
            
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        return df[numeric_cols].dropna()
    except Exception as e:
        logger.error(f"âŒ [Data] Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol}: {e}")
        return None

def fetch_and_cache_btc_data():
    """Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† ÙˆØªØ®Ø²ÙŠÙ†Ù‡Ø§ Ù…Ø¤Ù‚ØªÙ‹Ø§."""
    global btc_data_cache
    logger.info("â„¹ï¸ [BTC Data] Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† ÙˆØªØ®Ø²ÙŠÙ†Ù‡Ø§...")
    btc_data_cache = fetch_historical_data(BTC_SYMBOL, SIGNAL_TIMEFRAME, DATA_LOOKBACK_DAYS)
    if btc_data_cache is None:
        logger.critical("âŒ [BTC Data] ÙØ´Ù„ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ†."); exit(1)
    btc_data_cache['btc_log_return'] = np.log(btc_data_cache['close'] / btc_data_cache['close'].shift(1))
    btc_data_cache.dropna(inplace=True)

def engineer_features(df: pd.DataFrame, btc_df: pd.DataFrame) -> pd.DataFrame:
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©."""
    df_feat = df.copy()

    # --- 1. Ù…ÙŠØ²Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ© ÙˆÙ…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ø³Ø¹Ø± ÙˆØ§Ù„Ø­Ø¬Ù… ---
    df_feat['log_return'] = np.log(df_feat['close'] / df_feat['close'].shift(1))
    df_feat['relative_volume'] = df_feat['volume'] / (df_feat['volume'].rolling(window=VOLUME_MA_PERIOD, min_periods=1).mean() + 1e-9)
    df_feat['price_range'] = df_feat['high'] - df_feat['low']
    
    # *** Ù…ÙŠØ²Ø© Ø¬Ø¯ÙŠØ¯Ø©: ØªØ³Ø§Ø±Ø¹ Ø§Ù„Ø²Ø®Ù… (Ø§Ù„ØªØºÙŠØ± ÙÙŠ Ø§Ù„Ø¹Ø§Ø¦Ø¯) ***
    df_feat['momentum_change'] = df_feat['log_return'].diff()

    # *** Ù…ÙŠØ²Ø© Ø¬Ø¯ÙŠØ¯Ø©: ØªØ¯ÙÙ‚ Ø§Ù„Ø£ÙˆØ§Ù…Ø± (Taker Buy/Sell Ratio) ***
    # ØªÙ‚ÙŠØ³ Ø¶ØºØ· Ø§Ù„Ø´Ø±Ø§Ø¡ Ù…Ù‚Ø§Ø¨Ù„ Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ¹
    df_feat['taker_buy_sell_ratio'] = (df_feat['taker_buy_quote']) / (df_feat['quote_volume'] + 1e-9)

    # --- 2. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ© ---
    # ATR (Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„ØªÙ‚Ù„Ø¨) Ø¨ÙØªØ±Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
    for name, period in ATR_PERIODS.items():
        tr = pd.concat([df_feat['high'] - df_feat['low'], (df_feat['high'] - df_feat['close'].shift()).abs(), (df_feat['low'] - df_feat['close'].shift()).abs()], axis=1).max(axis=1)
        df_feat[f'atr_{name}'] = tr.ewm(span=period, adjust=False).mean()

    # RSI Ø¨ÙØªØ±Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
    for period in RSI_PERIODS:
        delta = df_feat['close'].diff()
        gain = delta.clip(lower=0).ewm(com=period - 1, adjust=False).mean()
        loss = -delta.clip(upper=0).ewm(com=period - 1, adjust=False).mean()
        df_feat[f'rsi_{period}'] = 100 - (100 / (1 + gain / (loss + 1e-9)))

    # MACD
    ema_fast = df_feat['close'].ewm(span=MACD_PARAMS['fast'], adjust=False).mean()
    ema_slow = df_feat['close'].ewm(span=MACD_PARAMS['slow'], adjust=False).mean()
    df_feat['macd_hist'] = (ema_fast - ema_slow) - ((ema_fast - ema_slow).ewm(span=MACD_PARAMS['signal'], adjust=False).mean())

    # --- 3. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØ§Ù„Ø£Ù†Ø¸Ù…Ø© (Regime Features) ---
    # *** Ù…ÙŠØ²Ø© Ø¬Ø¯ÙŠØ¯Ø©: Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚Ù„Ø¨ (Volatility Regime) ***
    # Ù‡Ù„ Ø§Ù„Ø³ÙˆÙ‚ Ù…ØªÙ‚Ù„Ø¨ Ø§Ù„Ø¢Ù† Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø§Ù„Ø¹Ø§Ø¯Ø©ØŸ
    df_feat['atr_ratio'] = df_feat['atr_short'] / (df_feat['atr_long'] + 1e-9)
    
    # *** Ù…ÙŠØ²Ø© Ø¬Ø¯ÙŠØ¯Ø©: ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¹Ø§Ù… Ø¨Ø´ÙƒÙ„ ØµØ±ÙŠØ­ ***
    ema_fast_trend = df_feat['close'].ewm(span=EMA_PERIODS['fast'], adjust=False).mean()
    ema_slow_trend = df_feat['close'].ewm(span=EMA_PERIODS['slow'], adjust=False).mean()
    df_feat['trend_direction'] = 0
    df_feat.loc[(df_feat['close'] > ema_fast_trend) & (ema_fast_trend > ema_slow_trend), 'trend_direction'] = 1 # Uptrend
    df_feat.loc[(df_feat['close'] < ema_fast_trend) & (ema_fast_trend < ema_slow_trend), 'trend_direction'] = -1 # Downtrend
    
    # --- 4. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ù…Ø¹ Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† ---
    merged_df = pd.merge(df_feat[['log_return']], btc_df[['btc_log_return']], left_index=True, right_index=True, how='left')
    df_feat[f'btc_correlation_{BTC_CORR_PERIOD}'] = merged_df['log_return'].rolling(window=BTC_CORR_PERIOD).corr(merged_df['btc_log_return']).fillna(0)

    # --- 5. Ù…ÙŠØ²Ø§Øª ØªÙØ§Ø¹Ù„ÙŠØ© (Interaction Features) ---
    # *** Ù…ÙŠØ²Ø© Ø¬Ø¯ÙŠØ¯Ø©: ØªÙØ§Ø¹Ù„ RSI Ù…Ø¹ Ø§Ù„Ø­Ø¬Ù… ***
    df_feat['rsi_x_volume'] = df_feat[f'rsi_{RSI_PERIODS[0]}'] * df_feat['relative_volume']
    
    # --- 6. Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙˆÙ‚Øª ---
    df_feat['hour'] = df_feat.index.hour
    df_feat['day_of_week'] = df_feat.index.dayofweek

    return df_feat.replace([np.inf, -np.inf], np.nan).dropna()


def get_triple_barrier_labels(prices: pd.Series, atr: pd.Series) -> pd.Series:
    """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‡Ø¯Ù (1: Ø±Ø¨Ø­, -1: Ø®Ø³Ø§Ø±Ø©, 0: Ù…Ø­Ø§ÙŠØ¯) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø­Ø§Ø¬Ø² Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ."""
    labels = pd.Series(0, index=prices.index, dtype=int)
    prices_np, atr_np = prices.to_numpy(), atr.to_numpy()

    for i in tqdm(range(len(prices) - MAX_HOLD_PERIOD), desc="Labeling Data", leave=False, ncols=80):
        entry_price, current_atr = prices_np[i], atr_np[i]
        if np.isnan(current_atr) or current_atr <= 1e-9: continue

        upper_barrier = entry_price + (current_atr * TP_ATR_MULTIPLIER)
        lower_barrier = entry_price - (current_atr * SL_ATR_MULTIPLIER)
        
        future_prices = prices_np[i + 1 : i + 1 + MAX_HOLD_PERIOD]
        
        profit_touch_indices = np.where(future_prices >= upper_barrier)[0]
        loss_touch_indices = np.where(future_prices <= lower_barrier)[0]

        first_profit_touch = profit_touch_indices[0] if len(profit_touch_indices) > 0 else None
        first_loss_touch = loss_touch_indices[0] if len(loss_touch_indices) > 0 else None

        if first_profit_touch is not None and (first_loss_touch is None or first_profit_touch < first_loss_touch):
            labels.iloc[i] = 1
        elif first_loss_touch is not None and (first_profit_touch is None or first_loss_touch < first_profit_touch):
            labels.iloc[i] = -1

    return labels

def prepare_data_for_ml(df: pd.DataFrame, btc_df: pd.DataFrame, symbol: str) -> Optional[Tuple[pd.DataFrame, pd.Series, Dict[int, float]]]:
    """ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‡Ø¯ÙØŒ Ø­Ø³Ø§Ø¨ Ø£ÙˆØ²Ø§Ù† Ø§Ù„ÙØ¦Ø§Øª."""
    logger.info(f"â„¹ï¸ [ML Prep] ØªØ¬Ù‡ÙŠØ² Ø¨ÙŠØ§Ù†Ø§Øª {symbol} Ù„Ù„Ù†Ù…ÙˆØ°Ø¬...")
    
    df_featured = engineer_features(df, btc_df)
    
    if 'atr_short' not in df_featured.columns or df_featured['atr_short'].isnull().all():
        logger.warning(f"âš ï¸ [ML Prep] Ù…ÙŠØ²Ø© 'atr_short' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ù„Ù€ {symbol}.")
        return None

    df_featured['target'] = get_triple_barrier_labels(df_featured['close'], df_featured['atr_short'])
    df_featured['target_mapped'] = df_featured['target'].map({-1: 0, 0: 1, 1: 2})
    
    feature_columns = [col for col in df_featured.columns if col not in ['open', 'high', 'low', 'close', 'volume', 'quote_volume', 'taker_buy_base', 'taker_buy_quote', 'target', 'target_mapped', 'atr_long']]
    
    df_cleaned = df_featured.dropna(subset=feature_columns + ['target_mapped']).copy()

    if df_cleaned.empty or df_cleaned['target_mapped'].nunique() < 3:
        logger.warning(f"âš ï¸ [ML Prep] Ø¨ÙŠØ§Ù†Ø§Øª {symbol} ÙØ§Ø±ØºØ© Ø£Ùˆ Ù„Ø§ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø«Ù„Ø§Ø«.")
        return None

    X = df_cleaned[feature_columns]
    y = df_cleaned['target_mapped']
    
    # *** Ù…ÙŠØ²Ø© Ø¬Ø¯ÙŠØ¯Ø©: Ø­Ø³Ø§Ø¨ Ø£ÙˆØ²Ø§Ù† Ø§Ù„ÙØ¦Ø§Øª Ù„Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ***
    class_weights_values = compute_class_weight('balanced', classes=np.unique(y), y=y)
    class_weights = dict(zip(np.unique(y), class_weights_values))
    logger.info(f"âš–ï¸ [ML Prep] Ø£ÙˆØ²Ø§Ù† Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ø­Ø³ÙˆØ¨Ø© Ù„Ù€ {symbol}: {class_weights}")

    categorical_features = ['hour', 'day_of_week', 'trend_direction']
    for col in categorical_features:
        if col in X.columns: X[col] = X[col].astype('category')

    logger.info(f"ğŸ“Š [ML Prep] ØªÙˆØ²ÙŠØ¹ Ø£Ù‡Ø¯Ø§Ù {symbol} (0=Ø®Ø³Ø§Ø±Ø©, 1=Ù…Ø­Ø§ÙŠØ¯, 2=Ø±Ø¨Ø­):\n{y.value_counts(normalize=True)}")
    
    return X, y, class_weights

# --- Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ---
def train_model(X: pd.DataFrame, y: pd.Series, class_weights: Dict[int, float]) -> Tuple[Optional[Any], Optional[Any], Optional[Dict[str, Any]]]:
    """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Walk-Forward Validation ÙˆØ£ÙˆØ²Ø§Ù† Ø§Ù„ÙØ¦Ø§Øª."""
    logger.info("â„¹ï¸ [ML Train] Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
    
    tscv = TimeSeriesSplit(n_splits=5)
    
    lgbm_params = {
        'objective': 'multiclass', 'num_class': 3, 'metric': 'multi_logloss',
        'boosting_type': 'gbdt', 'n_estimators': 2000, 'learning_rate': 0.01,
        'num_leaves': 40, 'max_depth': 7, 'seed': 42, 'n_jobs': -1, 'verbose': -1,
        'colsample_bytree': 0.7, 'subsample': 0.7, 'reg_alpha': 0.1, 'reg_lambda': 0.1,
    }

    final_model, final_scaler = None, None
    all_preds, all_true = [], []

    categorical_features_in_X = [col for col in ['hour', 'day_of_week', 'trend_direction'] if col in X.columns]

    for i, (train_index, test_index) in enumerate(tscv.split(X)):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        if y_train.nunique() < 3: continue
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
        sample_weight = y_train.map(class_weights).values

        numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()
        scaler = StandardScaler()
        X_train.loc[:, numerical_features] = scaler.fit_transform(X_train[numerical_features])
        X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])
        
        model = lgb.LGBMClassifier(**lgbm_params)
        model.fit(X_train, y_train,
                  eval_set=[(X_test, y_test)],
                  eval_metric='multi_logloss',
                  callbacks=[lgb.early_stopping(100, verbose=False)],
                  categorical_feature=categorical_features_in_X,
                  sample_weight=sample_weight) # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¹ÙŠÙ†Ø§Øª

        y_pred = model.predict(X_test)
        all_preds.extend(y_pred)
        all_true.extend(y_test)
        final_model, final_scaler = model, scaler

    if not all_true:
        logger.error("âŒ [ML Train] ÙØ´Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ù„Ù… ÙŠØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø£ÙŠ Ø·ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­."); return None, None, None
        
    accuracy = accuracy_score(all_true, all_preds)
    precision_profit = precision_score(all_true, all_preds, labels=[2], average='macro', zero_division=0)
    
    metrics = {'overall_accuracy': accuracy, 'precision_for_profit_class': precision_profit, 'num_samples_trained': len(X)}
    logger.info(f"ğŸ“Š [ML Train] Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: Accuracy={accuracy:.4f}, Precision (Profit)={precision_profit:.4f}")
    
    return final_model, final_scaler, metrics

def save_model_to_db(model_bundle: Dict[str, Any], model_name: str, metrics: Dict[str, Any]):
    """Ø­ÙØ¸ Ø­Ø²Ù…Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
    logger.info(f"â„¹ï¸ [DB Save] Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ '{model_name}'...")
    try:
        model_binary = pickle.dumps(model_bundle)
        metrics_json = json.dumps(metrics)
        with conn.cursor() as cur:
            cur.execute("""
                INSERT INTO ml_models (model_name, model_data, metrics)
                VALUES (%s, %s, %s) ON CONFLICT (model_name) DO UPDATE SET
                model_data = EXCLUDED.model_data, trained_at = NOW(), metrics = EXCLUDED.metrics;
            """, (model_name, model_binary, metrics_json))
        conn.commit()
        logger.info(f"âœ… [DB Save] ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ '{model_name}' Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        logger.error(f"âŒ [DB Save] Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}"); conn.rollback()

def send_telegram_notification(text: str):
    """Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø¹Ø§Ø± Ø¥Ù„Ù‰ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…."""
    if not TELEGRAM_TOKEN or not CHAT_ID: return
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    try:
        requests.post(url, json={'chat_id': CHAT_ID, 'text': text, 'parse_mode': 'Markdown'}, timeout=10)
    except Exception as e:
        logger.error(f"âŒ [Telegram] ÙØ´Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¥Ø´Ø¹Ø§Ø±: {e}")

# --- Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ØªØ¹Ù…Ù„ ÙÙŠ Ø®ÙŠØ· Ù…Ù†ÙØµÙ„ ---
def training_job():
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ù…ÙˆØ²."""
    logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ({BASE_ML_MODEL_NAME})...")
    initialize_database()
    initialize_binance_client()
    fetch_and_cache_btc_data()
    
    try:
        with open('crypto_list.txt', 'r', encoding='utf-8') as f:
            symbols = {s.strip().upper() + 'USDT' for s in f if s.strip()}
    except FileNotFoundError:
        logger.critical("âŒ [Main] Ù…Ù„Ù 'crypto_list.txt' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯."); return

    send_telegram_notification(f"ğŸš€ *Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ {BASE_ML_MODEL_NAME}*\nØ³ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ù„Ù€ {len(symbols)} Ø¹Ù…Ù„Ø©.")
    
    successful, failed = 0, 0
    for symbol in tqdm(symbols, desc="ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¹Ù…Ù„Ø§Øª"):
        logger.info(f"\n--- â³ [Main] Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù€ {symbol} ---")
        try:
            hist_data = fetch_historical_data(symbol, SIGNAL_TIMEFRAME, DATA_LOOKBACK_DAYS)
            if hist_data is None or hist_data.empty:
                logger.warning(f"âš ï¸ [Main] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol}, ØªØ®Ø·ÙŠ."); failed += 1; continue
            
            prepared_data = prepare_data_for_ml(hist_data, btc_data_cache, symbol)
            if prepared_data is None:
                failed += 1; continue
            X, y, class_weights = prepared_data
            
            model, scaler, metrics = train_model(X, y, class_weights)
            
            # Ø±ÙØ¹ Ø§Ù„Ø¹ØªØ¨Ø© Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ù„Ø£Ù†Ù†Ø§ Ù†ØªÙˆÙ‚Ø¹ Ø£Ø¯Ø§Ø¡ Ø£ÙØ¶Ù„ Ø§Ù„Ø¢Ù†
            if model and metrics and metrics.get('precision_for_profit_class', 0) > 0.42:
                model_bundle = {
                    'model': model, 'scaler': scaler, 'feature_names': list(X.columns)
                }
                model_name = f"{BASE_ML_MODEL_NAME}_{symbol}"
                save_model_to_db(model_bundle, model_name, metrics)
                successful += 1
                send_telegram_notification(f"âœ… *ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ {symbol}*\n_Precision (Profit): {metrics['precision_for_profit_class']:.3f}_")
            else:
                logger.warning(f"âš ï¸ [Main] Ù†Ù…ÙˆØ°Ø¬ {symbol} Ù„Ù… ÙŠØ­Ù‚Ù‚ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨. ØªÙ… ØªØ¬Ø§Ù‡Ù„Ù‡."); failed += 1
        except Exception as e:
            logger.critical(f"âŒ [Main] Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ ØªØ¯Ø±ÙŠØ¨ {symbol}: {e}", exc_info=True); failed += 1
        time.sleep(1)

    summary_msg = (f"ğŸ *Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© ØªØ¯Ø±ÙŠØ¨ {BASE_ML_MODEL_NAME}*\n"
                   f"- Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù†Ø§Ø¬Ø­Ø©: {successful}\n"
                   f"- Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙØ§Ø´Ù„Ø©/Ø§Ù„Ù…ØªØ¬Ø§Ù‡ÙÙ„Ø©: {failed}")
    send_telegram_notification(summary_msg)
    logger.info(summary_msg)

    if conn: conn.close(); logger.info("ğŸ‘‹ [Main] ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")

# --- Ø®Ø§Ø¯Ù… Flask Ù„Ù„Ø¨Ù‚Ø§Ø¡ Ù†Ø´Ø·Ù‹Ø§ Ø¹Ù„Ù‰ Render ---
app = Flask(__name__)
@app.route('/')
def health_check():
    return "Ø®Ø¯Ù…Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© ØªØ¹Ù…Ù„.", 200

if __name__ == "__main__":
    train_thread = Thread(target=training_job)
    train_thread.daemon = True
    train_thread.start()
    
    port = int(os.environ.get("PORT", 10000))
    logger.info(f"ğŸŒ ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ÙØ° {port}...")
    app.run(host='0.0.0.0', port=port)
