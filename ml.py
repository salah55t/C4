import time
import os
import json
import logging
import requests
import numpy as np
import pandas as pd
import psycopg2
import pickle
import lightgbm as lgb
from psycopg2 import sql
from psycopg2.extras import RealDictCursor
from binance.client import Client
from datetime import datetime, timedelta
from decouple import config
from typing import List, Dict, Optional, Any, Tuple
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import classification_report, accuracy_score, precision_score
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
from flask import Flask
from threading import Thread

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (Logging) ----------------------
# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø¬Ù„ Ù„ØªØªØ¨Ø¹ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙˆØ­ÙØ¸Ù‡Ø§ ÙÙŠ Ù…Ù„Ù
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ml_model_trainer_enhanced.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('CryptoMLTrainer')

# ---------------------- ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© ----------------------
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø© Ù…Ø«Ù„ Ù…ÙØ§ØªÙŠØ­ API Ù…Ù† Ù…Ù„Ù .env
try:
    API_KEY: str = config('BINANCE_API_KEY')
    API_SECRET: str = config('BINANCE_API_SECRET')
    DB_URL: str = config('DATABASE_URL')
    TELEGRAM_TOKEN: Optional[str] = config('TELEGRAM_BOT_TOKEN', default=None)
    CHAT_ID: Optional[str] = config('TELEGRAM_CHAT_ID', default=None)
except Exception as e:
     logger.critical(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©. ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„Ù .env: {e}")
     exit(1)

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø«ÙˆØ§Ø¨Øª ÙˆØ§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© ----------------------
BASE_ML_MODEL_NAME: str = 'LightGBM_Crypto_Predictor_V7'
SIGNAL_TIMEFRAME: str = '15m'  # Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù„ØªØ­Ù„ÙŠÙ„
DATA_LOOKBACK_DAYS: int = 200 # Ø²ÙŠØ§Ø¯Ø© ÙØªØ±Ø© Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ¯Ø±ÙŠØ¨ Ø£ÙØ¶Ù„
BTC_SYMBOL = 'BTCUSDT'

# --- Ù…Ø¹Ù„Ù…Ø§Øª Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª ---
RSI_PERIODS: List[int] = [14, 28]
MACD_FAST, MACD_SLOW, MACD_SIGNAL = 12, 26, 9
ATR_PERIOD: int = 14
EMA_PERIODS: Dict[str, int] = {'fast': 50, 'slow': 200}
VOLUME_MA_PERIOD: int = 20
BTC_CORR_PERIOD: int = 50

# --- Ù…Ø¹Ù„Ù…Ø§Øª Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø­Ø§Ø¬Ø² Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‡Ø¯Ù ---
TP_ATR_MULTIPLIER: float = 2.0  # Ù…Ø¶Ø§Ø¹Ù ATR Ù„ØªØ­Ø¯ÙŠØ¯ Ù‡Ø¯Ù Ø§Ù„Ø±Ø¨Ø­
SL_ATR_MULTIPLIER: float = 1.5  # Ù…Ø¶Ø§Ø¹Ù ATR Ù„ØªØ­Ø¯ÙŠØ¯ ÙˆÙ‚Ù Ø§Ù„Ø®Ø³Ø§Ø±Ø©
MAX_HOLD_PERIOD: int = 48 # Ø£Ù‚ØµÙ‰ Ø¹Ø¯Ø¯ Ø´Ù…ÙˆØ¹ Ù„Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„ØµÙÙ‚Ø© (48 * 15m = 12 hours)

# --- Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù„Ù…ÙŠØ© ---
conn: Optional[psycopg2.extensions.connection] = None
client: Optional[Client] = None
btc_data_cache: Optional[pd.DataFrame] = None

# --- Ø¯ÙˆØ§Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ ---
def initialize_database():
    """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§."""
    global conn
    try:
        conn = psycopg2.connect(DB_URL, cursor_factory=RealDictCursor)
        with conn.cursor() as cur:
            cur.execute("""
                CREATE TABLE IF NOT EXISTS ml_models (
                    id SERIAL PRIMARY KEY,
                    model_name TEXT NOT NULL UNIQUE,
                    model_data BYTEA NOT NULL,
                    trained_at TIMESTAMP DEFAULT NOW(),
                    metrics JSONB
                );
            """)
        conn.commit()
        logger.info("âœ… [DB] ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        logger.critical(f"âŒ [DB] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}"); exit(1)

def initialize_binance_client():
    """ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ Binance Ù„Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ API."""
    global client
    try:
        client = Client(API_KEY, API_SECRET)
        client.ping()
        logger.info("âœ… [Binance] ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Binance Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        logger.critical(f"âŒ [Binance] ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ Binance: {e}"); exit(1)

# --- Ø¯ÙˆØ§Ù„ Ø¬Ù„Ø¨ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
def fetch_historical_data(symbol: str, interval: str, days: int) -> Optional[pd.DataFrame]:
    """Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù…Ù† Binance Ù„Ø±Ù…Ø² Ù…Ø¹ÙŠÙ†."""
    try:
        start_str = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d %H:%M:%S")
        logger.info(f"â„¹ï¸ [Data] Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª {symbol} Ù…Ù† ØªØ§Ø±ÙŠØ® {start_str} Ø¨Ø¥Ø·Ø§Ø± Ø²Ù…Ù†ÙŠ {interval}")
        klines = client.get_historical_klines(symbol, interval, start_str)
        if not klines:
            logger.warning(f"âš ï¸ [Data] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol}.")
            return None
            
        df = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_volume', 'trades', 'taker_buy_base', 'taker_buy_quote', 'ignore'])
        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols: df[col] = pd.to_numeric(df[col], errors='coerce')
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        return df[numeric_cols].dropna()
    except Exception as e:
        logger.error(f"âŒ [Data] Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol}: {e}")
        return None

def fetch_and_cache_btc_data():
    """Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† ÙˆØªØ®Ø²ÙŠÙ†Ù‡Ø§ Ù…Ø¤Ù‚ØªÙ‹Ø§ Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª."""
    global btc_data_cache
    logger.info("â„¹ï¸ [BTC Data] Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† ÙˆØªØ®Ø²ÙŠÙ†Ù‡Ø§...")
    btc_data_cache = fetch_historical_data(BTC_SYMBOL, SIGNAL_TIMEFRAME, DATA_LOOKBACK_DAYS)
    if btc_data_cache is None:
        logger.critical("âŒ [BTC Data] ÙØ´Ù„ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ†ØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©."); exit(1)
    # Ø­Ø³Ø§Ø¨ Ø¹ÙˆØ§Ø¦Ø¯ Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† Ø§Ù„Ù„ÙˆØºØ§Ø±ÙŠØªÙ…ÙŠØ© ÙˆØªØ®Ø²ÙŠÙ†Ù‡Ø§
    btc_data_cache['btc_log_return'] = np.log(btc_data_cache['close'] / btc_data_cache['close'].shift(1))
    btc_data_cache.dropna(inplace=True)

def engineer_features(df: pd.DataFrame, btc_df: pd.DataFrame) -> pd.DataFrame:
    """
    Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª.
    ØªÙ‚ÙˆÙ… Ø¨Ø­Ø³Ø§Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ© ÙˆØ§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬.
    """
    df_feat = df.copy()

    # --- 1. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø³Ø¹Ø± ÙˆØ§Ù„Ø­Ø¬Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---
    df_feat['log_return'] = np.log(df_feat['close'] / df_feat['close'].shift(1))
    df_feat['volume_change_ratio'] = df_feat['volume'].diff() / df_feat['volume'].shift(1).replace(0, 1e-9)
    # Ø³ÙŠÙˆÙ„Ø© Ø§Ù„Ø´Ù…Ø¹Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø§Ù„Ù…ØªÙˆØ³Ø·
    df_feat['relative_volume'] = df_feat['volume'] / (df_feat['volume'].rolling(window=VOLUME_MA_PERIOD, min_periods=1).mean() + 1e-9)

    # --- 2. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ© (RSI, MACD, ATR, EMA) ---
    # ATR (Average True Range) Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„ØªÙ‚Ù„Ø¨
    high_low = df_feat['high'] - df_feat['low']
    high_close = (df_feat['high'] - df_feat['close'].shift()).abs()
    low_close = (df_feat['low'] - df_feat['close'].shift()).abs()
    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
    df_feat['atr'] = tr.ewm(span=ATR_PERIOD, adjust=False).mean()

    # RSI (Relative Strength Index) Ø¨ÙØªØ±Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
    for period in RSI_PERIODS:
        delta = df_feat['close'].diff()
        gain = delta.clip(lower=0).ewm(com=period - 1, adjust=False).mean()
        loss = -delta.clip(upper=0).ewm(com=period - 1, adjust=False).mean()
        rs = gain / (loss + 1e-9)
        df_feat[f'rsi_{period}'] = 100 - (100 / (1 + rs))

    # MACD (Moving Average Convergence Divergence)
    ema_fast = df_feat['close'].ewm(span=MACD_FAST, adjust=False).mean()
    ema_slow = df_feat['close'].ewm(span=MACD_SLOW, adjust=False).mean()
    macd_line = ema_fast - ema_slow
    signal_line = macd_line.ewm(span=MACD_SIGNAL, adjust=False).mean()
    df_feat['macd_hist'] = macd_line - signal_line
    
    # *** Ù…ÙŠØ²Ø© ØªÙ‚Ø§Ø·Ø¹Ø§Øª Ø§Ù„Ù…Ø§ÙƒØ¯ (Ø¬Ø¯ÙŠØ¯Ø©) ***
    # Ø¥Ø´Ø§Ø±Ø© Ù…ÙˆØ¬Ø¨Ø© (1) Ù„ØªÙ‚Ø§Ø·Ø¹ ØµØ¹ÙˆØ¯ÙŠØŒ ÙˆØ³Ø§Ù„Ø¨Ø© (-1) Ù„ØªÙ‚Ø§Ø·Ø¹ Ù‡Ø¨ÙˆØ·ÙŠ
    macd_cross_above = (macd_line.shift(1) < signal_line.shift(1)) & (macd_line > signal_line)
    macd_cross_below = (macd_line.shift(1) > signal_line.shift(1)) & (macd_line < signal_line)
    df_feat['macd_cross'] = 0
    df_feat.loc[macd_cross_above, 'macd_cross'] = 1
    df_feat.loc[macd_cross_below, 'macd_cross'] = -1

    # EMA (Exponential Moving Average) Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
    df_feat['ema_fast'] = df_feat['close'].ewm(span=EMA_PERIODS['fast'], adjust=False).mean()
    df_feat['ema_slow'] = df_feat['close'].ewm(span=EMA_PERIODS['slow'], adjust=False).mean()
    df_feat['price_vs_ema_slow'] = (df_feat['close'] / df_feat['ema_slow']) - 1
    df_feat['ema_fast_vs_ema_slow'] = (df_feat['ema_fast'] / df_feat['ema_slow']) - 1

    # --- 3. Ù…ÙŠØ²Ø§Øª Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø´Ù…Ø¹Ø© (Candle Structure) ---
    candle_range = df_feat['high'] - df_feat['low']
    candle_body = (df_feat['close'] - df_feat['open']).abs()
    df_feat['candle_body_ratio'] = candle_body / (candle_range + 1e-9)
    df_feat['upper_wick'] = df_feat['high'] - np.maximum(df_feat['open'], df_feat['close'])
    df_feat['lower_wick'] = np.minimum(df_feat['open'], df_feat['close']) - df_feat['low']
    df_feat['upper_wick_ratio'] = df_feat['upper_wick'] / (candle_range + 1e-9)

    # --- 4. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ù…Ø¹ Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† ---
    # Ø¯Ù…Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ù…Ù„Ø© Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·
    merged_df = pd.merge(df_feat[['log_return']], btc_df[['btc_log_return']], left_index=True, right_index=True, how='left')
    df_feat[f'btc_correlation_{BTC_CORR_PERIOD}'] = merged_df['log_return'].rolling(window=BTC_CORR_PERIOD).corr(merged_df['btc_log_return']).fillna(0)

    # --- 5. Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙˆÙ‚Øª ---
    df_feat['hour'] = df_feat.index.hour
    df_feat['day_of_week'] = df_feat.index.dayofweek

    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
    df_feat.drop(columns=['ema_fast', 'ema_slow', 'upper_wick', 'lower_wick'], inplace=True, errors='ignore')
    
    return df_feat.dropna()


def get_triple_barrier_labels(prices: pd.Series, atr: pd.Series) -> pd.Series:
    """
    ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‡Ø¯Ù (1: Ø±Ø¨Ø­, -1: Ø®Ø³Ø§Ø±Ø©, 0: Ù…Ø­Ø§ÙŠØ¯) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø­Ø§Ø¬Ø² Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ.
    """
    labels = pd.Series(0, index=prices.index, dtype=int)
    prices_np = prices.to_numpy()
    atr_np = atr.to_numpy()

    for i in tqdm(range(len(prices) - MAX_HOLD_PERIOD), desc="Labeling Data", leave=False, ncols=80):
        entry_price = prices_np[i]
        current_atr = atr_np[i]

        if np.isnan(current_atr) or current_atr <= 1e-9:
            continue

        upper_barrier = entry_price + (current_atr * TP_ATR_MULTIPLIER)
        lower_barrier = entry_price - (current_atr * SL_ATR_MULTIPLIER)
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù†Ø§ÙØ°Ø© Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
        future_prices = prices_np[i + 1 : i + 1 + MAX_HOLD_PERIOD]
        
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ù„Ù…Ø³ Ø­Ø§Ø¬Ø² Ø§Ù„Ø±Ø¨Ø­
        profit_touch_indices = np.where(future_prices >= upper_barrier)[0]
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ù„Ù…Ø³ Ø­Ø§Ø¬Ø² Ø§Ù„Ø®Ø³Ø§Ø±Ø©
        loss_touch_indices = np.where(future_prices <= lower_barrier)[0]

        first_profit_touch = profit_touch_indices[0] if len(profit_touch_indices) > 0 else None
        first_loss_touch = loss_touch_indices[0] if len(loss_touch_indices) > 0 else None

        if first_profit_touch is not None and (first_loss_touch is None or first_profit_touch < first_loss_touch):
            labels.iloc[i] = 1  # Ø±Ø¨Ø­
        elif first_loss_touch is not None and (first_profit_touch is None or first_loss_touch < first_profit_touch):
            labels.iloc[i] = -1 # Ø®Ø³Ø§Ø±Ø©
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙ„Ù…Ø³ Ø£ÙŠ Ø­Ø§Ø¬Ø²ØŒ Ø³ØªØ¨Ù‚Ù‰ Ø§Ù„Ù‚ÙŠÙ…Ø© 0 (Ù…Ø­Ø§ÙŠØ¯)

    return labels


def prepare_data_for_ml(df: pd.DataFrame, btc_df: pd.DataFrame, symbol: str) -> Optional[Tuple[pd.DataFrame, pd.Series]]:
    """ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬: Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‡Ø¯ÙØŒ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ."""
    logger.info(f"â„¹ï¸ [ML Prep] ØªØ¬Ù‡ÙŠØ² Ø¨ÙŠØ§Ù†Ø§Øª {symbol} Ù„Ù„Ù†Ù…ÙˆØ°Ø¬...")
    
    df_featured = engineer_features(df, btc_df)
    
    if 'atr' not in df_featured.columns or df_featured['atr'].isnull().all():
        logger.warning(f"âš ï¸ [ML Prep] Ù…ÙŠØ²Ø© 'atr' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ù„Ù€ {symbol}. Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù.")
        return None

    # ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ø¬Ø² Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù‡Ø¯Ù
    df_featured['target'] = get_triple_barrier_labels(df_featured['close'], df_featured['atr'])
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‡Ø¯Ù Ù…Ù† (-1, 0, 1) Ø¥Ù„Ù‰ (0, 1, 2) Ù„ÙŠØªÙˆØ§ÙÙ‚ Ù…Ø¹ LightGBM
    # 0: Ø®Ø³Ø§Ø±Ø©, 1: Ù…Ø­Ø§ÙŠØ¯, 2: Ø±Ø¨Ø­
    df_featured['target_mapped'] = df_featured['target'].map({-1: 0, 0: 1, 1: 2})
    
    # ØªØ­Ø¯ÙŠØ¯ Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    feature_columns = [col for col in df_featured.columns if col not in ['open', 'high', 'low', 'close', 'volume', 'target', 'target_mapped']]
    
    df_cleaned = df_featured.dropna(subset=feature_columns + ['target_mapped']).copy()

    if df_cleaned.empty or df_cleaned['target_mapped'].nunique() < 2:
        logger.warning(f"âš ï¸ [ML Prep] Ø¨ÙŠØ§Ù†Ø§Øª {symbol} ÙØ§Ø±ØºØ© Ø£Ùˆ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£Ù‚Ù„ Ù…Ù† ÙØ¦ØªÙŠÙ† Ø¨Ø¹Ø¯ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª.")
        return None

    X = df_cleaned[feature_columns]
    y = df_cleaned['target_mapped']
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†ÙˆØ¹ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù€ LightGBM
    categorical_features = ['hour', 'day_of_week', 'macd_cross']
    for col in categorical_features:
        if col in X.columns:
            X[col] = X[col].astype('category')

    logger.info(f"ğŸ“Š [ML Prep] ØªÙˆØ²ÙŠØ¹ Ø£Ù‡Ø¯Ø§Ù {symbol} (0=Ø®Ø³Ø§Ø±Ø©, 1=Ù…Ø­Ø§ÙŠØ¯, 2=Ø±Ø¨Ø­):\n{y.value_counts(normalize=True)}")
    
    return X, y

# --- Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ---
def train_model(X: pd.DataFrame, y: pd.Series) -> Tuple[Optional[Any], Optional[Any], Optional[Dict[str, Any]]]:
    """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ Ù„Ù„Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠØ© (Walk-Forward)."""
    logger.info("â„¹ï¸ [ML Train] Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Walk-Forward Validation...")
    
    tscv = TimeSeriesSplit(n_splits=5)
    
    # Ù…Ø¹Ù„Ù…Ø§Øª LightGBM Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ù…ÙˆØ§Ø²Ù†Ø© Ø¨ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø¯Ù‚Ø©
    lgbm_params = {
        'objective': 'multiclass',
        'num_class': 3,
        'metric': 'multi_logloss',
        'boosting_type': 'gbdt',
        'n_estimators': 2000,
        'learning_rate': 0.01,
        'num_leaves': 40,
        'max_depth': 7,
        'seed': 42,
        'n_jobs': -1,
        'verbose': -1,
        'colsample_bytree': 0.7,
        'subsample': 0.7,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1,
    }

    final_model, final_scaler = None, None
    all_preds, all_true = [], []

    categorical_features_in_X = [col for col in ['hour', 'day_of_week', 'macd_cross'] if col in X.columns]

    for i, (train_index, test_index) in enumerate(tscv.split(X)):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        if y_train.nunique() < 3: # Ù†Ø­ØªØ§Ø¬ ÙƒÙ„ Ø§Ù„ÙØ¦Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
             logger.warning(f"âš ï¸ [ML Train] Fold {i+1} Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø«Ù„Ø§Ø«. Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ®Ø·ÙŠ.")
             continue
        
        # ÙØµÙ„ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ© ÙˆØ§Ù„ÙØ¦ÙˆÙŠØ© Ù„Ù„ØªØ·Ø¨ÙŠØ¹
        numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()
        
        scaler = StandardScaler()
        X_train.loc[:, numerical_features] = scaler.fit_transform(X_train[numerical_features])
        X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])
        
        model = lgb.LGBMClassifier(**lgbm_params)
        model.fit(X_train, y_train,
                  eval_set=[(X_test, y_test)],
                  eval_metric='multi_logloss',
                  callbacks=[lgb.early_stopping(100, verbose=False)],
                  categorical_feature=categorical_features_in_X)

        y_pred = model.predict(X_test)
        all_preds.extend(y_pred)
        all_true.extend(y_test)
        
        final_model, final_scaler = model, scaler # Ø­ÙØ¸ Ø¢Ø®Ø± Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…ÙØ·Ø¨Ù‘ÙØ¹

    if not all_true:
        logger.error("âŒ [ML Train] ÙØ´Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ù„Ù… ÙŠØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø£ÙŠ Ø·ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­.")
        return None, None, None
        
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©
    accuracy = accuracy_score(all_true, all_preds)
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø© (Precision) Ù„ÙØ¦Ø© Ø§Ù„Ø±Ø¨Ø­ (2) ÙÙ‚Ø·
    precision_profit = precision_score(all_true, all_preds, labels=[2], average='macro', zero_division=0)
    
    metrics = {
        'overall_accuracy': accuracy,
        'precision_for_profit_class': precision_profit,
        'num_samples_trained': len(X)
    }
    logger.info(f"ğŸ“Š [ML Train] Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: Accuracy={accuracy:.4f}, Precision (Profit)={precision_profit:.4f}")
    
    return final_model, final_scaler, metrics

def save_model_to_db(model_bundle: Dict[str, Any], model_name: str, metrics: Dict[str, Any]):
    """Ø­ÙØ¸ Ø­Ø²Ù…Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ Ø§Ù„Ù…Ø·Ø¨Ø¹ØŒ Ø§Ù„Ù…ÙŠØ²Ø§Øª) ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
    logger.info(f"â„¹ï¸ [DB Save] Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ '{model_name}'...")
    try:
        model_binary = pickle.dumps(model_bundle)
        metrics_json = json.dumps(metrics)
        with conn.cursor() as cur:
            cur.execute("""
                INSERT INTO ml_models (model_name, model_data, metrics)
                VALUES (%s, %s, %s) ON CONFLICT (model_name) DO UPDATE SET
                model_data = EXCLUDED.model_data, trained_at = NOW(), metrics = EXCLUDED.metrics;
            """, (model_name, model_binary, metrics_json))
        conn.commit()
        logger.info(f"âœ… [DB Save] ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ '{model_name}' Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        logger.error(f"âŒ [DB Save] Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}"); conn.rollback()

def send_telegram_notification(text: str):
    """Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø¹Ø§Ø± Ø¥Ù„Ù‰ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…."""
    if not TELEGRAM_TOKEN or not CHAT_ID:
        return
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    try:
        requests.post(url, json={'chat_id': CHAT_ID, 'text': text, 'parse_mode': 'Markdown'}, timeout=10)
    except Exception as e:
        logger.error(f"âŒ [Telegram] ÙØ´Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¥Ø´Ø¹Ø§Ø±: {e}")

# --- Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ØªØ¹Ù…Ù„ ÙÙŠ Ø®ÙŠØ· Ù…Ù†ÙØµÙ„ ---
def training_job():
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ù…ÙˆØ²."""
    logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ({BASE_ML_MODEL_NAME})...")
    initialize_database()
    initialize_binance_client()
    fetch_and_cache_btc_data()
    
    try:
        with open('crypto_list.txt', 'r', encoding='utf-8') as f:
            symbols = {s.strip().upper() + 'USDT' for s in f if s.strip()}
    except FileNotFoundError:
        logger.critical("âŒ [Main] Ù…Ù„Ù 'crypto_list.txt' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©."); return

    send_telegram_notification(f"ğŸš€ *Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ {BASE_ML_MODEL_NAME}*\nØ³ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ù„Ù€ {len(symbols)} Ø¹Ù…Ù„Ø©.")
    
    successful, failed = 0, 0
    for symbol in tqdm(symbols, desc="ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¹Ù…Ù„Ø§Øª"):
        logger.info(f"\n--- â³ [Main] Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù€ {symbol} ---")
        try:
            hist_data = fetch_historical_data(symbol, SIGNAL_TIMEFRAME, DATA_LOOKBACK_DAYS)
            if hist_data is None or hist_data.empty:
                logger.warning(f"âš ï¸ [Main] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ù€ {symbol}, Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ®Ø·ÙŠ."); failed += 1; continue
            
            prepared_data = prepare_data_for_ml(hist_data, btc_data_cache, symbol)
            if prepared_data is None:
                failed += 1; continue
            X, y = prepared_data
            
            model, scaler, metrics = train_model(X, y)
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¯Ù‚ØªÙ‡ Ù„ÙØ¦Ø© Ø§Ù„Ø±Ø¨Ø­ Ø¬ÙŠØ¯Ø©
            if model and metrics and metrics.get('precision_for_profit_class', 0) > 0.45:
                model_bundle = {
                    'model': model, 'scaler': scaler, 'feature_names': list(X.columns)
                }
                model_name = f"{BASE_ML_MODEL_NAME}_{symbol}"
                save_model_to_db(model_bundle, model_name, metrics)
                successful += 1
                send_telegram_notification(f"âœ… *ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ {symbol}*\n_Precision (Profit): {metrics['precision_for_profit_class']:.3f}_")
            else:
                logger.warning(f"âš ï¸ [Main] Ù†Ù…ÙˆØ°Ø¬ {symbol} Ù„Ù… ÙŠØ­Ù‚Ù‚ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (Precision < 0.45). ØªÙ… ØªØ¬Ø§Ù‡Ù„Ù‡."); failed += 1
        except Exception as e:
            logger.critical(f"âŒ [Main] Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ ØªØ¯Ø±ÙŠØ¨ {symbol}: {e}", exc_info=True); failed += 1
        time.sleep(2) # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ø¨ÙŠÙ† Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ù„ØªØ¬Ù†Ø¨ Ø¥ØºØ±Ø§Ù‚ Ø§Ù„Ù€ API

    summary_msg = (f"ğŸ *Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© ØªØ¯Ø±ÙŠØ¨ {BASE_ML_MODEL_NAME}*\n"
                   f"- Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù†Ø§Ø¬Ø­Ø©: {successful}\n"
                   f"- Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙØ§Ø´Ù„Ø©/Ø§Ù„Ù…ØªØ¬Ø§Ù‡ÙÙ„Ø©: {failed}")
    send_telegram_notification(summary_msg)
    logger.info(summary_msg)

    if conn: conn.close(); logger.info("ğŸ‘‹ [Main] ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")

# --- Ø®Ø§Ø¯Ù… Flask Ù„Ù„Ø¨Ù‚Ø§Ø¡ Ù†Ø´Ø·Ù‹Ø§ Ø¹Ù„Ù‰ Render ---
app = Flask(__name__)
@app.route('/')
def health_check():
    return "Ø®Ø¯Ù…Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ØªØ¹Ù…Ù„.", 200

if __name__ == "__main__":
    # Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙŠ Ø®ÙŠØ· Ù…Ù†ÙØµÙ„ Ø­ØªÙ‰ Ù„Ø§ ÙŠØªØ¹Ø§Ø±Ø¶ Ù…Ø¹ Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨
    train_thread = Thread(target=training_job)
    train_thread.daemon = True
    train_thread.start()
    
    # ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨
    port = int(os.environ.get("PORT", 10000))
    logger.info(f"ğŸŒ ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ÙØ° {port} Ù„Ø¥Ø¨Ù‚Ø§Ø¡ Ø§Ù„Ø®Ø¯Ù…Ø© Ù†Ø´Ø·Ø©...")
    app.run(host='0.0.0.0', port=port)
