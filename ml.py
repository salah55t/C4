import time
import os
import json
import logging
import requests
import numpy as np
import pandas as pd
import psycopg2
import pickle
from psycopg2 import sql, OperationalError, InterfaceError
from psycopg2.extras import RealDictCursor
from binance.client import Client
from binance.exceptions import BinanceAPIException, BinanceRequestException
from datetime import datetime, timedelta
from decouple import config
from typing import List, Dict, Optional, Any, Tuple

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…ÙƒØªØ¨Ø§Øª Flask ÙˆØ§Ù„Ø®ÙŠÙˆØ·
from flask import Flask, request, Response
from threading import Thread

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ----------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ml_model_trainer.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('MLTrainer')

# ---------------------- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ© ----------------------
try:
    API_KEY: str = config('BINANCE_API_KEY')
    API_SECRET: str = config('BINANCE_API_SECRET')
    DB_URL: str = config('DATABASE_URL')
    WEBHOOK_URL: Optional[str] = config('WEBHOOK_URL', default=None) # Ø¥Ø¶Ø§ÙØ© WEBHOOK_URL
except Exception as e:
     logger.critical(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {e}")
     exit(1)

logger.info(f"Binance API Key: {'Available' if API_KEY else 'Not available'}")
logger.info(f"Database URL: {'Available' if DB_URL else 'Not available'}")
logger.info(f"Webhook URL: {WEBHOOK_URL if WEBHOOK_URL else 'Not specified'} (Flask will always run for Render)")


# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø«ÙˆØ§Ø¨Øª ÙˆØ§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© ----------------------
SIGNAL_GENERATION_TIMEFRAME: str = '5m' # ØªÙ… Ø§Ù„ØªØºÙŠÙŠØ± Ø¥Ù„Ù‰ 5 Ø¯Ù‚Ø§Ø¦Ù‚ Ù„ÙŠØªÙ†Ø§Ø³Ø¨ Ù…Ø¹ 3 Ø´Ù…Ø¹Ø§Øª = 15 Ø¯Ù‚ÙŠÙ‚Ø©
DATA_LOOKBACK_DAYS_FOR_TRAINING: int = 90 # 3 Ø£Ø´Ù‡Ø± Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
ML_MODEL_NAME: str = 'DecisionTree_Scalping_V1'

# Indicator Parameters (Ù†Ø³Ø® Ù…Ù† c4.py Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§ØªØ³Ø§Ù‚)
# ØªÙ… Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ ÙÙ‚Ø· Ø¨Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
VOLUME_LOOKBACK_CANDLES: int = 3 # Ø¹Ø¯Ø¯ Ø§Ù„Ø´Ù…Ø¹Ø§Øª Ù„Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø­Ø¬Ù… (3 Ø´Ù…Ø¹Ø§Øª * 5 Ø¯Ù‚Ø§Ø¦Ù‚ = 15 Ø¯Ù‚ÙŠÙ‚Ø©)
RSI_PERIOD: int = 9 # Ù…Ø·Ù„ÙˆØ¨ Ù„Ø­Ø³Ø§Ø¨ RSI Ø§Ù„Ø°ÙŠ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„ÙŠÙ‡ RSI Momentum
RSI_MOMENTUM_LOOKBACK_CANDLES: int = 2 # Ø¹Ø¯Ø¯ Ø§Ù„Ø´Ù…Ø¹Ø§Øª Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ²Ø§ÙŠØ¯ RSI Ù„Ù„Ø²Ø®Ù…

# Global variables
conn: Optional[psycopg2.extensions.connection] = None
cur: Optional[psycopg2.extensions.cursor] = None
client: Optional[Client] = None

# Ù…ØªØºÙŠØ±Ø§Øª Ù„ØªØªØ¨Ø¹ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
# ØªÙ… ØªØ¹Ø±ÙŠÙÙ‡Ø§ Ù‡Ù†Ø§ ÙƒÙ…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù…Ø©
training_status: str = "Idle"
last_training_time: Optional[datetime] = None
last_training_metrics: Dict[str, Any] = {}
training_error: Optional[str] = None


# ---------------------- Binance Client Setup ----------------------
try:
    logger.info("â„¹ï¸ [Binance] ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ Binance...")
    client = Client(API_KEY, API_SECRET)
    client.ping()
    server_time = client.get_server_time()
    logger.info(f"âœ… [Binance] ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ Binance. ÙˆÙ‚Øª Ø§Ù„Ø®Ø§Ø¯Ù…: {datetime.fromtimestamp(server_time['serverTime']/1000)}")
except BinanceRequestException as req_err:
     logger.critical(f"âŒ [Binance] Ø®Ø·Ø£ ÙÙŠ Ø·Ù„Ø¨ Binance (Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© Ø£Ùˆ Ø§Ù„Ø·Ù„Ø¨): {req_err}")
     exit(1)
except BinanceAPIException as api_err:
     logger.critical(f"âŒ [Binance] Ø®Ø·Ø£ ÙÙŠ ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Binance (Ù…ÙØ§ØªÙŠØ­ ØºÙŠØ± ØµØ§Ù„Ø­Ø© Ø£Ùˆ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„Ø®Ø§Ø¯Ù…): {api_err}")
     exit(1)
except Exception as e:
    logger.critical(f"âŒ [Binance] ÙØ´Ù„ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ Binance: {e}")
    exit(1)

# ---------------------- Database Connection Setup (Ù†Ø³Ø® Ù…Ù† c4.py) ----------------------
def init_db(retries: int = 5, delay: int = 5) -> None:
    """Initializes database connection and creates tables if they don't exist."""
    global conn, cur
    logger.info("[DB] Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    for attempt in range(retries):
        try:
            logger.info(f"[DB] Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}/{retries})...")
            conn = psycopg2.connect(DB_URL, connect_timeout=10, cursor_factory=RealDictCursor)
            conn.autocommit = False
            cur = conn.cursor()
            logger.info("âœ… [DB] ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")

            # --- Create or update signals table (Modified schema) ---
            logger.info("[DB] Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†/Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ 'signals'...")
            cur.execute("""
                CREATE TABLE IF NOT EXISTS signals (
                    id SERIAL PRIMARY KEY,
                    symbol TEXT NOT NULL,
                    entry_price DOUBLE PRECISION NOT NULL,
                    initial_target DOUBLE PRECISION NOT NULL,
                    current_target DOUBLE PRECISION NOT NULL,
                    r2_score DOUBLE PRECISION,
                    volume_15m DOUBLE PRECISION,
                    achieved_target BOOLEAN DEFAULT FALSE,
                    closing_price DOUBLE PRECISION,
                    closed_at TIMESTAMP,
                    sent_at TIMESTAMP DEFAULT NOW(),
                    entry_time TIMESTAMP DEFAULT NOW(),
                    time_to_target INTERVAL,
                    profit_percentage DOUBLE PRECISION,
                    strategy_name TEXT,
                    signal_details JSONB
                );""")
            conn.commit()
            logger.info("âœ… [DB] Ø¬Ø¯ÙˆÙ„ 'signals' Ù…ÙˆØ¬ÙˆØ¯ Ø£Ùˆ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡.")

            # --- Create ml_models table (NEW) ---
            logger.info("[DB] Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†/Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ 'ml_models'...")
            cur.execute("""
                CREATE TABLE IF NOT EXISTS ml_models (
                    id SERIAL PRIMARY KEY,
                    model_name TEXT NOT NULL UNIQUE,
                    model_data BYTEA NOT NULL,
                    trained_at TIMESTAMP DEFAULT NOW(),
                    metrics JSONB
                );""")
            conn.commit()
            logger.info("âœ… [DB] Ø¬Ø¯ÙˆÙ„ 'ml_models' Ù…ÙˆØ¬ÙˆØ¯ Ø£Ùˆ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡.")

            # --- Create market_dominance table (if it doesn't exist) ---
            logger.info("[DB] Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†/Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ 'market_dominance'...")
            cur.execute("""
                CREATE TABLE IF NOT EXISTS market_dominance (
                    id SERIAL PRIMARY KEY,
                    recorded_at TIMESTAMP DEFAULT NOW(),
                    btc_dominance DOUBLE PRECISION,
                    eth_dominance DOUBLE PRECISION
                );
            """)
            conn.commit()
            logger.info("âœ… [DB] Ø¬Ø¯ÙˆÙ„ 'market_dominance' Ù…ÙˆØ¬ÙˆØ¯ Ø£Ùˆ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡.")

            logger.info("âœ… [DB] ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")
            return

        except OperationalError as op_err:
            logger.error(f"âŒ [DB] Ø®Ø·Ø£ ØªØ´ØºÙŠÙ„ÙŠ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ (Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}): {op_err}")
            if conn: conn.rollback()
            if attempt == retries - 1:
                 logger.critical("âŒ [DB] ÙØ´Ù„Øª Ø¬Ù…ÙŠØ¹ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
                 raise op_err
            time.sleep(delay)
        except Exception as e:
            logger.critical(f"âŒ [DB] ÙØ´Ù„ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}): {e}", exc_info=True)
            if conn: conn.rollback()
            if attempt == retries - 1:
                 logger.critical("âŒ [DB] ÙØ´Ù„Øª Ø¬Ù…ÙŠØ¹ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
                 raise e
            time.sleep(delay)

    logger.critical("âŒ [DB] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø¹Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø§Øª.")
    exit(1)


def check_db_connection() -> bool:
    """Checks database connection status and re-initializes if necessary."""
    global conn, cur
    try:
        if conn is None or conn.closed != 0:
            logger.warning("âš ï¸ [DB] Ø§Ù„Ø§ØªØµØ§Ù„ Ù…ØºÙ„Ù‚ Ø£Ùˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙ‡ÙŠØ¦Ø©...")
            init_db()
            return True
        else:
             with conn.cursor() as check_cur:
                  check_cur.execute("SELECT 1;")
                  check_cur.fetchone()
             return True
    except (OperationalError, InterfaceError) as e:
        logger.error(f"âŒ [DB] ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ({e}). Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙ‡ÙŠØ¦Ø©...")
        try:
             init_db()
             return True
        except Exception as recon_err:
            logger.error(f"âŒ [DB] ÙØ´Ù„Øª Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø¹Ø¯ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ø§ØªØµØ§Ù„: {recon_err}")
            return False
    except Exception as e:
        logger.error(f"âŒ [DB] Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„: {e}", exc_info=True)
        try:
            init_db()
            return True
        except Exception as recon_err:
             logger.error(f"âŒ [DB] ÙØ´Ù„Øª Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø¹Ø¯ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {recon_err}")
             return False

def convert_np_values(obj: Any) -> Any:
    """Converts NumPy data types to native Python types for JSON and DB compatibility."""
    if isinstance(obj, dict):
        return {k: convert_np_values(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_np_values(item) for item in obj]
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (np.integer, np.int_)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64)):
        return float(obj)
    elif isinstance(obj, (np.bool_)):
        return bool(obj)
    elif pd.isna(obj):
        return None
    else:
        return obj

def get_crypto_symbols(filename: str = 'crypto_list.txt') -> List[str]:
    """
    Reads the list of currency symbols from a text file, then validates them
    as valid USDT pairs available for Spot trading on Binance. (Ù†Ø³Ø® Ù…Ù† c4.py)
    """
    raw_symbols: List[str] = []
    logger.info(f"â„¹ï¸ [Data] Ù‚Ø±Ø§Ø¡Ø© Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ù…Ù† Ø§Ù„Ù…Ù„Ù '{filename}'...")
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, filename)

        if not os.path.exists(file_path):
            file_path = os.path.abspath(filename)
            if not os.path.exists(file_path):
                 logger.error(f"âŒ [Data] Ø§Ù„Ù…Ù„Ù '{filename}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ø£Ùˆ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠ.")
                 return []
            else:
                 logger.warning(f"âš ï¸ [Data] Ø§Ù„Ù…Ù„Ù '{filename}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ù„Ù ÙÙŠ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠ: '{file_path}'")

        with open(file_path, 'r', encoding='utf-8') as f:
            raw_symbols = [f"{line.strip().upper().replace('USDT', '')}USDT"
                           for line in f if line.strip() and not line.startswith('#')]
        raw_symbols = sorted(list(set(raw_symbols)))
        logger.info(f"â„¹ï¸ [Data] ØªÙ… Ù‚Ø±Ø§Ø¡Ø© {len(raw_symbols)} Ø±Ù…Ø²Ù‹Ø§ Ù…Ø¨Ø¯Ø¦ÙŠÙ‹Ø§ Ù…Ù† '{file_path}'.")

    except FileNotFoundError:
         logger.error(f"âŒ [Data] Ø§Ù„Ù…Ù„Ù '{filename}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")
         return []
    except Exception as e:
        logger.error(f"âŒ [Data] Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù '{filename}': {e}", exc_info=True)
        return []

    if not raw_symbols:
         logger.warning("âš ï¸ [Data] Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø£ÙˆÙ„ÙŠØ© ÙØ§Ø±ØºØ©.")
         return []

    if not client:
        logger.error("âŒ [Data Validation] Ø¹Ù…ÙŠÙ„ Binance ØºÙŠØ± Ù…Ù‡ÙŠØ£. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ².")
        return raw_symbols

    try:
        logger.info("â„¹ï¸ [Data Validation] Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ­Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø§ÙˆÙ„ Ù…Ù† Binance API...")
        exchange_info = client.get_exchange_info()
        valid_trading_usdt_symbols = {
            s['symbol'] for s in exchange_info['symbols']
            if s.get('quoteAsset') == 'USDT' and
               s.get('status') == 'TRADING' and
               s.get('isSpotTradingAllowed') is True
        }
        logger.info(f"â„¹ï¸ [Data Validation] ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(valid_trading_usdt_symbols)} Ø²ÙˆØ¬ ØªØ¯Ø§ÙˆÙ„ USDT ØµØ§Ù„Ø­ ÙÙŠ Spot Ø¹Ù„Ù‰ Binance.")
        validated_symbols = [symbol for symbol in raw_symbols if symbol in valid_trading_usdt_symbols]

        removed_count = len(raw_symbols) - len(validated_symbols)
        if removed_count > 0:
            removed_symbols = set(raw_symbols) - set(validated_symbols)
            logger.warning(f"âš ï¸ [Data Validation] ØªÙ… Ø¥Ø²Ø§Ù„Ø© {removed_count} Ø±Ù…Ø² ØªØ¯Ø§ÙˆÙ„ USDT ØºÙŠØ± ØµØ§Ù„Ø­ Ø£Ùˆ ØºÙŠØ± Ù…ØªØ§Ø­ Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©: {', '.join(removed_symbols)}")

        logger.info(f"âœ… [Data Validation] ØªÙ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ². Ø§Ø³ØªØ®Ø¯Ø§Ù… {len(validated_symbols)} Ø±Ù…Ø²Ù‹Ø§ ØµØ§Ù„Ø­Ù‹Ø§.")
        return validated_symbols

    except (BinanceAPIException, BinanceRequestException) as binance_err:
         logger.error(f"âŒ [Data Validation] Ø®Ø·Ø£ ÙÙŠ Binance API Ø£Ùˆ Ø§Ù„Ø´Ø¨ÙƒØ© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ²: {binance_err}")
         logger.warning("âš ï¸ [Data Validation] Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø¨Ø¯ÙˆÙ† Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Binance.")
         return raw_symbols
    except Exception as api_err:
         logger.error(f"âŒ [Data Validation] Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø±Ù…ÙˆØ² Binance: {api_err}", exc_info=True)
         logger.warning("âš ï¸ [Data Validation] Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø¨Ø¯ÙˆÙ† Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Binance.")
         return raw_symbols


def fetch_historical_data(symbol: str, interval: str, days: int) -> Optional[pd.DataFrame]:
    """
    Fetches historical candlestick data from Binance for a specified number of days.
    This function relies on python-binance's get_historical_klines to handle
    internal pagination for large data ranges.
    """
    if not client:
        logger.error("âŒ [Data] Ø¹Ù…ÙŠÙ„ Binance ØºÙŠØ± Ù…Ù‡ÙŠØ£ Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        return None
    try:
        # Calculate the start date for the entire data range needed
        start_dt = datetime.utcnow() - timedelta(days=days + 1)
        start_str_overall = start_dt.strftime("%Y-%m-%d %H:%M:%S")

        logger.debug(f"â„¹ï¸ [Data] Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª {interval} Ù„Ù€ {symbol} Ù…Ù† {start_str_overall} Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†...")

        # Call get_historical_klines for the entire period.
        # The python-binance library is designed to handle internal pagination
        # if the requested range exceeds the API's single-request limit (e.g., 1000 klines).
        klines = client.get_historical_klines(symbol, interval, start_str_overall)

        if not klines:
            logger.warning(f"âš ï¸ [Data] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ§Ø±ÙŠØ®ÙŠØ© ({interval}) Ù„Ù€ {symbol} Ù„Ù„ÙØªØ±Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©.")
            return None

        df = pd.DataFrame(klines, columns=[
            'timestamp', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_volume', 'trades', 'taker_buy_base', 'taker_buy_quote', 'ignore'
        ])

        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce')

        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        df = df[numeric_cols]
        initial_len = len(df)
        df.dropna(subset=numeric_cols, inplace=True)

        if len(df) < initial_len:
            logger.debug(f"â„¹ï¸ [Data] {symbol}: ØªÙ… Ø¥Ø³Ù‚Ø§Ø· {initial_len - len(df)} ØµÙÙ‹Ø§ Ø¨Ø³Ø¨Ø¨ Ù‚ÙŠÙ… NaN ÙÙŠ Ø¨ÙŠØ§Ù†Ø§Øª OHLCV.")

        if df.empty:
            logger.warning(f"âš ï¸ [Data] DataFrame Ù„Ù€ {symbol} ÙØ§Ø±Øº Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ù‚ÙŠÙ… NaN Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©.")
            return None

        # Sort by index (timestamp) to ensure chronological order
        df.sort_index(inplace=True)

        logger.debug(f"âœ… [Data] ØªÙ… Ø¬Ù„Ø¨ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© {len(df)} Ø´Ù…Ø¹Ø© ØªØ§Ø±ÙŠØ®ÙŠØ© ({interval}) Ù„Ù€ {symbol}.")
        return df

    except BinanceAPIException as api_err:
         logger.error(f"âŒ [Data] Ø®Ø·Ø£ ÙÙŠ Binance API Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol}: {api_err}")
         return None
    except BinanceRequestException as req_err:
         logger.error(f"âŒ [Data] Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø·Ù„Ø¨ Ø£Ùˆ Ø§Ù„Ø´Ø¨ÙƒØ© Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol}: {req_err}")
         return None
    except Exception as e:
        logger.error(f"âŒ [Data] Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ù€ {symbol}: {e}", exc_info=True)
        return None

# ---------------------- Technical Indicator Functions (Only those needed for ML features) ----------------------
def calculate_ema(series: pd.Series, span: int) -> pd.Series:
    """Calculates Exponential Moving Average (EMA)."""
    if series is None or series.isnull().all() or len(series) < span:
        return pd.Series(index=series.index if series is not None else None, dtype=float)
    return series.ewm(span=span, adjust=False).mean()

def calculate_rsi_indicator(df: pd.DataFrame, period: int = RSI_PERIOD) -> pd.DataFrame:
    """Calculates Relative Strength Index (RSI)."""
    df = df.copy()
    if 'close' not in df.columns or df['close'].isnull().all():
        logger.warning("âš ï¸ [Indicator RSI] Ø¹Ù…ÙˆØ¯ 'close' Ù…ÙÙ‚ÙˆØ¯ Ø£Ùˆ ÙØ§Ø±Øº.")
        df['rsi'] = np.nan
        return df
    if len(df) < period:
        logger.warning(f"âš ï¸ [Indicator RSI] Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© ({len(df)} < {period}) Ù„Ø­Ø³Ø§Ø¨ RSI.")
        df['rsi'] = np.nan
        return df

    delta = df['close'].diff()
    gain = delta.clip(lower=0)
    loss = -delta.clip(upper=0)

    avg_gain = gain.ewm(com=period - 1, adjust=False).mean()
    avg_loss = loss.ewm(com=period - 1, adjust=False).mean()

    rs = avg_gain / avg_loss.replace(0, np.nan)

    rsi_series = 100 - (100 / (1 + rs))
    df['rsi'] = rsi_series.ffill().fillna(50)

    return df


# ---------------------- ÙˆØ¸Ø§Ø¦Ù ØªØ¯Ø±ÙŠØ¨ ÙˆØ­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ----------------------
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler

def prepare_data_for_ml(df: pd.DataFrame, symbol: str, target_period: int = 5) -> Optional[pd.DataFrame]:
    """
    ÙŠØ¬Ù‡Ø² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.
    ÙŠØ¶ÙŠÙ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª (ÙÙ‚Ø· Ø­Ø¬Ù… Ø§Ù„Ø³ÙŠÙˆÙ„Ø© ÙˆÙ…Ø¤Ø´Ø± Ø§Ù„Ø²Ø®Ù…) ÙˆÙŠØ²ÙŠÙ„ Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚ÙŠÙ… NaN.
    ÙŠØ¶ÙŠÙ Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‡Ø¯Ù 'target' Ø§Ù„Ø°ÙŠ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¹Ø± Ø³ÙŠØ±ØªÙØ¹ ÙÙŠ Ø§Ù„Ø´Ù…ÙˆØ¹ Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©.
    """
    logger.info(f"â„¹ï¸ [ML Prep] ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ù€ {symbol} (Ø­Ø¬Ù… Ø§Ù„Ø³ÙŠÙˆÙ„Ø© ÙˆØ§Ù„Ø²Ø®Ù… ÙÙ‚Ø·)...")

    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ø·ÙˆÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙÙ‚Ø· Ù„Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
    min_len_required = max(VOLUME_LOOKBACK_CANDLES, RSI_PERIOD + RSI_MOMENTUM_LOOKBACK_CANDLES) + target_period + 5

    if len(df) < min_len_required:
        logger.warning(f"âš ï¸ [ML Prep] DataFrame Ù„Ù€ {symbol} Ù‚ØµÙŠØ± Ø¬Ø¯Ù‹Ø§ ({len(df)} < {min_len_required}) Ù„ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        return None

    df_calc = df.copy()

    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙÙ‚Ø·: Ù…ØªÙˆØ³Ø· Ø­Ø¬Ù… Ø§Ù„Ø³ÙŠÙˆÙ„Ø© Ù„Ø¢Ø®Ø± 15 Ø¯Ù‚ÙŠÙ‚Ø© (3 Ø´Ù…Ø¹Ø§Øª 5m)
    df_calc['volume_15m_avg'] = df_calc['volume'].rolling(window=VOLUME_LOOKBACK_CANDLES, min_periods=1).mean()
    logger.debug(f"â„¹ï¸ [ML Prep] ØªÙ… Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø­Ø¬Ù… 15 Ø¯Ù‚ÙŠÙ‚Ø© Ù„Ù€ {symbol}.")

    # Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø± Ø§Ù„Ù‚ÙˆØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ© (RSI) Ù„Ø£Ù†Ù‡ Ù…Ø·Ù„ÙˆØ¨ Ù„Ù…Ø¤Ø´Ø± Ø§Ù„Ø²Ø®Ù…
    df_calc = calculate_rsi_indicator(df_calc, RSI_PERIOD)

    # Ø¥Ø¶Ø§ÙØ© Ù…Ø¤Ø´Ø± Ø²Ø®Ù… ØµØ¹ÙˆØ¯ÙŠ (RSI Momentum)
    df_calc['rsi_momentum_bullish'] = 0
    if len(df_calc) >= RSI_MOMENTUM_LOOKBACK_CANDLES + 1:
        # Check if RSI is increasing over the last N candles and is above 50 (bullish territory)
        for i in range(RSI_MOMENTUM_LOOKBACK_CANDLES, len(df_calc)):
            rsi_slice = df_calc['rsi'].iloc[i - RSI_MOMENTUM_LOOKBACK_CANDLES : i + 1]
            if not rsi_slice.isnull().any() and np.all(np.diff(rsi_slice) > 0) and rsi_slice.iloc[-1] > 50:
                df_calc.loc[df_calc.index[i], 'rsi_momentum_bullish'] = 1
    logger.debug(f"â„¹ï¸ [ML Prep] ØªÙ… Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø± Ø²Ø®Ù… RSI Ø§Ù„ØµØ¹ÙˆØ¯ÙŠ Ù„Ù€ {symbol}.")


    # ØªØ¹Ø±ÙŠÙ Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ØªÙŠ Ø³ÙŠØ³ØªØ®Ø¯Ù…Ù‡Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙ‚Ø· Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©)
    feature_columns = [
        'volume_15m_avg', # Ù…ÙŠØ²Ø© Ø¬Ø¯ÙŠØ¯Ø©: Ù…ØªÙˆØ³Ø· Ø­Ø¬Ù… Ø§Ù„Ø³ÙŠÙˆÙ„Ø© Ù„Ø¢Ø®Ø± 15 Ø¯Ù‚ÙŠÙ‚Ø©
        'rsi_momentum_bullish' # Ù…ÙŠØ²Ø© Ø¬Ø¯ÙŠØ¯Ø©: Ø²Ø®Ù… RSI Ø§Ù„ØµØ¹ÙˆØ¯ÙŠ
    ]

    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¬Ù…ÙŠØ¹ Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…
    for col in feature_columns:
        if col not in df_calc.columns:
            logger.warning(f"âš ï¸ [ML Prep] Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù…ÙŠØ²Ø© Ø§Ù„Ù…ÙÙ‚ÙˆØ¯: {col}. Ø³ÙŠØªÙ… Ø¥Ø¶Ø§ÙØªÙ‡ ÙƒÙ€ NaN.")
            df_calc[col] = np.nan
        else:
            df_calc[col] = pd.to_numeric(df_calc[col], errors='coerce')

    # Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‡Ø¯Ù: Ù‡Ù„ Ø§Ù„Ø³Ø¹Ø± Ø³ÙŠØµØ¹Ø¯ Ø¨Ù†Ø³Ø¨Ø© Ù…Ø¹ÙŠÙ†Ø© ÙÙŠ Ø§Ù„Ø´Ù…ÙˆØ¹ Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©ØŸ
    # Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¹Ø± Ø³ÙŠØµØ¹Ø¯ Ø¨Ù†Ø³Ø¨Ø© 0.5% Ø®Ù„Ø§Ù„ Ø§Ù„Ø´Ù…ÙˆØ¹ Ø§Ù„Ø®Ù…Ø³ Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©
    price_change_threshold = 0.005 # 0.5%
    # Ensure 'close' column is numeric before shifting
    df_calc['close'] = pd.to_numeric(df_calc['close'], errors='coerce')
    df_calc['future_max_close'] = df_calc['close'].shift(-target_period).rolling(window=target_period, min_periods=1).max()
    # Corrected target calculation: check if future max close is significantly higher than current close
    df_calc['target'] = ((df_calc['future_max_close'] / df_calc['close']) - 1 > price_change_threshold).astype(int)


    # Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚ÙŠÙ… NaN Ø¨Ø¹Ø¯ Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙˆØ§Ù„Ù‡Ø¯Ù
    initial_len = len(df_calc)
    df_cleaned = df_calc.dropna(subset=feature_columns + ['target']).copy()
    dropped_count = initial_len - len(df_cleaned)

    if dropped_count > 0:
        logger.info(f"â„¹ï¸ [ML Prep] Ù„Ù€ {symbol}: ØªÙ… Ø¥Ø³Ù‚Ø§Ø· {dropped_count} ØµÙÙ‹Ø§ Ø¨Ø³Ø¨Ø¨ Ù‚ÙŠÙ… NaN Ø¨Ø¹Ø¯ Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙˆØ§Ù„Ù‡Ø¯Ù.")
    if df_cleaned.empty:
        logger.warning(f"âš ï¸ [ML Prep] DataFrame Ù„Ù€ {symbol} ÙØ§Ø±Øº Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ù‚ÙŠÙ… NaN Ù„ØªØ¬Ù‡ÙŠØ² ML.")
        return None

    logger.info(f"âœ… [ML Prep] ØªÙ… ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol} Ø¨Ù†Ø¬Ø§Ø­. Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ: {len(df_cleaned)}")
    return df_cleaned[feature_columns + ['target']]


def train_and_evaluate_model(data: pd.DataFrame) -> Tuple[Any, Dict[str, Any]]:
    """
    ÙŠÙ‚ÙˆÙ… Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Decision Tree ÙˆÙŠÙ‚ÙŠÙ… Ø£Ø¯Ø§Ø¡Ù‡.
    """
    logger.info("â„¹ï¸ [ML Train] Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")

    if data.empty:
        logger.error("âŒ [ML Train] DataFrame ÙØ§Ø±Øº Ù„Ù„ØªØ¯Ø±ÙŠØ¨.")
        return None, {}

    X = data.drop('target', axis=1)
    y = data['target']

    if X.empty or y.empty:
        logger.error("âŒ [ML Train] Ù…ÙŠØ²Ø§Øª Ø£Ùˆ Ø£Ù‡Ø¯Ø§Ù ÙØ§Ø±ØºØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨.")
        return None, {}

    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… stratify=y Ù„Ø¶Ù…Ø§Ù† ØªÙˆØ²ÙŠØ¹ Ù…ØªÙˆØ§Ø²Ù† Ù„Ù„ÙØ¦Ø§Øª ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    try:
        X_train, X_test, y_train = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    except ValueError as ve:
        logger.warning(f"âš ï¸ [ML Train] Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… stratify Ø¨Ø³Ø¨Ø¨ ÙˆØ¬ÙˆØ¯ ÙØ¦Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙŠ Ø§Ù„Ù‡Ø¯Ù: {ve}. Ø³ÙŠØªÙ… Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø¨Ø¯ÙˆÙ† stratify.")
        X_train, X_test, y_train = train_test_split(X, y, test_size=0.2, random_state=42)

    # Ø§Ù„ØªØ­Ø¬ÙŠÙ… (Scaling) Ù„Ù„Ù…ÙŠØ²Ø§Øª (Ù…Ù‡Ù… Ù„Ø¨Ø¹Ø¶ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ØŒ ÙˆÙ„ÙŠØ³ Ø¨Ø§Ù„Ø¶Ø±ÙˆØ±Ø© Ù„Ù€ Decision Tree ÙˆÙ„ÙƒÙ† Ù…Ù…Ø§Ø±Ø³Ø© Ø¬ÙŠØ¯Ø©)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Decision Tree Classifier
    model = DecisionTreeClassifier(random_state=42, max_depth=10) # ÙŠÙ…ÙƒÙ† ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª
    model.fit(X_train_scaled, y_train)
    logger.info("âœ… [ML Train] ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­.")

    # Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)

    metrics = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'num_samples_trained': len(X_train),
        'num_samples_tested': len(X_test),
        'feature_names': X.columns.tolist() # Ø­ÙØ¸ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªÙ†Ø§Ø³Ù‚ Ø¹Ù†Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„
    }

    logger.info(f"ğŸ“Š [ML Train] Ù…Ù‚Ø§ÙŠÙŠØ³ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:")
    logger.info(f"  - Ø§Ù„Ø¯Ù‚Ø© (Accuracy): {accuracy:.4f}")
    logger.info(f"  - Ø§Ù„Ø¯Ù‚Ø© (Precision): {precision:.4f}")
    logger.info(f"  - Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ (Recall): {recall:.4f}")
    logger.info(f"  - Ù…Ù‚ÙŠØ§Ø³ F1: {f1:.4f}")

    return model, metrics

def save_ml_model_to_db(model: Any, model_name: str, metrics: Dict[str, Any]) -> bool:
    """
    ÙŠØ­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ ÙˆØ¨ÙŠØ§Ù†Ø§ØªÙ‡ Ø§Ù„ÙˆØµÙÙŠØ© (Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³) ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.
    """
    logger.info(f"â„¹ï¸ [DB Save] Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„Ø­ÙØ¸...")
    if not check_db_connection() or not conn:
        logger.error("âŒ [DB Save] Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ ML Ø¨Ø³Ø¨Ø¨ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        return False

    logger.info(f"â„¹ï¸ [DB Save] Ù…Ø­Ø§ÙˆÙ„Ø© Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ ML '{model_name}' ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    try:
        # ØªØ³Ù„Ø³Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… pickle
        model_binary = pickle.dumps(model)
        logger.info(f"âœ… [DB Save] ØªÙ… ØªØ³Ù„Ø³Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­. Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {len(model_binary)} Ø¨Ø§ÙŠØª.")

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¥Ù„Ù‰ JSONB
        metrics_json = json.dumps(convert_np_values(metrics))
        logger.info(f"âœ… [DB Save] ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¥Ù„Ù‰ JSON Ø¨Ù†Ø¬Ø§Ø­.")

        with conn.cursor() as db_cur:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§ Ø¨Ø§Ù„ÙØ¹Ù„ (Ù„Ù„ØªØ­Ø¯ÙŠØ« Ø£Ùˆ Ø§Ù„Ø¥Ø¯Ø±Ø§Ø¬)
            db_cur.execute("SELECT id FROM ml_models WHERE model_name = %s;", (model_name,))
            existing_model = db_cur.fetchone()

            if existing_model:
                logger.info(f"â„¹ï¸ [DB Save] Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ '{model_name}' Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„ÙØ¹Ù„. Ø³ÙŠØªÙ… ØªØ­Ø¯ÙŠØ«Ù‡.")
                update_query = sql.SQL("""
                    UPDATE ml_models
                    SET model_data = %s, trained_at = NOW(), metrics = %s
                    WHERE id = %s;
                """)
                db_cur.execute(update_query, (model_binary, metrics_json, existing_model['id']))
                logger.info(f"âœ… [DB Save] ØªÙ… ØªØ­Ø¯ÙŠØ« Ù†Ù…ÙˆØ°Ø¬ ML '{model_name}' ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")
            else:
                logger.info(f"â„¹ï¸ [DB Save] Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ '{model_name}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ø³ÙŠØªÙ… Ø¥Ø¯Ø±Ø§Ø¬Ù‡ ÙƒÙ†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙŠØ¯.")
                insert_query = sql.SQL("""
                    INSERT INTO ml_models (model_name, model_data, trained_at, metrics)
                    VALUES (%s, %s, NOW(), %s);
                """)
                db_cur.execute(insert_query, (model_name, model_binary, metrics_json))
                logger.info(f"âœ… [DB Save] ØªÙ… Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ ML '{model_name}' Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")
        conn.commit()
        logger.info(f"âœ… [DB Save] ØªÙ… ØªÙ†ÙÙŠØ° commit Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")
        return True
    except psycopg2.Error as db_err:
        logger.error(f"âŒ [DB Save] Ø®Ø·Ø£ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø«Ù†Ø§Ø¡ Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ ML: {db_err}", exc_info=True)
        if conn: conn.rollback()
        return False
    except pickle.PicklingError as pickle_err:
        logger.error(f"âŒ [DB Save] Ø®Ø·Ø£ ÙÙŠ ØªØ³Ù„Ø³Ù„ Ù†Ù…ÙˆØ°Ø¬ ML: {pickle_err}", exc_info=True)
        if conn: conn.rollback()
        return False
    except Exception as e:
        logger.error(f"âŒ [DB Save] Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ ML: {e}", exc_info=True)
        if conn: conn.rollback()
        return False

def cleanup_resources() -> None:
    """Closes used resources like the database connection."""
    global conn
    logger.info("â„¹ï¸ [Cleanup] Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯...")
    if conn:
        try:
            conn.close()
            logger.info("âœ… [DB] ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        except Exception as close_err:
            logger.error(f"âš ï¸ [DB] Ø®Ø·Ø£ ÙÙŠ Ø¥ØºÙ„Ø§Ù‚ Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {close_err}")
    logger.info("âœ… [Cleanup] Ø§ÙƒØªÙ…Ù„ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯.")


# ---------------------- Flask Service ----------------------
app = Flask(__name__)

@app.route('/')
def home() -> Response:
    """Simple home page to show the bot is running."""
    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    status_message = (
        f"ğŸ¤– *ML Trainer Service Status:*\n"
        f"- Current Time: {now}\n"
        f"- Training Status: *{training_status}*\n"
    )
    if last_training_time:
        status_message += f"- Last Training Time: {last_training_time.strftime('%Y-%m-%d %H:%M:%S')}\n"
    if last_training_metrics:
        status_message += f"- Last Training Metrics (Accuracy): {last_training_metrics.get('accuracy', 'N/A'):.4f}\n"
    if training_error:
        status_message += f"- Last Error: {training_error}\n"

    return Response(status_message, status=200, mimetype='text/plain')

@app.route('/favicon.ico')
def favicon() -> Response:
    """Handles favicon request to avoid 404 errors in logs."""
    return Response(status=204)

def run_flask_service() -> None:
    """Runs the Flask application."""
    host = "0.0.0.0"
    port = int(os.environ.get('PORT', 10000))
    logger.info(f"â„¹ï¸ [Flask] Ø¨Ø¯Ø¡ ØªØ·Ø¨ÙŠÙ‚ Flask Ø¹Ù„Ù‰ {host}:{port}...")
    try:
        from waitress import serve
        logger.info("âœ… [Flask] Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø§Ø¯Ù… 'waitress'.")
        serve(app, host=host, port=port, threads=6)
    except ImportError:
        logger.warning("âš ï¸ [Flask] 'waitress' ØºÙŠØ± Ù…Ø«Ø¨Øª. Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø®Ø§Ø¯Ù… ØªØ·ÙˆÙŠØ± Flask (Ù„Ø§ ÙŠÙˆØµÙ‰ Ø¨Ù‡ Ù„Ù„Ø¥Ù†ØªØ§Ø¬).")
        try:
            app.run(host=host, port=port)
        except Exception as flask_run_err:
            logger.critical(f"âŒ [Flask] ÙØ´Ù„ Ø¨Ø¯Ø¡ Ø®Ø§Ø¯Ù… Ø§Ù„ØªØ·ÙˆÙŠØ±: {flask_run_err}", exc_info=True)
    except Exception as serve_err:
        logger.critical(f"âŒ [Flask] ÙØ´Ù„ Ø¨Ø¯Ø¡ Ø§Ù„Ø®Ø§Ø¯Ù… (waitressØŸ): {serve_err}", exc_info=True)


# ---------------------- Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ----------------------
if __name__ == "__main__":
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø³ÙƒØ±ÙŠØ¨Øª ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ...")
    logger.info(f"Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø­Ù„ÙŠ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | ÙˆÙ‚Øª UTC: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}")

    flask_thread: Optional[Thread] = None

    try:
        # 1. Ø¨Ø¯Ø¡ Ø®Ø¯Ù…Ø© Flask ÙÙŠ Ø®ÙŠØ· Ù…Ù†ÙØµÙ„ Ø£ÙˆÙ„Ø§Ù‹
        # Ù‡Ø°Ø§ ÙŠØ¶Ù…Ù† Ø£Ù† Ø§Ù„Ø®Ø¯Ù…Ø© Ø³ØªÙƒÙˆÙ† Ù…ØªØ§Ø­Ø© Ù„Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ø·Ù„Ø¨Ø§Øª Uptime Monitor
        # Ø¨ÙŠÙ†Ù…Ø§ ÙŠØªÙ… ØªÙ†ÙÙŠØ° Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙŠ Ø§Ù„Ø®ÙŠØ· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ.
        flask_thread = Thread(target=run_flask_service, daemon=False, name="FlaskServiceThread")
        flask_thread.start()
        logger.info("âœ… [Main] ØªÙ… Ø¨Ø¯Ø¡ Ø®Ø¯Ù…Ø© Flask.")
        time.sleep(2) # Ø¥Ø¹Ø·Ø§Ø¡ Ø¨Ø¹Ø¶ Ø§Ù„ÙˆÙ‚Øª Ù„Ù€ Flask Ù„Ù„Ø¨Ø¯Ø¡

        # 2. ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        init_db()

        # 3. Ø¬Ù„Ø¨ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ù…ÙˆØ²
        symbols = get_crypto_symbols()
        if not symbols:
            logger.critical("âŒ [Main] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø±Ù…ÙˆØ² ØµØ§Ù„Ø­Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† 'crypto_list.txt'.")
            training_status = "Failed: No valid symbols"
            exit(1)

        training_status = "In Progress: Fetching Data"
        training_error = None # Reset error

        all_data_for_training = pd.DataFrame()
        logger.info(f"â„¹ï¸ [Main] Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ù€ {len(symbols)} Ø±Ù…Ø²Ù‹Ø§ Ù„Ù„ØªØ¯Ø±ÙŠØ¨...")

        # 4. Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ¯Ù…Ø¬Ù‡Ø§
        for symbol in symbols:
            logger.info(f"â³ [Main] Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol}...")
            df_hist = fetch_historical_data(symbol, interval=SIGNAL_GENERATION_TIMEFRAME, days=DATA_LOOKBACK_DAYS_FOR_TRAINING)
            if df_hist is not None and not df_hist.empty:
                df_hist['symbol'] = symbol # Ø¥Ø¶Ø§ÙØ© Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø±Ù…Ø² Ù„ØªØ­Ø¯ÙŠØ¯ Ù…ØµØ¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                all_data_for_training = pd.concat([all_data_for_training, df_hist])
                logger.info(f"âœ… [Main] ØªÙ… Ø¬Ù„Ø¨ {len(df_hist)} Ø´Ù…Ø¹Ø© Ù„Ù€ {symbol}.")
            else:
                logger.warning(f"âš ï¸ [Main] Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ù…Ù† Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù€ {symbol}. ØªØ®Ø·ÙŠ.")
            time.sleep(0.1) # ØªØ£Ø®ÙŠØ± Ù„ØªØ¬Ù†Ø¨ Ø­Ø¯ÙˆØ¯ Ù…Ø¹Ø¯Ù„ API

        if all_data_for_training.empty:
            logger.critical("âŒ [Main] Ù„Ù… ÙŠØªÙ… Ø¬Ù„Ø¨ Ø£ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù† Ø£ÙŠ Ø±Ù…Ø². Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©.")
            training_status = "Failed: No sufficient data"
            exit(1)

        logger.info(f"âœ… [Main] ØªÙ… Ø¬Ù„Ø¨ Ø¥Ø¬Ù…Ø§Ù„ÙŠ {len(all_data_for_training)} ØµÙÙ‹Ø§ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ù…ÙˆØ².")

        # 5. ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ
        training_status = "In Progress: Preparing Data"
        processed_dfs = []
        # Group by symbol and process each symbol's data independently
        for symbol, group_df in all_data_for_training.groupby('symbol'):
            if not group_df.empty:
                df_processed = prepare_data_for_ml(group_df.drop(columns=['symbol']), symbol)
                if df_processed is not None and not df_processed.empty:
                    processed_dfs.append(df_processed)
            else:
                logger.warning(f"âš ï¸ [Main] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø®Ø§Ù… Ù„Ù€ {symbol} Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙˆÙ„ÙŠ.")


        if not processed_dfs:
            logger.critical("âŒ [Main] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ù…Ø¤Ø´Ø±Ø§Øª.")
            training_status = "Failed: No processed data"
            exit(1)

        final_training_data = pd.concat(processed_dfs).reset_index(drop=True)
        logger.info(f"âœ… [Main] ØªÙ… ØªØ¬Ù‡ÙŠØ² Ø¥Ø¬Ù…Ø§Ù„ÙŠ {len(final_training_data)} ØµÙÙ‹Ø§ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨.")

        if final_training_data.empty:
            logger.critical("âŒ [Main] DataFrame Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙØ§Ø±Øº. Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.")
            training_status = "Failed: Empty training data"
            exit(1)

        # 6. ØªØ¯Ø±ÙŠØ¨ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        training_status = "In Progress: Training Model"
        trained_model, model_metrics = train_and_evaluate_model(final_training_data)

        if trained_model is None:
            logger.critical("âŒ [Main] ÙØ´Ù„ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø­ÙØ¸Ù‡.")
            training_status = "Failed: Model training failed"
            exit(1)

        # 7. Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        training_status = "In Progress: Saving Model"
        logger.info(f"â„¹ï¸ [Main] Ù…Ø­Ø§ÙˆÙ„Ø© Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ '{ML_MODEL_NAME}' ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...") # Ø±Ø³Ø§Ù„Ø© Ø³Ø¬Ù„ Ø¥Ø¶Ø§ÙÙŠØ© Ù‡Ù†Ø§
        if save_ml_model_to_db(trained_model, ML_MODEL_NAME, model_metrics):
            logger.info(f"âœ… [Main] ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ '{ML_MODEL_NAME}' Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
            training_status = "Completed Successfully"
            last_training_time = datetime.now()
            last_training_metrics = model_metrics
        else:
            logger.error(f"âŒ [Main] ÙØ´Ù„ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ '{ML_MODEL_NAME}' ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
            training_status = "Completed with Errors: Model save failed"
            training_error = "Model save failed"

        # Ø§Ù†ØªØ¸Ø± Ø®ÙŠØ· Flask Ù„Ø¥Ù†Ù‡Ø§Ø¡ (Ù…Ù…Ø§ ÙŠØ¨Ù‚ÙŠ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‚ÙŠØ¯ Ø§Ù„ØªØ´ØºÙŠÙ„)
        if flask_thread:
            flask_thread.join()

    except Exception as e:
        logger.critical(f"âŒ [Main] Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ ØªØ´ØºÙŠÙ„ Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {e}", exc_info=True)
        training_status = "Failed: Unhandled exception"
        training_error = str(e)
    finally:
        logger.info("ğŸ›‘ [Main] ÙŠØªÙ… Ø¥ÙŠÙ‚Ø§Ù ØªØ´ØºÙŠÙ„ Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
        cleanup_resources()
        logger.info("ğŸ‘‹ [Main] ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø³ÙƒØ±ÙŠØ¨Øª ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.")
        # os._exit(0) # Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… os._exit(0) Ù‡Ù†Ø§ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ Ø£Ù† ÙŠØ¨Ù‚Ù‰ Flask ÙŠØ¹Ù…Ù„

