import os
import json
import logging
import requests
import numpy as np
import pandas as pd
import psycopg2
import pickle
from psycopg2 import sql, OperationalError, InterfaceError
from psycopg2.extras import RealDictCursor
from binance.client import Client
from binance.exceptions import BinanceAPIException, BinanceRequestException
from datetime import datetime, timedelta
from decouple import config
from typing import List, Dict, Optional, Tuple, Any, Union

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ----------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crypto_bot_elliott_fib.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('CryptoBot')

# ---------------------- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ© ----------------------
try:
    API_KEY: str = config('BINANCE_API_KEY')
    API_SECRET: str = config('BINANCE_API_SECRET')
    TELEGRAM_TOKEN: str = config('TELEGRAM_BOT_TOKEN')
    CHAT_ID: str = config('TELEGRAM_CHAT_ID')
    DB_URL: str = config('DATABASE_URL')
    WEBHOOK_URL: Optional[str] = config('WEBHOOK_URL', default=None)
except Exception as e:
     logger.critical(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {e}")
     exit(1)

logger.info(f"Binance API Key: {'Available' if API_KEY else 'Not available'}")
logger.info(f"Telegram Token: {TELEGRAM_TOKEN[:10]}...{'*' * (len(TELEGRAM_TOKEN)-10)}")
logger.info(f"Telegram Chat ID: {CHAT_ID}")
logger.info(f"Database URL: {'Available' if DB_URL else 'Not available'}")
logger.info(f"Webhook URL: {WEBHOOK_URL if WEBHOOK_URL else 'Not specified'} (Flask will always run for Render)")

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø«ÙˆØ§Ø¨Øª ÙˆØ§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© ----------------------
RSI_PERIOD: int = 9
VOLUME_LOOKBACK_CANDLES: int = 3
RSI_MOMENTUM_LOOKBACK_CANDLES: int = 2
ENTRY_ATR_PERIOD: int = 10

BASE_ML_MODEL_NAME: str = 'DecisionTree_Scalping_V1'
ML_TARGET_LOOKAHEAD_CANDLES: int = 3 # Ø¹Ø¯Ø¯ Ø§Ù„Ø´Ù…Ø¹Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‡Ø¯Ù (3 Ø´Ù…Ø¹Ø§Øª * 5 Ø¯Ù‚Ø§Ø¦Ù‚ = 15 Ø¯Ù‚ÙŠÙ‚Ø©)

# Global variables for shared resources
conn: Optional[psycopg2.extensions.connection] = None
cur: Optional[psycopg2.extensions.cursor] = None
client: Optional[Client] = None
ml_models: Dict[str, Any] = {} # Global dictionary to hold loaded ML models, keyed by symbol

# ---------------------- Binance Client Setup ----------------------
def initialize_binance_client() -> None:
    """Initializes the global Binance client."""
    global client
    try:
        logger.info("â„¹ï¸ [Binance] ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ Binance...")
        client = Client(API_KEY, API_SECRET)
        client.ping()
        server_time = client.get_server_time()
        logger.info(f"âœ… [Binance] ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ Binance. ÙˆÙ‚Øª Ø§Ù„Ø®Ø§Ø¯Ù…: {datetime.fromtimestamp(server_time['serverTime']/1000)}")
    except BinanceRequestException as req_err:
         logger.critical(f"âŒ [Binance] Ø®Ø·Ø£ ÙÙŠ Ø·Ù„Ø¨ Binance (Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© Ø£Ùˆ Ø§Ù„Ø·Ù„Ø¨): {req_err}")
         raise
    except BinanceAPIException as api_err:
         logger.critical(f"âŒ [Binance] Ø®Ø·Ø£ ÙÙŠ ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Binance (Ù…ÙØ§ØªÙŠØ­ ØºÙŠØ± ØµØ§Ù„Ø­Ø© Ø£Ùˆ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„Ø®Ø§Ø¯Ù…): {api_err}")
         raise
    except Exception as e:
        logger.critical(f"âŒ [Binance] ÙØ´Ù„ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ Binance: {e}")
        raise

# ---------------------- Database Connection Setup ----------------------
def init_db(retries: int = 5, delay: int = 5) -> None:
    """Initializes database connection and creates tables if they don't exist."""
    global conn, cur
    logger.info("[DB] Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    for attempt in range(retries):
        try:
            logger.info(f"[DB] Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}/{retries})...")
            conn = psycopg2.connect(DB_URL, connect_timeout=10, cursor_factory=RealDictCursor)
            conn.autocommit = False
            cur = conn.cursor()
            logger.info("âœ… [DB] ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")

            # --- Create or update signals table ---
            logger.info("[DB] Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†/Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ 'signals'...")
            cur.execute("""
                CREATE TABLE IF NOT EXISTS signals (
                    id SERIAL PRIMARY KEY,
                    symbol TEXT NOT NULL,
                    entry_price DOUBLE PRECISION NOT NULL,
                    initial_target DOUBLE PRECISION NOT NULL,
                    current_target DOUBLE PRECISION NOT NULL,
                    r2_score DOUBLE PRECISION,
                    volume_15m DOUBLE PRECISION,
                    achieved_target BOOLEAN DEFAULT FALSE,
                    closing_price DOUBLE PRECISION,
                    closed_at TIMESTAMP,
                    sent_at TIMESTAMP DEFAULT NOW(),
                    entry_time TIMESTAMP DEFAULT NOW(),
                    time_to_target INTERVAL,
                    profit_percentage DOUBLE PRECISION,
                    strategy_name TEXT,
                    signal_details JSONB
                );""")
            conn.commit()
            logger.info("âœ… [DB] Ø¬Ø¯ÙˆÙ„ 'signals' Ù…ÙˆØ¬ÙˆØ¯ Ø£Ùˆ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡.")

            # --- Create ml_models table ---
            logger.info("[DB] Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†/Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ 'ml_models'...")
            cur.execute("""
                CREATE TABLE IF NOT EXISTS ml_models (
                    id SERIAL PRIMARY KEY,
                    model_name TEXT NOT NULL UNIQUE,
                    model_data BYTEA NOT NULL,
                    trained_at TIMESTAMP DEFAULT NOW(),
                    metrics JSONB
                );""")
            conn.commit()
            logger.info("âœ… [DB] Ø¬Ø¯ÙˆÙ„ 'ml_models' Ù…ÙˆØ¬ÙˆØ¯ Ø£Ùˆ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡.")

            # --- Create market_dominance table ---
            logger.info("[DB] Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†/Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ 'market_dominance'...")
            cur.execute("""
                CREATE TABLE IF NOT EXISTS market_dominance (
                    id SERIAL PRIMARY KEY,
                    recorded_at TIMESTAMP DEFAULT NOW(),
                    btc_dominance DOUBLE PRECISION,
                    eth_dominance DOUBLE PRECISION
                );
            """)
            conn.commit()
            logger.info("âœ… [DB] Ø¬Ø¯ÙˆÙ„ 'market_dominance' Ù…ÙˆØ¬ÙˆØ¯ Ø£Ùˆ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡.")

            logger.info("âœ… [DB] ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")
            return

        except OperationalError as op_err:
            logger.error(f"âŒ [DB] Ø®Ø·Ø£ ØªØ´ØºÙŠÙ„ÙŠ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ (Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}): {op_err}")
            if conn: conn.rollback()
            if attempt == retries - 1:
                 logger.critical("âŒ [DB] ÙØ´Ù„Øª Ø¬Ù…ÙŠØ¹ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
                 raise op_err
            time.sleep(delay)
        except Exception as e:
            logger.critical(f"âŒ [DB] ÙØ´Ù„ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}): {e}", exc_info=True)
            if conn: conn.rollback()
            if attempt == retries - 1:
                 logger.critical("âŒ [DB] ÙØ´Ù„Øª Ø¬Ù…ÙŠØ¹ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
                 raise e
            time.sleep(delay)

    logger.critical("âŒ [DB] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø¹Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø§Øª.")
    exit(1)


def check_db_connection() -> bool:
    """Checks database connection status and re-initializes if necessary."""
    global conn, cur
    try:
        if conn is None or conn.closed != 0:
            logger.warning("âš ï¸ [DB] Ø§Ù„Ø§ØªØµØ§Ù„ Ù…ØºÙ„Ù‚ Ø£Ùˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙ‡ÙŠØ¦Ø©...")
            init_db()
            return True
        else:
             with conn.cursor() as check_cur:
                  check_cur.execute("SELECT 1;")
                  check_cur.fetchone()
             return True
    except (OperationalError, InterfaceError) as e:
        logger.error(f"âŒ [DB] ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ({e}). Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙ‡ÙŠØ¦Ø©...")
        try:
             init_db()
             return True
        except Exception as recon_err:
            logger.error(f"âŒ [DB] ÙØ´Ù„Øª Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø¹Ø¯ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ø§ØªØµØ§Ù„: {recon_err}")
            return False
    except Exception as e:
        logger.error(f"âŒ [DB] Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„: {e}", exc_info=True)
        try:
            init_db()
            return True
        except Exception as recon_err:
             logger.error(f"âŒ [DB] ÙØ´Ù„Øª Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø¹Ø¯ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {recon_err}")
             return False

def load_ml_model_from_db(symbol: str) -> Optional[Any]:
    """Loads the latest trained ML model for a specific symbol from the database."""
    global ml_models
    model_name = f"{BASE_ML_MODEL_NAME}_{symbol}"

    if model_name in ml_models:
        logger.debug(f"â„¹ï¸ [ML Model] Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ '{model_name}' Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„ÙØ¹Ù„ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©.")
        return ml_models[model_name]

    if not check_db_connection() or not conn:
        logger.error(f"âŒ [ML Model] Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ML Ù„Ù€ {symbol} Ø¨Ø³Ø¨Ø¨ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        return None

    try:
        with conn.cursor() as db_cur:
            db_cur.execute("SELECT model_data FROM ml_models WHERE model_name = %s ORDER BY trained_at DESC LIMIT 1;", (model_name,))
            result = db_cur.fetchone()
            if result and result['model_data']:
                model = pickle.loads(result['model_data'])
                ml_models[model_name] = model # Store in global dictionary
                logger.info(f"âœ… [ML Model] ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ML '{model_name}' Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")
                return model
            else:
                logger.warning(f"âš ï¸ [ML Model] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ ML Ø¨Ø§Ø³Ù… '{model_name}' ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. ÙŠØ±Ø¬Ù‰ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£ÙˆÙ„Ø§Ù‹.")
                return None
    except psycopg2.Error as db_err:
        logger.error(f"âŒ [ML Model] Ø®Ø·Ø£ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ML Ù„Ù€ {symbol}: {db_err}", exc_info=True)
        return None
    except pickle.UnpicklingError as unpickle_err:
        logger.error(f"âŒ [ML Model] Ø®Ø·Ø£ ÙÙŠ ÙÙƒ ØªØ³Ù„Ø³Ù„ Ù†Ù…ÙˆØ°Ø¬ ML Ù„Ù€ {symbol}: {unpickle_err}. Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØªØ§Ù„ÙÙ‹Ø§ Ø£Ùˆ ØªÙ… Ø­ÙØ¸Ù‡ Ø¨Ø¥ØµØ¯Ø§Ø± Ù…Ø®ØªÙ„Ù.", exc_info=True)
        return None
    except Exception as e:
        logger.error(f"âŒ [ML Model] Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ML Ù„Ù€ {symbol}: {e}", exc_info=True)
        return None


def save_ml_model_to_db(symbol: str, model: Any, metrics: Dict[str, Any]) -> bool:
    """Saves the trained ML model and its metrics to the database."""
    if not check_db_connection() or not conn:
        logger.error(f"âŒ [ML Save] ÙØ´Ù„ Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ ML Ù„Ù€ {symbol} Ø¨Ø³Ø¨Ø¨ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        return False

    model_name = f"{BASE_ML_MODEL_NAME}_{symbol}"
    try:
        model_data = pickle.dumps(model)
        metrics_json = json.dumps(convert_np_values(metrics))

        with conn.cursor() as cur_save:
            insert_query = sql.SQL("""
                INSERT INTO ml_models (model_name, model_data, trained_at, metrics)
                VALUES (%s, %s, NOW(), %s)
                ON CONFLICT (model_name) DO UPDATE
                SET model_data = EXCLUDED.model_data,
                    trained_at = NOW(),
                    metrics = EXCLUDED.metrics;
            """)
            cur_save.execute(insert_query, (model_name, psycopg2.Binary(model_data), metrics_json))
        conn.commit()
        logger.info(f"âœ… [ML Save] ØªÙ… Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ ML '{model_name}' ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")
        return True
    except psycopg2.Error as db_err:
        logger.error(f"âŒ [ML Save] Ø®Ø·Ø£ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø«Ù†Ø§Ø¡ Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ ML Ù„Ù€ {symbol}: {db_err}", exc_info=True)
        if conn: conn.rollback()
        return False
    except pickle.PicklingError as pickle_err:
        logger.error(f"âŒ [ML Save] Ø®Ø·Ø£ ÙÙŠ ØªØ³Ù„Ø³Ù„ Ù†Ù…ÙˆØ°Ø¬ ML Ù„Ù€ {symbol}: {pickle_err}", exc_info=True)
        if conn: conn.rollback()
        return False
    except Exception as e:
        logger.error(f"âŒ [ML Save] Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ ML Ù„Ù€ {symbol}: {e}", exc_info=True)
        if conn: conn.rollback()
        return False

def convert_np_values(obj: Any) -> Any:
    """Converts NumPy data types to native Python types for JSON and DB compatibility."""
    if isinstance(obj, dict):
        return {k: convert_np_values(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_np_values(item) for item in obj]
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (np.integer, np.int_)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64)):
        return float(obj)
    elif isinstance(obj, (np.bool_)):
        return bool(obj)
    elif pd.isna(obj):
        return None
    else:
        return obj

def fetch_historical_data(symbol: str, interval: str, days: int) -> Optional[pd.DataFrame]:
    """Fetches historical candlestick data from Binance."""
    if not client:
        logger.error("âŒ [Data] Ø¹Ù…ÙŠÙ„ Binance ØºÙŠØ± Ù…Ù‡ÙŠØ£ Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        return None
    try:
        start_dt = datetime.utcnow() - timedelta(days=days + 1)
        start_str = start_dt.strftime("%Y-%m-%d %H:%M:%S")
        logger.debug(f"â„¹ï¸ [Data] Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª {interval} Ù„Ù€ {symbol} Ù…Ù†Ø° {start_str} (Ø­Ø¯ 1000 Ø´Ù…Ø¹Ø©)...")

        klines = client.get_historical_klines(symbol, interval, start_str, limit=1000)

        if not klines:
            logger.warning(f"âš ï¸ [Data] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ§Ø±ÙŠØ®ÙŠØ© ({interval}) Ù„Ù€ {symbol} Ù„Ù„ÙØªØ±Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©.")
            return None

        df = pd.DataFrame(klines, columns=[
            'timestamp', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_volume', 'trades', 'taker_buy_base', 'taker_buy_quote', 'ignore'
        ])

        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce')

        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        df = df[numeric_cols]
        initial_len = len(df)
        df.dropna(subset=numeric_cols, inplace=True)

        if len(df) < initial_len:
            logger.debug(f"â„¹ï¸ [Data] {symbol}: ØªÙ… Ø¥Ø³Ù‚Ø§Ø· {initial_len - len(df)} ØµÙÙ‹Ø§ Ø¨Ø³Ø¨Ø¨ Ù‚ÙŠÙ… NaN ÙÙŠ Ø¨ÙŠØ§Ù†Ø§Øª OHLCV.")

        if df.empty:
            logger.warning(f"âš ï¸ [Data] DataFrame Ù„Ù€ {symbol} ÙØ§Ø±Øº Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ù‚ÙŠÙ… NaN Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©.")
            return None

        logger.debug(f"âœ… [Data] ØªÙ… Ø¬Ù„Ø¨ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© {len(df)} Ø´Ù…Ø¹Ø© ØªØ§Ø±ÙŠØ®ÙŠØ© ({interval}) Ù„Ù€ {symbol}.")
        return df

    except BinanceAPIException as api_err:
         logger.error(f"âŒ [Data] Ø®Ø·Ø£ ÙÙŠ Binance API Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol}: {api_err}")
         return None
    except BinanceRequestException as req_err:
         logger.error(f"âŒ [Data] Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø·Ù„Ø¨ Ø£Ùˆ Ø§Ù„Ø´Ø¨ÙƒØ© Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol}: {req_err}")
         return None
    except Exception as e:
        logger.error(f"âŒ [Data] Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ù€ {symbol}: {e}", exc_info=True)
        return None

def calculate_ema(series: pd.Series, span: int) -> pd.Series:
    """Calculates Exponential Moving Average (EMA)."""
    if series is None or series.isnull().all() or len(series) < span:
        return pd.Series(index=series.index if series is not None else None, dtype=float)
    return series.ewm(span=span, adjust=False).mean()

def calculate_rsi_indicator(df: pd.DataFrame, period: int = RSI_PERIOD) -> pd.DataFrame:
    """Calculates Relative Strength Index (RSI)."""
    df = df.copy()
    if 'close' not in df.columns or df['close'].isnull().all():
        logger.warning("âš ï¸ [Indicator RSI] Ø¹Ù…ÙˆØ¯ 'close' Ù…ÙÙ‚ÙˆØ¯ Ø£Ùˆ ÙØ§Ø±Øº.")
        df['rsi'] = np.nan
        return df
    if len(df) < period:
        logger.warning(f"âš ï¸ [Indicator RSI] Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© ({len(df)} < {period}) Ù„Ø­Ø³Ø§Ø¨ RSI.")
        df['rsi'] = np.nan
        return df

    delta = df['close'].diff()
    gain = delta.clip(lower=0)
    loss = -delta.clip(upper=0)

    avg_gain = gain.ewm(com=period - 1, adjust=False).mean()
    avg_loss = loss.ewm(com=period - 1, adjust=False).mean()

    rs = avg_gain / avg_loss.replace(0, np.nan)

    rsi_series = 100 - (100 / (1 + rs))
    df['rsi'] = rsi_series.ffill().fillna(50)

    return df

def calculate_atr_indicator(df: pd.DataFrame, period: int = ENTRY_ATR_PERIOD) -> pd.DataFrame:
    """Calculates Average True Range (ATR)."""
    df = df.copy()
    required_cols = ['high', 'low', 'close']
    if not all(col in df.columns for col in required_cols) or df[required_cols].isnull().all().any():
        logger.warning("âš ï¸ [Indicator ATR] Ø£Ø¹Ù…Ø¯Ø© 'high', 'low', 'close' Ù…ÙÙ‚ÙˆØ¯Ø© Ø£Ùˆ ÙØ§Ø±ØºØ©.")
        df['atr'] = np.nan
        return df
    if len(df) < period + 1:
        logger.warning(f"âš ï¸ [Indicator ATR] Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© ({len(df)} < {period + 1}) Ù„Ø­Ø³Ø§Ø¨ ATR.")
        df['atr'] = np.nan
        return df

    high_low = df['high'] - df['low']
    high_close_prev = (df['high'] - df['close'].shift(1)).abs()
    low_close_prev = (df['low'] - df['close'].shift(1)).abs()

    tr = pd.concat([high_low, high_close_prev, low_close_prev], axis=1).max(axis=1, skipna=False)

    df['atr'] = tr.ewm(span=period, adjust=False).mean()
    return df

def get_btc_trend_4h() -> str:
    """Calculates Bitcoin trend on 4-hour timeframe using EMA20 and EMA50."""
    logger.debug("â„¹ï¸ [Indicators] Ø­Ø³Ø§Ø¨ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† Ø¹Ù„Ù‰ 4 Ø³Ø§Ø¹Ø§Øª...")
    try:
        df = fetch_historical_data("BTCUSDT", interval=Client.KLINE_INTERVAL_4HOUR, days=10)
        if df is None or df.empty or len(df) < 50 + 1:
            logger.warning("âš ï¸ [Indicators] Ø¨ÙŠØ§Ù†Ø§Øª BTC/USDT 4H ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§ØªØ¬Ø§Ù‡.")
            return "N/A (Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©)"

        df['close'] = pd.to_numeric(df['close'], errors='coerce')
        df.dropna(subset=['close'], inplace=True)
        if len(df) < 50:
             logger.warning("âš ï¸ [Indicators] Ø¨ÙŠØ§Ù†Ø§Øª BTC/USDT 4H ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ù‚ÙŠÙ… NaN.")
             return "N/A (Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©)"

        ema20 = calculate_ema(df['close'], 20).iloc[-1]
        ema50 = calculate_ema(df['close'], 50).iloc[-1]
        current_close = df['close'].iloc[-1]

        if pd.isna(ema20) or pd.isna(ema50) or pd.isna(current_close):
            logger.warning("âš ï¸ [Indicators] Ù‚ÙŠÙ… EMA Ø£Ùˆ Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† Ù‡ÙŠ NaN.")
            return "N/A (Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­Ø³Ø§Ø¨)"

        diff_ema20_pct = abs(current_close - ema20) / current_close if current_close > 0 else 0

        if current_close > ema20 > ema50:
            trend = "ØµØ¹ÙˆØ¯ ğŸ“ˆ"
        elif current_close < ema20 < ema50:
            trend = "Ù‡Ø¨ÙˆØ· ğŸ“‰"
        elif diff_ema20_pct < 0.005:
            trend = "Ø§Ø³ØªÙ‚Ø±Ø§Ø± ğŸ”„"
        else:
            trend = "ØªØ°Ø¨Ø°Ø¨ ğŸ”€"

        logger.debug(f"âœ… [Indicators] Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† 4H: {trend}")
        return trend
    except Exception as e:
        logger.error(f"âŒ [Indicators] Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† Ø¹Ù„Ù‰ 4 Ø³Ø§Ø¹Ø§Øª: {e}", exc_info=True)
        return "N/A (Ø®Ø·Ø£)"

def fetch_recent_volume(symbol: str, interval: str, num_candles: int) -> float:
    """Fetches the trading volume in USDT for the last `num_candles` of the specified `interval`."""
    if not client:
         logger.error(f"âŒ [Data Volume] Ø¹Ù…ÙŠÙ„ Binance ØºÙŠØ± Ù…Ù‡ÙŠØ£ Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø­Ø¬Ù… Ù„Ù€ {symbol}.")
         return 0.0
    try:
        logger.debug(f"â„¹ï¸ [Data Volume] Ø¬Ù„Ø¨ Ø­Ø¬Ù… Ø¢Ø®Ø± {num_candles} Ø´Ù…Ø¹Ø§Øª {interval} Ù„Ù€ {symbol}...")
        klines = client.get_klines(symbol=symbol, interval=interval, limit=num_candles)
        if not klines or len(klines) < num_candles:
             logger.warning(f"âš ï¸ [Data Volume] Ø¨ÙŠØ§Ù†Ø§Øª {interval} ØºÙŠØ± ÙƒØ§ÙÙŠØ© (Ø£Ù‚Ù„ Ù…Ù† {num_candles} Ø´Ù…Ø¹Ø©) Ù„Ù€ {symbol}.")
             return 0.0

        # k[7] is the quote asset volume (e.g., USDT volume)
        volume_usdt = sum(float(k[7]) for k in klines if len(k) > 7 and k[7])
        logger.debug(f"âœ… [Data Volume] Ø³ÙŠÙˆÙ„Ø© Ø¢Ø®Ø± {num_candles} Ø´Ù…Ø¹Ø§Øª {interval} Ù„Ù€ {symbol}: {volume_usdt:.2f} USDT")
        return volume_usdt
    except (BinanceAPIException, BinanceRequestException) as binance_err:
         logger.error(f"âŒ [Data Volume] Ø®Ø·Ø£ ÙÙŠ Binance API Ø£Ùˆ Ø§Ù„Ø´Ø¨ÙƒØ© Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø­Ø¬Ù… Ù„Ù€ {symbol}: {binance_err}")
         return 0.0
    except Exception as e:
        logger.error(f"âŒ [Data Volume] Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø­Ø¬Ù… Ù„Ù€ {symbol}: {e}", exc_info=True)
        return 0.0

def get_crypto_symbols(filename: str = 'crypto_list.txt') -> List[str]:
    """
    Reads the list of currency symbols from a text file, then validates them
    as valid USDT pairs available for Spot trading on Binance.
    """
    raw_symbols: List[str] = []
    logger.info(f"â„¹ï¸ [Data] Ù‚Ø±Ø§Ø¡Ø© Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ù…Ù† Ø§Ù„Ù…Ù„Ù '{filename}'...")
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, filename)

        if not os.path.exists(file_path):
            file_path = os.path.abspath(filename)
            if not os.path.exists(file_path):
                 logger.error(f"âŒ [Data] Ø§Ù„Ù…Ù„Ù '{filename}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ø£Ùˆ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠ.")
                 return []
            else:
                 logger.warning(f"âš ï¸ [Data] Ø§Ù„Ù…Ù„Ù '{filename}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ù„Ù ÙÙŠ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠ: '{file_path}'")

        with open(file_path, 'r', encoding='utf-8') as f:
            raw_symbols = [f"{line.strip().upper()}USDT"
                           for line in f if line.strip() and not line.startswith('#')]
        raw_symbols = sorted(list(set(raw_symbols)))
        logger.info(f"â„¹ï¸ [Data] ØªÙ… Ù‚Ø±Ø§Ø¡Ø© {len(raw_symbols)} Ø±Ù…Ø²Ù‹Ø§ Ù…Ø¨Ø¯Ø¦ÙŠÙ‹Ø§ Ù…Ù† '{file_path}'.")

    except FileNotFoundError:
         logger.error(f"âŒ [Data] Ø§Ù„Ù…Ù„Ù '{filename}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")
         return []
    except Exception as e:
        logger.error(f"âŒ [Data] Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù '{filename}': {e}", exc_info=True)
        return []

    if not raw_symbols:
         logger.warning("âš ï¸ [Data] Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø£ÙˆÙ„ÙŠØ© ÙØ§Ø±ØºØ©.")
         return []

    if not client:
        logger.error("âŒ [Data Validation] Ø¹Ù…ÙŠÙ„ Binance ØºÙŠØ± Ù…Ù‡ÙŠØ£. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ².")
        return raw_symbols

    try:
        logger.info("â„¹ï¸ [Data Validation] Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ­Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø§ÙˆÙ„ Ù…Ù† Binance API...")
        exchange_info = client.get_exchange_info()
        valid_trading_usdt_symbols = {
            s['symbol'] for s in exchange_info['symbols']
            if s.get('quoteAsset') == 'USDT' and
               s.get('status') == 'TRADING' and
               s.get('isSpotTradingAllowed') is True
        }
        logger.info(f"â„¹ï¸ [Data Validation] ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(valid_trading_usdt_symbols)} Ø²ÙˆØ¬ ØªØ¯Ø§ÙˆÙ„ USDT ØµØ§Ù„Ø­ ÙÙŠ Spot Ø¹Ù„Ù‰ Binance.")
        validated_symbols = [symbol for symbol in raw_symbols if symbol in valid_trading_usdt_symbols]

        removed_count = len(raw_symbols) - len(validated_symbols)
        if removed_count > 0:
            removed_symbols = set(raw_symbols) - set(validated_symbols)
            logger.warning(f"âš ï¸ [Data Validation] ØªÙ… Ø¥Ø²Ø§Ù„Ø© {removed_count} Ø±Ù…Ø² ØªØ¯Ø§ÙˆÙ„ USDT ØºÙŠØ± ØµØ§Ù„Ø­ Ø£Ùˆ ØºÙŠØ± Ù…ØªØ§Ø­ Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©: {', '.join(removed_symbols)}")

        logger.info(f"âœ… [Data Validation] ØªÙ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ². Ø§Ø³ØªØ®Ø¯Ø§Ù… {len(validated_symbols)} Ø±Ù…Ø²Ù‹Ø§ ØµØ§Ù„Ø­Ù‹Ø§.")
        return validated_symbols

    except (BinanceAPIException, BinanceRequestException) as binance_err:
         logger.error(f"âŒ [Data Validation] Ø®Ø·Ø£ ÙÙŠ Binance API Ø£Ùˆ Ø§Ù„Ø´Ø¨ÙƒØ© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ²: {binance_err}")
         logger.warning("âš ï¸ [Data Validation] Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø¨Ø¯ÙˆÙ† Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Binance.")
         return raw_symbols
    except Exception as api_err:
         logger.error(f"âŒ [Data Validation] Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø±Ù…ÙˆØ² Binance: {api_err}", exc_info=True)
         logger.warning("âš ï¸ [Data Validation] Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø¨Ø¯ÙˆÙ† Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Binance.")
         return raw_symbols

def get_interval_minutes(interval: str) -> int:
    """Helper function to convert Binance interval string to minutes."""
    if interval.endswith('m'):
        return int(interval[:-1])
    elif interval.endswith('h'):
        return int(interval[:-1]) * 60
    elif interval.endswith('d'):
        return int(interval[:-1]) * 60 * 24
    return 0

def get_fear_greed_index() -> str:
    """Fetches the Fear & Greed Index from alternative.me and translates classification to Arabic."""
    classification_translation_ar = {
        "Extreme Fear": "Ø®ÙˆÙ Ø´Ø¯ÙŠØ¯", "Fear": "Ø®ÙˆÙ", "Neutral": "Ù…Ø­Ø§ÙŠØ¯",
        "Greed": "Ø¬Ø´Ø¹", "Extreme Greed": "Ø¬Ø´Ø¹ Ø´Ø¯ÙŠØ¯",
    }
    url = "https://api.alternative.me/fng/"
    logger.debug(f"â„¹ï¸ [Indicators] Ø¬Ù„Ø¨ Ù…Ø¤Ø´Ø± Ø§Ù„Ø®ÙˆÙ ÙˆØ§Ù„Ø¬Ø´Ø¹ Ù…Ù† {url}...")
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        data = response.json()
        value = int(data["data"][0]["value"])
        classification_en = data["data"][0]["value_classification"]
        classification_ar = classification_translation_ar.get(classification_en, classification_en)
        logger.debug(f"âœ… [Indicators] Ù…Ø¤Ø´Ø± Ø§Ù„Ø®ÙˆÙ ÙˆØ§Ù„Ø¬Ø´Ø¹: {value} ({classification_ar})")
        return f"{value} ({classification_ar})"
    except requests.exceptions.RequestException as e:
         logger.error(f"âŒ [Indicators] Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ù…Ø¤Ø´Ø± Ø§Ù„Ø®ÙˆÙ ÙˆØ§Ù„Ø¬Ø´Ø¹: {e}")
         return "N/A (Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ©)"
    except (KeyError, IndexError, ValueError, json.JSONDecodeError) as e:
        logger.error(f"âŒ [Indicators] Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù…Ø¤Ø´Ø± Ø§Ù„Ø®ÙˆÙ ÙˆØ§Ù„Ø¬Ø´Ø¹: {e}")
        return "N/A (Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)"
    except Exception as e:
        logger.error(f"âŒ [Indicators] Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ù…Ø¤Ø´Ø± Ø§Ù„Ø®ÙˆÙ ÙˆØ§Ù„Ø¬Ø´Ø¹: {e}", exc_info=True)
        return "N/A (Ø®Ø·Ø£ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ)"

