import os
import gc
import pickle
import logging
import warnings
import pandas as pd
import numpy as np
import psycopg2
from decouple import config
from binance.client import Client
from psycopg2.extras import RealDictCursor
from datetime import datetime, timedelta, timezone
from backtesting import Backtest, Strategy
from backtesting.lib import crossover
from tqdm import tqdm

# --- ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª ØºÙŠØ± Ø§Ù„Ù‡Ø§Ù…Ø© ---
warnings.simplefilter(action='ignore', category=FutureWarning)
pd.options.mode.chained_assignment = None

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« (Logging) ----------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('backtester_v6_with_sr.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('Backtester_V6_With_SR')

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØ§Ù„Ø«ÙˆØ§Ø¨Øª ----------------------
try:
    API_KEY = config('BINANCE_API_KEY')
    API_SECRET = config('BINANCE_API_SECRET')
    DB_URL = config('DATABASE_URL')
except Exception as e:
    logger.critical(f"âŒ ÙØ´Ù„ Ø­Ø±Ø¬ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©: {e}")
    exit(1)

# --- Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ (Backtesting) ---
INITIAL_CASH = 100000.0  # Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ù„Ù„Ù…Ø­ÙØ¸Ø© Ù„Ù„Ù…Ø­Ø§ÙƒØ§Ø©
TRADE_AMOUNT_USDT = 10.0 # Ù…Ø¨Ù„Øº Ø«Ø§Ø¨Øª Ù„ÙƒÙ„ ØµÙÙ‚Ø©
FEE = 0.001  # 0.1% Ø±Ø³ÙˆÙ… Ø§Ù„ØªØ¯Ø§ÙˆÙ„ Ø§Ù„ÙÙˆØ±ÙŠ ÙÙŠ Ø¨ÙŠÙ†Ø§Ù†Ø³
SLIPPAGE = 0.0005 # 0.05% Ø§Ù†Ø²Ù„Ø§Ù‚ Ø³Ø¹Ø±ÙŠ Ù…Ø­Ø§ÙƒÙ‰ ÙÙŠ Ø§Ù„ØµÙÙ‚Ø§Øª
COMMISSION = FEE + SLIPPAGE # Ø§Ù„Ø¹Ù…ÙˆÙ„Ø© Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© Ù„Ù…ÙƒØªØ¨Ø© backtesting.py
BACKTEST_PERIOD_DAYS = 90 # ÙØªØ±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ

# --- Ø«ÙˆØ§Ø¨Øª Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙŠØ¬Ø¨ Ø£Ù† ØªØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ø§Ù„Ø¨ÙˆØª ÙˆØ§Ù„Ù…Ø¯Ø±Ø¨) ---
BASE_ML_MODEL_NAME = 'LightGBM_Scalping_V6_With_SR'
SIGNAL_GENERATION_TIMEFRAME = '15m'
HIGHER_TIMEFRAME = '4h'
BTC_SYMBOL = 'BTCUSDT'
MODEL_CONFIDENCE_THRESHOLD = 0.70 # Ø¹ØªØ¨Ø© Ø«Ù‚Ø© Ø§Ù„Ø¥Ø´Ø§Ø±Ø©
ATR_SL_MULTIPLIER = 1.5
ATR_TP_MULTIPLIER = 2.0

# --- Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ---
ADX_PERIOD, BBANDS_PERIOD, RSI_PERIOD = 14, 20, 14
MACD_FAST, MACD_SLOW, MACD_SIGNAL = 12, 26, 9
ATR_PERIOD, EMA_SLOW_PERIOD, EMA_FAST_PERIOD = 14, 200, 50
BTC_CORR_PERIOD, STOCH_RSI_PERIOD, STOCH_K, STOCH_D, REL_VOL_PERIOD = 30, 14, 3, 3, 30
RSI_OVERBOUGHT, RSI_OVERSOLD = 70, 30
STOCH_RSI_OVERBOUGHT, STOCH_RSI_OVERSOLD = 80, 20

# ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ø¹Ø§Ù…Ø©
conn = None
client = None

# ---------------------- Ø¯ÙˆØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ----------------------
def init_db():
    global conn
    try:
        conn = psycopg2.connect(DB_URL, cursor_factory=RealDictCursor)
        logger.info("âœ… [DB] ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        logger.critical(f"âŒ [DB] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        exit(1)

def load_ml_model_bundle_from_db(symbol: str) -> dict | None:
    model_name = f"{BASE_ML_MODEL_NAME}_{symbol}"
    if not conn:
        logger.error("[DB] Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªØ§Ø­.")
        return None
    try:
        with conn.cursor() as db_cur:
            db_cur.execute("SELECT model_data FROM ml_models WHERE model_name = %s ORDER BY trained_at DESC LIMIT 1;", (model_name,))
            result = db_cur.fetchone()
            if result and result['model_data']:
                model_bundle = pickle.loads(result['model_data'])
                logger.info(f"âœ… [ML Model] ØªÙ… ØªØ­Ù…ÙŠÙ„ '{model_name}' Ù„Ù„Ø¹Ù…Ù„Ø© {symbol} Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
                return model_bundle
        logger.warning(f"âš ï¸ [ML Model] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ '{model_name}' ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¹Ù…Ù„Ø© {symbol}.")
        return None
    except Exception as e:
        logger.error(f"âŒ [ML Model] Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø­Ø²Ù…Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¹Ù…Ù„Ø© {symbol}: {e}")
        return None

def fetch_sr_levels_from_db(symbol: str) -> pd.DataFrame:
    if not conn: return pd.DataFrame()
    query = "SELECT level_price, level_type, score FROM support_resistance_levels WHERE symbol = %s"
    try:
        df = pd.read_sql(query, conn, params=(symbol,))
        if not df.empty:
            logger.info(f"âœ… [S/R Levels] ØªÙ… Ø¬Ù„Ø¨ {len(df)} Ù…Ù† Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© Ù„Ù„Ø¹Ù…Ù„Ø© {symbol} Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        return df
    except Exception as e:
        logger.error(f"âŒ [S/R Levels] Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ù…Ù† Ø¬Ù„Ø¨ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© Ù„Ù„Ø¹Ù…Ù„Ø© {symbol}: {e}")
        return pd.DataFrame()

# ---------------------- Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¥Ø¹Ø¯Ø§Ø¯Ù‡Ø§ ----------------------
def fetch_historical_data(symbol: str, interval: str, days: int) -> pd.DataFrame | None:
    if not client: return None
    try:
        start_dt = datetime.now(timezone.utc) - timedelta(days=days)
        start_str = start_dt.strftime("%Y-%m-%d %H:%M:%S")
        klines = client.get_historical_klines(symbol, interval, start_str)
        if not klines: return None
        df = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_volume', 'trades', 'taker_buy_base', 'taker_buy_quote', 'ignore'])
        df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]
        numeric_cols = {'open': 'float32', 'high': 'float32', 'low': 'float32', 'close': 'float32', 'volume': 'float32'}
        df = df.astype(numeric_cols)
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        # --- Ø¥Ø¹Ø§Ø¯Ø© ØªØ³Ù…ÙŠØ© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù„ØªÙ†Ø§Ø³Ø¨ Ù…ÙƒØªØ¨Ø© backtesting.py ---
        df.rename(columns={'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'volume': 'Volume'}, inplace=True)
        return df.dropna()
    except Exception as e:
        logger.error(f"âŒ [Data] Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ù„Ø¹Ù…Ù„Ø© {symbol}: {e}")
        return None

# ---------------------- Ø¯ÙˆØ§Ù„ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª (Ù…Ù†Ø³ÙˆØ®Ø© Ù…Ù† Ø§Ù„Ø¨ÙˆØª/Ø§Ù„Ù…Ø¯Ø±Ø¨) ----------------------
def calculate_candlestick_patterns(df: pd.DataFrame) -> pd.DataFrame:
    op, hi, lo, cl = df['Open'], df['High'], df['Low'], df['Close']
    body = abs(cl - op); candle_range = hi - lo
    candle_range[candle_range == 0] = 1e-9
    upper_wick = hi - pd.concat([op, cl], axis=1).max(axis=1)
    lower_wick = pd.concat([op, cl], axis=1).min(axis=1) - lo
    df['candlestick_pattern'] = 0
    is_bullish_engulfing = (cl.shift(1) < op.shift(1)) & (cl > op) & (cl >= op.shift(1)) & (op <= cl.shift(1)) & (body > body.shift(1))
    is_bearish_engulfing = (cl.shift(1) > op.shift(1)) & (cl < op) & (op >= cl.shift(1)) & (cl <= op.shift(1)) & (body > body.shift(1))
    df.loc[is_bullish_engulfing, 'candlestick_pattern'] = 1
    df.loc[is_bearish_engulfing, 'candlestick_pattern'] = -1
    return df

def calculate_sr_features(df: pd.DataFrame, sr_levels_df: pd.DataFrame) -> pd.DataFrame:
    if sr_levels_df.empty:
        df['dist_to_support'] = 0.0; df['dist_to_resistance'] = 0.0
        df['score_of_support'] = 0.0; df['score_of_resistance'] = 0.0
        return df
    supports = sr_levels_df[sr_levels_df['level_type'].str.contains('support|poc|confluence', case=False)]['level_price'].sort_values().to_numpy()
    resistances = sr_levels_df[sr_levels_df['level_type'].str.contains('resistance|poc|confluence', case=False)]['level_price'].sort_values().to_numpy()
    support_scores = pd.Series(sr_levels_df['score'].values, index=sr_levels_df['level_price']).to_dict()
    def get_sr_info(price):
        dist_support, score_support, dist_resistance, score_resistance = 1.0, 0.0, 1.0, 0.0
        if supports.size > 0:
            idx = np.searchsorted(supports, price, side='right') - 1
            if idx >= 0:
                nearest_support = supports[idx]
                dist_support = (price - nearest_support) / price if price > 0 else 0
                score_support = support_scores.get(nearest_support, 0)
        if resistances.size > 0:
            idx = np.searchsorted(resistances, price, side='left')
            if idx < len(resistances):
                nearest_resistance = resistances[idx]
                dist_resistance = (nearest_resistance - price) / price if price > 0 else 0
                score_resistance = support_scores.get(nearest_resistance, 0)
        return dist_support, score_support, dist_resistance, score_resistance
    results = df['Close'].apply(get_sr_info)
    df[['dist_to_support', 'score_of_support', 'dist_to_resistance', 'score_of_resistance']] = pd.DataFrame(results.tolist(), index=df.index)
    return df

def create_all_features(df: pd.DataFrame, btc_df: pd.DataFrame) -> pd.DataFrame:
    df_calc = df.copy()
    high_low = df_calc['High'] - df_calc['Low']; high_close = (df_calc['High'] - df_calc['Close'].shift()).abs(); low_close = (df_calc['Low'] - df_calc['Close'].shift()).abs()
    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
    df_calc['atr'] = tr.ewm(span=ATR_PERIOD, adjust=False).mean()
    up_move = df_calc['High'].diff(); down_move = -df_calc['Low'].diff()
    plus_dm = pd.Series(np.where((up_move > down_move) & (up_move > 0), up_move, 0.0), index=df_calc.index)
    minus_dm = pd.Series(np.where((down_move > up_move) & (down_move > 0), down_move, 0.0), index=df_calc.index)
    plus_di = 100 * plus_dm.ewm(span=ADX_PERIOD, adjust=False).mean() / df_calc['atr']
    minus_di = 100 * minus_dm.ewm(span=ADX_PERIOD, adjust=False).mean() / df_calc['atr']
    dx = 100 * (abs(plus_di - minus_di) / (plus_di + minus_di).replace(0, 1e-9))
    df_calc['adx'] = dx.ewm(span=ADX_PERIOD, adjust=False).mean()
    delta = df_calc['Close'].diff()
    gain = delta.clip(lower=0).ewm(com=RSI_PERIOD - 1, adjust=False).mean()
    loss = -delta.clip(upper=0).ewm(com=RSI_PERIOD - 1, adjust=False).mean()
    df_calc['rsi'] = 100 - (100 / (1 + (gain / loss.replace(0, 1e-9))))
    ema_fast = df_calc['Close'].ewm(span=MACD_FAST, adjust=False).mean(); ema_slow = df_calc['Close'].ewm(span=MACD_SLOW, adjust=False).mean()
    macd_line = ema_fast - ema_slow; signal_line = macd_line.ewm(span=MACD_SIGNAL, adjust=False).mean()
    df_calc['macd_hist'] = macd_line - signal_line
    df_calc['macd_cross'] = 0
    df_calc.loc[(df_calc['macd_hist'].shift(1) < 0) & (df_calc['macd_hist'] >= 0), 'macd_cross'] = 1
    df_calc.loc[(df_calc['macd_hist'].shift(1) > 0) & (df_calc['macd_hist'] <= 0), 'macd_cross'] = -1
    sma = df_calc['Close'].rolling(window=BBANDS_PERIOD).mean(); std_dev = df_calc['Close'].rolling(window=BBANDS_PERIOD).std()
    upper_band = sma + (std_dev * 2); lower_band = sma - (std_dev * 2)
    df_calc['bb_width'] = (upper_band - lower_band) / (sma + 1e-9)
    rsi_val = df_calc['rsi']
    min_rsi = rsi_val.rolling(window=STOCH_RSI_PERIOD).min(); max_rsi = rsi_val.rolling(window=STOCH_RSI_PERIOD).max()
    stoch_rsi_val = (rsi_val - min_rsi) / (max_rsi - min_rsi).replace(0, 1e-9)
    df_calc['stoch_rsi_k'] = stoch_rsi_val.rolling(window=STOCH_K).mean() * 100
    df_calc['stoch_rsi_d'] = df_calc['stoch_rsi_k'].rolling(window=STOCH_D).mean()
    df_calc['relative_volume'] = df_calc['Volume'] / (df_calc['Volume'].rolling(window=REL_VOL_PERIOD, min_periods=1).mean() + 1e-9)
    df_calc['market_condition'] = 0
    df_calc.loc[(df_calc['rsi'] > RSI_OVERBOUGHT) | (df_calc['stoch_rsi_k'] > STOCH_RSI_OVERBOUGHT), 'market_condition'] = 1
    df_calc.loc[(df_calc['rsi'] < RSI_OVERSOLD) | (df_calc['stoch_rsi_k'] < STOCH_RSI_OVERSOLD), 'market_condition'] = -1
    ema_fast_trend = df_calc['Close'].ewm(span=EMA_FAST_PERIOD, adjust=False).mean()
    ema_slow_trend = df_calc['Close'].ewm(span=EMA_SLOW_PERIOD, adjust=False).mean()
    df_calc['price_vs_ema50'] = (df_calc['Close'] / ema_fast_trend) - 1
    df_calc['price_vs_ema200'] = (df_calc['Close'] / ema_slow_trend) - 1
    df_calc['returns'] = df_calc['Close'].pct_change()
    merged_df = pd.merge(df_calc, btc_df[['btc_returns']], left_index=True, right_index=True, how='left').fillna(0)
    df_calc['btc_correlation'] = merged_df['returns'].rolling(window=BTC_CORR_PERIOD).corr(merged_df['btc_returns'])
    df_calc['hour_of_day'] = df_calc.index.hour
    df_calc = calculate_candlestick_patterns(df_calc)
    return df_calc

# ---------------------- ÙØ¦Ø© Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Backtesting.py ----------------------
class MLStrategy(Strategy):
    # ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù‡Ù†Ø§
    ml_model = None
    scaler = None
    feature_names = None

    def init(self):
        # ÙŠØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ init Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ù‚Ø¨Ù„ Ø¨Ø¯Ø¡ Ø­Ù„Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ
        # Ù†Ù‚ÙˆÙ… Ø¨Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ù…Ø³Ø¨Ù‚Ù‹Ø§ØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ Ø­Ø§Ø¬Ø© Ù„Ù„ÙƒØ«ÙŠØ± Ù‡Ù†Ø§.
        # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ÙŠØªÙ… ØªÙ…Ø±ÙŠØ±Ù‡Ø§ Ø¥Ù„Ù‰ Backtest() Ø³ØªØ­ØªÙˆÙŠ Ø¨Ø§Ù„ÙØ¹Ù„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª.
        pass

    def next(self):
        # ÙŠØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ next Ù„ÙƒÙ„ Ø´Ù…Ø¹Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ù†Ø§Ùƒ ØµÙÙ‚Ø© Ù…ÙØªÙˆØ­Ø© Ø¨Ø§Ù„ÙØ¹Ù„ØŒ Ù„Ø§ ØªÙØ¹Ù„ Ø´ÙŠØ¦Ù‹Ø§. ÙŠØªÙ… Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ SL/TP Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙˆØ³ÙŠØ·.
        if self.position:
            return

        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚ÙŠÙ… Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ù„Ø´Ù…Ø¹Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
        # Ù…Ù„Ø§Ø­Ø¸Ø©: self.data ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø­Ø³ÙˆØ¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§
        try:
            features = self.data.df.loc[self.data.index[-1], self.feature_names]
            if features.isnull().any():
                return # ØªØ®Ø·ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙÙ‚ÙˆØ¯Ø©
        except (KeyError, IndexError):
            return # ØªØ®Ø·ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ØµÙÙˆÙ Ø£Ùˆ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù…ÙÙ‚ÙˆØ¯Ø©

        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ù„Ù„Ù€ scaler ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬
        features_df = pd.DataFrame([features])
        
        # --- Ø§Ù„Ø¥ØµÙ„Ø§Ø­ 1: ØªØ­ÙˆÙŠÙ„ Ù…ØµÙÙˆÙØ© numpy Ø§Ù„Ù…Ù‚Ø§Ø³Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¥Ù„Ù‰ DataFrame Ø¨Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª ---
        features_scaled_np = self.scaler.transform(features_df)
        features_scaled_df = pd.DataFrame(features_scaled_np, columns=self.feature_names)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙ†Ø¨Ø¤ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… DataFrame Ù…Ø¹ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        prediction = self.ml_model.predict(features_scaled_df)[0]
        prediction_proba = self.ml_model.predict_proba(features_scaled_df)[0]
        
        try:
            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© ÙØ¦Ø© "Ø§Ù„Ø´Ø±Ø§Ø¡" (1)
            class_1_index = list(self.ml_model.classes_).index(1)
            prob_for_class_1 = prediction_proba[class_1_index]
        except ValueError:
            return # Ø§Ù„ÙØ¦Ø© '1' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬

        # --- Ù…Ù†Ø·Ù‚ Ø§Ù„ØªØ¯Ø§ÙˆÙ„ ---
        if prediction == 1 and prob_for_class_1 >= MODEL_CONFIDENCE_THRESHOLD:
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ATR Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù€ SL/TP Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
            current_atr = self.data.atr[-1]
            if pd.isna(current_atr) or current_atr == 0:
                return # Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ¹ÙŠÙŠÙ† SL/TP Ø¨Ø¯ÙˆÙ† ATR

            current_price = self.data.Close[-1]
            
            # --- Ø§Ù„Ø¥ØµÙ„Ø§Ø­ 2: Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Ø§Ù„ØµÙÙ‚Ø© ÙƒØ¬Ø²Ø¡ Ù…Ù† Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ ---
            # ØªØªØ·Ù„Ø¨ Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø­Ø¬Ù… Ø¥Ù…Ø§ Ø¬Ø²Ø¡Ù‹Ø§ Ù…Ù† Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ (0 < size < 1)
            # Ø£Ùˆ Ø¹Ø¯Ø¯Ù‹Ø§ ØµØ­ÙŠØ­Ù‹Ø§ Ù…Ù† Ø§Ù„ÙˆØ­Ø¯Ø§Øª (Ù…Ø«Ù„ 1ØŒ 2ØŒ 3).
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø© (Ù…Ø«Ù„ 10 / Ø§Ù„Ø³Ø¹Ø±) ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙ†ØªØ¬ Ø¹Ù†Ù‡ Ø±Ù‚Ù… ÙƒØ³Ø±ÙŠ
            # Ø£ÙƒØ¨Ø± Ù…Ù† 1 (Ù…Ø«Ù„ 1.25)ØŒ Ù…Ù…Ø§ ÙŠØ³Ø¨Ø¨ Ø®Ø·Ø£ ØªØ£ÙƒÙŠØ¯.
            # Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„ØµØ­ÙŠØ­ Ù‡Ùˆ Ø­Ø³Ø§Ø¨ Ø§Ù„ÙƒØ³Ø± Ø§Ù„Ø°ÙŠ ÙŠÙ…Ø«Ù„Ù‡ Ù…Ø¨Ù„Øº Ø§Ù„ØµÙÙ‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
            # Ù…Ù† Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ Ù„Ø¯ÙŠÙ†Ø§.
            size_as_fraction = TRADE_AMOUNT_USDT / self.equity

            # ÙŠØ¬Ø¨ Ø£Ù† Ù†ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ø­Ø³ÙˆØ¨ Ù‡Ùˆ ÙƒØ³Ø± ØµØ§Ù„Ø­ Ù„Ø·Ø±ÙŠÙ‚Ø© `buy`.
            # ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† > 0 Ùˆ < 1. Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù„Ø¯ÙŠÙ†Ø§ Ø±Ø£Ø³ Ù…Ø§Ù„ ÙƒØ§ÙÙØŒ ÙÙ‚Ø¯ ÙŠÙƒÙˆÙ† >= 1.
            if size_as_fraction > 0 and size_as_fraction < 1:
                # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙŠØ§Øª ÙˆÙ‚Ù Ø§Ù„Ø®Ø³Ø§Ø±Ø© ÙˆØ¬Ù†ÙŠ Ø§Ù„Ø£Ø±Ø¨Ø§Ø­
                stop_loss_price = current_price - (current_atr * ATR_SL_MULTIPLIER)
                take_profit_price = current_price + (current_atr * ATR_TP_MULTIPLIER)

                # ÙˆØ¶Ø¹ Ø£Ù…Ø± Ø§Ù„Ø´Ø±Ø§Ø¡ Ù…Ø¹ SL Ùˆ TP
                self.buy(size=size_as_fraction, sl=stop_loss_price, tp=take_profit_price)

# ---------------------- ÙƒØªÙ„Ø© Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ----------------------
def run_backtest():
    global client, conn
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© V6...")
    
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª
    init_db()
    client = Client(API_KEY, API_SECRET)
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ø±Ø§Ø¯ Ø§Ø®ØªØ¨Ø§Ø±Ù‡Ø§
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, 'crypto_list.txt')
        with open(file_path, 'r', encoding='utf-8') as f:
            symbols_to_test = [line.strip().upper() + "USDT" for line in f if line.strip() and not line.startswith('#')]
    except FileNotFoundError:
        logger.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ 'crypto_list.txt'. Ø³ÙŠØªÙ… Ø§Ù„Ø®Ø±ÙˆØ¬.")
        return

    logger.info("â„¹ï¸ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª BTC Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ù„ÙØªØ±Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ...")
    btc_df_full = fetch_historical_data(BTC_SYMBOL, SIGNAL_GENERATION_TIMEFRAME, BACKTEST_PERIOD_DAYS + 10)
    if btc_df_full is None:
        logger.critical("âŒ ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª BTC. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©."); return
    btc_df_full['btc_returns'] = btc_df_full['Close'].pct_change()

    all_stats = []
    
    for symbol in tqdm(symbols_to_test, desc="Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¹Ù…Ù„Ø§Øª"):
        logger.info(f"\n--- â³ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¹Ù…Ù„Ø©: {symbol} ---")
        
        # 1. ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ù„Ø¹Ù…Ù„Ø©
        model_bundle = load_ml_model_bundle_from_db(symbol)
        if not model_bundle:
            logger.warning(f"âš ï¸ ØªØ®Ø·ÙŠ {symbol}: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬.")
            continue
        
        # 2. Ø¬Ù„Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        df_15m = fetch_historical_data(symbol, SIGNAL_GENERATION_TIMEFRAME, BACKTEST_PERIOD_DAYS)
        df_4h = fetch_historical_data(symbol, HIGHER_TIMEFRAME, BACKTEST_PERIOD_DAYS * 5) # Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª 4h Ù„Ù€ EMA
        
        if df_15m is None or df_15m.empty or df_4h is None or df_4h.empty:
            logger.warning(f"âš ï¸ ØªØ®Ø·ÙŠ {symbol}: Ø¨ÙŠØ§Ù†Ø§Øª ØªØ§Ø±ÙŠØ®ÙŠØ© ØºÙŠØ± ÙƒØ§ÙÙŠØ©.")
            continue
            
        sr_levels = fetch_sr_levels_from_db(symbol)

        # 3. Ø¥Ø¹Ø¯Ø§Ø¯ DataFrame Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        logger.info(f"Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ù„Ø¹Ù…Ù„Ø© {symbol}...")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        data = create_all_features(df_15m, btc_df_full)
        
        # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ¯Ù…Ø¬ Ù…ÙŠØ²Ø§Øª MTF
        delta_4h = df_4h['Close'].diff()
        gain_4h = delta_4h.clip(lower=0).ewm(com=RSI_PERIOD - 1, adjust=False).mean()
        loss_4h = -delta_4h.clip(upper=0).ewm(com=RSI_PERIOD - 1, adjust=False).mean()
        df_4h['rsi_4h'] = 100 - (100 / (1 + (gain_4h / loss_4h.replace(0, 1e-9))))
        ema_fast_4h = df_4h['Close'].ewm(span=EMA_FAST_PERIOD, adjust=False).mean()
        df_4h['price_vs_ema50_4h'] = (df_4h['Close'] / ema_fast_4h) - 1
        mtf_features = df_4h[['rsi_4h', 'price_vs_ema50_4h']]
        data = data.join(mtf_features, how='left').fillna(method='ffill')

        # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ¯Ù…Ø¬ Ù…ÙŠØ²Ø§Øª S/R
        data = calculate_sr_features(data, sr_levels)
        
        data.dropna(inplace=True)
        
        if data.empty:
            logger.warning(f"âš ï¸ ØªØ®Ø·ÙŠ {symbol}: DataFrame ÙØ§Ø±Øº Ø¨Ø¹Ø¯ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª.")
            continue

        # 4. ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ Ù„Ù„Ø¹Ù…Ù„Ø©
        logger.info(f"ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ Ù„Ù„Ø¹Ù…Ù„Ø© {symbol}...")
        bt = Backtest(
            data,
            MLStrategy,
            cash=INITIAL_CASH,
            commission=COMMISSION, # ÙŠØ´Ù…Ù„ Ø§Ù„Ø±Ø³ÙˆÙ… + Ø§Ù„Ø§Ù†Ø²Ù„Ø§Ù‚
            exclusive_orders=True # Ù…Ù†Ø¹ Ø£ÙˆØ§Ù…Ø± Ù…ØªØ¹Ø¯Ø¯Ø© ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª
        )
        
        # ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ù…Ù„ Ùˆ scaler Ø¥Ù„Ù‰ ÙØ¦Ø© Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©
        stats = bt.run(
            ml_model=model_bundle['model'],
            scaler=model_bundle['scaler'],
            feature_names=model_bundle['feature_names']
        )
        
        # *** ØªØ¹Ø¯ÙŠÙ„: Ø¥Ø¶Ø§ÙØ© Ø§Ø³Ù… Ø§Ù„Ø¹Ù…Ù„Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù„ØªØ³Ù‡ÙŠÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ***
        stats['Symbol'] = symbol
        
        logger.info(f"\n--- Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ Ù„Ù„Ø¹Ù…Ù„Ø© {symbol} ---")
        print(stats)
        all_stats.append(stats)
        
        # Ø§Ø®ØªÙŠØ§Ø±ÙŠ: Ø±Ø³Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        # Ù‚Ù… Ø¨Ø¥Ù„ØºØ§Ø¡ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø·Ø± Ø£Ø¯Ù†Ø§Ù‡ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø·Ø· HTML Ù„ÙƒÙ„ Ø¹Ù…Ù„Ø©
        # bt.plot(filename=f"backtest_plot_{symbol}.html", open_browser=False)

        del data, df_15m, df_4h, sr_levels, model_bundle
        gc.collect()

    # 5. *** Ø¬Ø¯ÙŠØ¯: Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© ÙˆØ¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„ ***
    logger.info("\n\n--- ğŸ Ù…Ù„Ø®Øµ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ğŸ ---")
    if all_stats:
        summary_df = pd.DataFrame(all_stats)
        if 'Symbol' in summary_df.columns:
            summary_df.set_index('Symbol', inplace=True)

        # --- Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© ---
        total_symbols_tested = len(summary_df)
        symbols_with_trades = summary_df[summary_df['# Trades'] > 0]
        total_symbols_with_trades = len(symbols_with_trades)

        total_trades = summary_df['# Trades'].sum()
        total_duration_days = BACKTEST_PERIOD_DAYS
        
        # Ø­Ø³Ø§Ø¨ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø±Ø¨Ø­/Ø§Ù„Ø®Ø³Ø§Ø±Ø© ÙˆØ§Ù„Ø¹Ø§Ø¦Ø¯
        initial_portfolio_value = INITIAL_CASH * total_symbols_tested
        final_portfolio_value = summary_df['Equity Final [$]'].sum()
        total_net_profit_loss = final_portfolio_value - initial_portfolio_value
        total_return_pct = (total_net_profit_loss / initial_portfolio_value) * 100 if initial_portfolio_value > 0 else 0

        # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ù„Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„ØªÙŠ ÙƒØ§Ù† Ø¨Ù‡Ø§ ØªØ¯Ø§ÙˆÙ„Ø§Øª ÙÙ‚Ø·
        if total_symbols_with_trades > 0:
            avg_win_rate = symbols_with_trades['Win Rate [%]'].mean()
            avg_profit_factor = symbols_with_trades['Profit Factor'].replace([np.inf, -np.inf], np.nan).mean()
            avg_sharpe = symbols_with_trades['Sharpe Ratio'].mean()
            avg_sortino = symbols_with_trades['Sortino Ratio'].mean()
            avg_max_drawdown = symbols_with_trades['Max. Drawdown [%]'].mean()
            best_performer = symbols_with_trades.sort_values(by='Return [%]', ascending=False).iloc[0]
            worst_performer = symbols_with_trades.sort_values(by='Return [%]', ascending=True).iloc[0]
        else:
            avg_win_rate, avg_profit_factor, avg_sharpe, avg_sortino, avg_max_drawdown = 0, 0, 0, 0, 0
            best_performer, worst_performer = None, None

        # --- Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ---
        report_lines = [
            "======================================================",
            "=          ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ Ø§Ù„Ø´Ø§Ù…Ù„              =",
            "======================================================",
            f"ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"ÙØªØ±Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {total_duration_days} ÙŠÙˆÙ…",
            f"Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©: {BASE_ML_MODEL_NAME}",
            "------------------------------------------------------",
            "                     Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù…                      ",
            "------------------------------------------------------",
            f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø®ØªØ¨Ø±Ø©: {total_symbols_tested}",
            f"Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… Ø§Ù„ØªØ¯Ø§ÙˆÙ„ Ø¹Ù„ÙŠÙ‡Ø§: {total_symbols_with_trades}",
            f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ Ø§Ù„Ø£ÙˆÙ„ÙŠ: ${initial_portfolio_value:,.2f}",
            f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: ${final_portfolio_value:,.2f}",
            f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ ØµØ§ÙÙŠ Ø§Ù„Ø±Ø¨Ø­/Ø§Ù„Ø®Ø³Ø§Ø±Ø©: ${total_net_profit_loss:,.2f}",
            f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹Ø§Ø¦Ø¯: {total_return_pct:.2f}%",
            f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙ‚Ø§Øª: {int(total_trades)}",
            "------------------------------------------------------",
            "               Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ (Ù„ÙƒÙ„ Ø¹Ù…Ù„Ø©)             ",
            "------------------------------------------------------",
            f"Ù…ØªÙˆØ³Ø· Ù†Ø³Ø¨Ø© Ø§Ù„Ø±Ø¨Ø­: {avg_win_rate:.2f}%",
            f"Ù…ØªÙˆØ³Ø· Ø¹Ø§Ù…Ù„ Ø§Ù„Ø±Ø¨Ø­: {avg_profit_factor:.2f}",
            f"Ù…ØªÙˆØ³Ø· Ø£Ù‚ØµÙ‰ ØªØ±Ø§Ø¬Ø¹: {avg_max_drawdown:.2f}%",
            f"Ù…ØªÙˆØ³Ø· Ù†Ø³Ø¨Ø© Ø´Ø§Ø±Ø¨: {avg_sharpe:.2f}",
            f"Ù…ØªÙˆØ³Ø· Ù†Ø³Ø¨Ø© Ø³ÙˆØ±ØªÙŠÙ†Ùˆ: {avg_sortino:.2f}",
            "------------------------------------------------------"
        ]

        if best_performer is not None:
            report_lines.append("              Ø£ÙØ¶Ù„ ÙˆØ£Ø³ÙˆØ£ Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø£Ø¯Ø§Ø¡Ù‹             ")
            report_lines.append("------------------------------------------------------")
            profit_best = best_performer['Equity Final [$]'] - best_performer['Start Equity [$]']
            report_lines.append(f"Ø§Ù„Ø£ÙØ¶Ù„ Ø£Ø¯Ø§Ø¡Ù‹: {best_performer.name}")
            report_lines.append(f"  - ØµØ§ÙÙŠ Ø§Ù„Ø±Ø¨Ø­: ${profit_best:,.2f}")
            report_lines.append(f"  - Ø§Ù„Ø¹Ø§Ø¦Ø¯: {best_performer['Return [%]']:.2f}%")
            report_lines.append(f"  - Ù†Ø³Ø¨Ø© Ø§Ù„Ø±Ø¨Ø­: {best_performer['Win Rate [%]']:.2f}%")
            report_lines.append(f"  - Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙ‚Ø§Øª: {int(best_performer['# Trades'])}")
        
        if worst_performer is not None:
            profit_worst = worst_performer['Equity Final [$]'] - worst_performer['Start Equity [$]']
            report_lines.append(f"\nØ§Ù„Ø£Ø³ÙˆØ£ Ø£Ø¯Ø§Ø¡Ù‹: {worst_performer.name}")
            report_lines.append(f"  - ØµØ§ÙÙŠ Ø§Ù„Ø±Ø¨Ø­: ${profit_worst:,.2f}")
            report_lines.append(f"  - Ø§Ù„Ø¹Ø§Ø¦Ø¯: {worst_performer['Return [%]']:.2f}%")
            report_lines.append(f"  - Ù†Ø³Ø¨Ø© Ø§Ù„Ø±Ø¨Ø­: {worst_performer['Win Rate [%]']:.2f}%")
            report_lines.append(f"  - Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙ‚Ø§Øª: {int(worst_performer['# Trades'])}")
        
        report_lines.append("======================================================")

        final_report_str = "\n".join(report_lines)

        # --- Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ Ø§Ù„Ø·Ø±ÙÙŠØ© ---
        print("\n\n" + final_report_str)

        # --- Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ Ù…Ù„ÙØ§Øª ---
        report_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_dir = 'backtest_reports'
        os.makedirs(report_dir, exist_ok=True)

        # Ø­ÙØ¸ Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ Ù…Ù„Ù Ù†ØµÙŠ
        summary_filename = os.path.join(report_dir, f'report_summary_{report_timestamp}.txt')
        with open(summary_filename, 'w', encoding='utf-8') as f:
            f.write(final_report_str)
        logger.info(f"âœ… ØªÙ… Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø´Ø§Ù…Ù„ ÙÙŠ: {summary_filename}")

        # Ø­ÙØ¸ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ© ÙÙŠ Ù…Ù„Ù CSV
        details_filename = os.path.join(report_dir, f'report_details_{report_timestamp}.csv')
        summary_df.to_csv(details_filename)
        logger.info(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ© Ù„ÙƒÙ„ Ø¹Ù…Ù„Ø© ÙÙŠ: {details_filename}")

    else:
        logger.warning("Ù„Ù… ÙŠØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø£ÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø®Ù„ÙÙŠ Ø¨Ù†Ø¬Ø§Ø­.")
        
    if conn:
        conn.close()

if __name__ == "__main__":
    run_backtest()
