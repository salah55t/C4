import os
import gc
import pickle
import logging
import warnings
import pandas as pd
import numpy as np
import psycopg2
from decouple import config
from binance.client import Client
from psycopg2.extras import RealDictCursor
from datetime import datetime, timedelta, timezone
from backtesting import Backtest, Strategy
from backtesting.lib import crossover
from tqdm import tqdm
import threading
from flask import Flask

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨ ---
# Ø³ÙŠÙ‚ÙˆÙ… Ù‡Ø°Ø§ Ø§Ù„Ø®Ø§Ø¯Ù… Ø¨Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ù„Ù„Ø·Ù„Ø¨Ø§Øª Ù„Ø¥Ø¨Ù‚Ø§Ø¡ Ø§Ù„Ø®Ø¯Ù…Ø© Ù†Ø´Ø·Ø© Ø¹Ù„Ù‰ Render
app = Flask(__name__)

@app.route('/')
def health_check():
    """Ù‡Ø°Ù‡ Ù‡ÙŠ Ù†Ù‚Ø·Ø© Ø§Ù„ÙˆØµÙˆÙ„ Ø§Ù„ØªÙŠ Ø³ØªØ³ØªØ¯Ø¹ÙŠÙ‡Ø§ Ø®Ø¯Ù…Ø© cron-job."""
    # ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù‡Ù†Ø§ Ø¥Ø°Ø§ Ø£Ø±Ø¯ØªØŒ Ù…Ø«Ù„ Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ
    return "Backtester service is running.", 200

# --- ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª ØºÙŠØ± Ø§Ù„Ù‡Ø§Ù…Ø© ---
warnings.simplefilter(action='ignore', category=FutureWarning)
pd.options.mode.chained_assignment = None

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (Logging) ----------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('backtester_v7_with_ichimoku.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('Backtester_V7_With_Ichimoku')

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØ§Ù„Ø«ÙˆØ§Ø¨Øª ----------------------
try:
    API_KEY = config('BINANCE_API_KEY')
    API_SECRET = config('BINANCE_API_SECRET')
    DB_URL = config('DATABASE_URL')
except Exception as e:
    logger.critical(f"âŒ ÙØ´Ù„ Ø­Ø±Ø¬ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©: {e}")
    exit(1)

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ ---
INITIAL_CASH = 100.0
TRADE_AMOUNT_USDT = 10.0
FEE = 0.001
SLIPPAGE = 0.0005
COMMISSION = FEE + SLIPPAGE
BACKTEST_PERIOD_DAYS = 30
OUT_OF_SAMPLE_OFFSET_DAYS = 90

# --- Ø«ÙˆØ§Ø¨Øª Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙŠØ¬Ø¨ Ø£Ù† ØªØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ø§Ù„Ø¨ÙˆØª ÙˆØ§Ù„Ù…Ø¯Ø±Ø¨) ---
BASE_ML_MODEL_NAME = 'LightGBM_Scalping_V7_With_Ichimoku'
SIGNAL_GENERATION_TIMEFRAME = '15m'
HIGHER_TIMEFRAME = '4h'
BTC_SYMBOL = 'BTCUSDT'
MODEL_CONFIDENCE_THRESHOLD = 0.70
ATR_SL_MULTIPLIER = 1.5
ATR_TP_MULTIPLIER = 2.0

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ---
ADX_PERIOD, BBANDS_PERIOD, RSI_PERIOD = 14, 20, 14
MACD_FAST, MACD_SLOW, MACD_SIGNAL = 12, 26, 9
ATR_PERIOD, EMA_SLOW_PERIOD, EMA_FAST_PERIOD = 14, 200, 50
BTC_CORR_PERIOD, STOCH_RSI_PERIOD, STOCH_K, STOCH_D, REL_VOL_PERIOD = 30, 14, 3, 3, 30
RSI_OVERBOUGHT, RSI_OVERSOLD = 70, 30
STOCH_RSI_OVERBOUGHT, STOCH_RSI_OVERSOLD = 80, 20

# Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ø¹Ø§Ù…Ø©
conn = None
client = None

# ---------------------- Ø¯ÙˆØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ----------------------
def init_db():
    global conn
    try:
        conn = psycopg2.connect(DB_URL, cursor_factory=RealDictCursor)
        logger.info("âœ… [DB] ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        logger.critical(f"âŒ [DB] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        # Ù„Ø§ Ù†Ø³ØªØ®Ø¯Ù… exit(1) Ù‡Ù†Ø§ Ù„Ù„Ø³Ù…Ø§Ø­ Ù„Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨ Ø¨Ø§Ù„Ø¹Ù…Ù„ Ø­ØªÙ‰ Ù„Ùˆ ÙØ´Ù„Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        conn = None # ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø§ØªØµØ§Ù„ ÙØ§Ø±Øº

def load_ml_model_bundle_from_db(symbol: str) -> dict | None:
    model_name = f"{BASE_ML_MODEL_NAME}_{symbol}"
    if not conn:
        logger.error("[DB] Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªØ§Ø­.")
        return None
    try:
        with conn.cursor() as db_cur:
            db_cur.execute("SELECT model_data FROM ml_models WHERE model_name = %s ORDER BY trained_at DESC LIMIT 1;", (model_name,))
            result = db_cur.fetchone()
            if result and result['model_data']:
                model_bundle = pickle.loads(result['model_data'])
                logger.info(f"âœ… [ML Model] ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ '{model_name}' Ù„Ù„Ø¹Ù…Ù„Ø© {symbol} Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
                return model_bundle
        logger.warning(f"âš ï¸ [ML Model] Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ '{model_name}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¹Ù…Ù„Ø© {symbol}.")
        return None
    except Exception as e:
        logger.error(f"âŒ [ML Model] Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø­Ø²Ù…Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¹Ù…Ù„Ø© {symbol}: {e}")
        return None

def fetch_sr_levels_from_db(symbol: str) -> pd.DataFrame:
    if not conn: return pd.DataFrame()
    query = "SELECT level_price, level_type, score FROM support_resistance_levels WHERE symbol = %s"
    try:
        df = pd.read_sql(query, conn, params=(symbol,))
        if not df.empty:
            logger.info(f"âœ… [S/R Levels] ØªÙ… Ø¬Ù„Ø¨ {len(df)} Ù…Ù† Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© Ù„Ù„Ø¹Ù…Ù„Ø© {symbol} Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        return df
    except Exception as e:
        logger.error(f"âŒ [S/R Levels] Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¬Ù„Ø¨ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© Ù„Ù„Ø¹Ù…Ù„Ø© {symbol}: {e}")
        return pd.DataFrame()

def fetch_ichimoku_features_from_db(symbol: str, timeframe: str) -> pd.DataFrame:
    if not conn: return pd.DataFrame()
    logger.info(f"ğŸ” [Ichimoku Fetch] Fetching Ichimoku features for {symbol} on {timeframe}...")
    query = """
        SELECT timestamp, tenkan_sen, kijun_sen, senkou_span_a, senkou_span_b, chikou_span
        FROM ichimoku_features
        WHERE symbol = %s AND timeframe = %s
        ORDER BY timestamp;
    """
    try:
        with conn.cursor() as cur:
            cur.execute(query, (symbol, timeframe))
            features = cur.fetchall()
            if not features:
                logger.warning(f"âš ï¸ [Ichimoku Fetch] No Ichimoku features found for {symbol}.")
                return pd.DataFrame()

            colnames = [desc[0] for desc in cur.description]
            df_ichimoku = pd.DataFrame(features, columns=colnames)

        df_ichimoku['timestamp'] = pd.to_datetime(df_ichimoku['timestamp'], utc=True)
        df_ichimoku.set_index('timestamp', inplace=True)

        logger.info(f"âœ… [Ichimoku Fetch] Found {len(df_ichimoku)} Ichimoku records for {symbol}.")
        return df_ichimoku
    except Exception as e:
        logger.error(f"âŒ [Ichimoku Fetch] Could not fetch Ichimoku features for {symbol}: {e}")
        if conn and not getattr(conn, 'autocommit', True):
             conn.rollback()
        return pd.DataFrame()

# ---------------------- Ø¬Ù„Ø¨ ÙˆØ¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ----------------------
def fetch_historical_data(symbol: str, interval: str, days: int, out_of_sample_period_days: int = 0) -> pd.DataFrame | None:
    global client
    if not client:
        try:
            client = Client(API_KEY, API_SECRET)
            logger.info("âœ… [Binance] ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§ØªØµØ§Ù„ Binance.")
        except Exception as e:
            logger.error(f"âŒ [Binance] ÙØ´Ù„ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§ØªØµØ§Ù„ Binance: {e}")
            return None
    try:
        now = datetime.now(timezone.utc)
        end_dt = now - timedelta(days=out_of_sample_period_days)
        start_dt = end_dt - timedelta(days=days)
        start_str = start_dt.strftime("%Y-%m-%d %H:%M:%S")
        end_str = end_dt.strftime("%Y-%m-%d %H:%M:%S")
        klines = client.get_historical_klines(symbol, interval, start_str, end_str)
        
        if not klines: return None
        
        df = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_volume', 'trades', 'taker_buy_base', 'taker_buy_quote', 'ignore'])
        df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]
        numeric_cols = {'open': 'float32', 'high': 'float32', 'low': 'float32', 'close': 'float32', 'volume': 'float32'}
        df = df.astype(numeric_cols)
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)
        df.set_index('timestamp', inplace=True)
        df.rename(columns={'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'volume': 'Volume'}, inplace=True)
        return df.dropna()
    except Exception as e:
        logger.error(f"âŒ [Data] Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ù„Ø¹Ù…Ù„Ø© {symbol}: {e}")
        return None

# ---------------------- Ø¯ÙˆØ§Ù„ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª (Ù…Ù†Ø³ÙˆØ®Ø© Ù…Ù† Ø§Ù„Ø¨ÙˆØª/Ø§Ù„Ù…Ø¯Ø±Ø¨) ----------------------
def calculate_candlestick_patterns(df: pd.DataFrame) -> pd.DataFrame:
    op, hi, lo, cl = df['Open'], df['High'], df['Low'], df['Close']
    body = abs(cl - op); candle_range = hi - lo
    candle_range[candle_range == 0] = 1e-9
    upper_wick = hi - pd.concat([op, cl], axis=1).max(axis=1)
    lower_wick = pd.concat([op, cl], axis=1).min(axis=1) - lo
    df['candlestick_pattern'] = 0
    is_bullish_engulfing = (cl.shift(1) < op.shift(1)) & (cl > op) & (cl >= op.shift(1)) & (op <= cl.shift(1)) & (body > body.shift(1))
    is_bearish_engulfing = (cl.shift(1) > op.shift(1)) & (cl < op) & (op >= cl.shift(1)) & (cl <= op.shift(1)) & (body > body.shift(1))
    df.loc[is_bullish_engulfing, 'candlestick_pattern'] = 1
    df.loc[is_bearish_engulfing, 'candlestick_pattern'] = -1
    return df

def calculate_sr_features(df: pd.DataFrame, sr_levels_df: pd.DataFrame) -> pd.DataFrame:
    if sr_levels_df.empty:
        df['dist_to_support'] = 0.0; df['dist_to_resistance'] = 0.0
        df['score_of_support'] = 0.0; df['score_of_resistance'] = 0.0
        return df
    supports = sr_levels_df[sr_levels_df['level_type'].str.contains('support|poc|confluence', case=False)]['level_price'].sort_values().to_numpy()
    resistances = sr_levels_df[sr_levels_df['level_type'].str.contains('resistance|poc|confluence', case=False)]['level_price'].sort_values().to_numpy()
    support_scores = sr_levels_df[sr_levels_df['level_type'].str.contains('support|poc|confluence', case=False)].set_index('level_price')['score'].to_dict()
    resistance_scores = sr_levels_df[sr_levels_df['level_type'].str.contains('resistance|poc|confluence', case=False)].set_index('level_price')['score'].to_dict()

    def get_sr_info(price):
        dist_support, score_support, dist_resistance, score_resistance = 1.0, 0.0, 1.0, 0.0
        if supports.size > 0:
            idx = np.searchsorted(supports, price, side='right') - 1
            if idx >= 0:
                nearest_support = supports[idx]
                dist_support = (price - nearest_support) / price if price > 0 else 0
                score_support = support_scores.get(nearest_support, 0)
        if resistances.size > 0:
            idx = np.searchsorted(resistances, price, side='left')
            if idx < len(resistances):
                nearest_resistance = resistances[idx]
                dist_resistance = (nearest_resistance - price) / price if price > 0 else 0
                score_resistance = resistance_scores.get(nearest_resistance, 0)
        return dist_support, score_support, dist_resistance, score_resistance
    results = df['Close'].apply(get_sr_info)
    df[['dist_to_support', 'score_of_support', 'dist_to_resistance', 'score_of_resistance']] = pd.DataFrame(results.tolist(), index=df.index)
    return df

def calculate_ichimoku_based_features(df: pd.DataFrame) -> pd.DataFrame:
    df['price_vs_tenkan'] = (df['Close'] - df['tenkan_sen']) / df['tenkan_sen']
    df['price_vs_kijun'] = (df['Close'] - df['kijun_sen']) / df['kijun_sen']
    df['tenkan_vs_kijun'] = (df['tenkan_sen'] - df['kijun_sen']) / df['kijun_sen']
    df['price_vs_kumo_a'] = (df['Close'] - df['senkou_span_a']) / df['senkou_span_a']
    df['price_vs_kumo_b'] = (df['Close'] - df['senkou_span_b']) / df['senkou_span_b']
    df['kumo_thickness'] = (df['senkou_span_a'] - df['senkou_span_b']).abs() / df['Close']
    kumo_high = df[['senkou_span_a', 'senkou_span_b']].max(axis=1)
    kumo_low = df[['senkou_span_a', 'senkou_span_b']].min(axis=1)
    df['price_above_kumo'] = (df['Close'] > kumo_high).astype(int)
    df['price_below_kumo'] = (df['Close'] < kumo_low).astype(int)
    df['price_in_kumo'] = ((df['Close'] >= kumo_low) & (df['Close'] <= kumo_high)).astype(int)
    df['chikou_above_kumo'] = (df['chikou_span'] > kumo_high).astype(int)
    df['chikou_below_kumo'] = (df['chikou_span'] < kumo_low).astype(int)
    df['tenkan_kijun_cross'] = 0
    cross_up = (df['tenkan_sen'].shift(1) < df['kijun_sen'].shift(1)) & (df['tenkan_sen'] > df['kijun_sen'])
    cross_down = (df['tenkan_sen'].shift(1) > df['kijun_sen'].shift(1)) & (df['tenkan_sen'] < df['kijun_sen'])
    df.loc[cross_up, 'tenkan_kijun_cross'] = 1
    df.loc[cross_down, 'tenkan_kijun_cross'] = -1
    return df

def create_all_features(df: pd.DataFrame, btc_df: pd.DataFrame) -> pd.DataFrame:
    df_calc = df.copy()
    high_low = df_calc['High'] - df_calc['Low']; high_close = (df_calc['High'] - df_calc['Close'].shift()).abs(); low_close = (df_calc['Low'] - df_calc['Close'].shift()).abs()
    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
    df_calc['atr'] = tr.ewm(span=ATR_PERIOD, adjust=False).mean()
    up_move = df_calc['High'].diff(); down_move = -df_calc['Low'].diff()
    plus_dm = pd.Series(np.where((up_move > down_move) & (up_move > 0), up_move, 0.0), index=df_calc.index)
    minus_dm = pd.Series(np.where((down_move > up_move) & (down_move > 0), down_move, 0.0), index=df_calc.index)
    plus_di = 100 * plus_dm.ewm(span=ADX_PERIOD, adjust=False).mean() / df_calc['atr']
    minus_di = 100 * minus_dm.ewm(span=ADX_PERIOD, adjust=False).mean() / df_calc['atr']
    dx = 100 * (abs(plus_di - minus_di) / (plus_di + minus_di).replace(0, 1e-9))
    df_calc['adx'] = dx.ewm(span=ADX_PERIOD, adjust=False).mean()
    delta = df_calc['Close'].diff()
    gain = delta.clip(lower=0).ewm(com=RSI_PERIOD - 1, adjust=False).mean()
    loss = -delta.clip(upper=0).ewm(com=RSI_PERIOD - 1, adjust=False).mean()
    df_calc['rsi'] = 100 - (100 / (1 + (gain / loss.replace(0, 1e-9))))
    ema_fast = df_calc['Close'].ewm(span=MACD_FAST, adjust=False).mean(); ema_slow = df_calc['Close'].ewm(span=MACD_SLOW, adjust=False).mean()
    macd_line = ema_fast - ema_slow; signal_line = macd_line.ewm(span=MACD_SIGNAL, adjust=False).mean()
    df_calc['macd_hist'] = macd_line - signal_line
    df_calc['macd_cross'] = 0
    df_calc.loc[(df_calc['macd_hist'].shift(1) < 0) & (df_calc['macd_hist'] >= 0), 'macd_cross'] = 1
    df_calc.loc[(df_calc['macd_hist'].shift(1) > 0) & (df_calc['macd_hist'] <= 0), 'macd_cross'] = -1
    sma = df_calc['Close'].rolling(window=BBANDS_PERIOD).mean(); std_dev = df_calc['Close'].rolling(window=BBANDS_PERIOD).std()
    upper_band = sma + (std_dev * 2); lower_band = sma - (std_dev * 2)
    df_calc['bb_width'] = (upper_band - lower_band) / (sma + 1e-9)
    rsi_val = df_calc['rsi']
    min_rsi = rsi_val.rolling(window=STOCH_RSI_PERIOD).min(); max_rsi = rsi_val.rolling(window=STOCH_RSI_PERIOD).max()
    stoch_rsi_val = (rsi_val - min_rsi) / (max_rsi - min_rsi).replace(0, 1e-9)
    df_calc['stoch_rsi_k'] = stoch_rsi_val.rolling(window=STOCH_K).mean() * 100
    df_calc['stoch_rsi_d'] = df_calc['stoch_rsi_k'].rolling(window=STOCH_D).mean()
    df_calc['relative_volume'] = df_calc['Volume'] / (df_calc['Volume'].rolling(window=REL_VOL_PERIOD, min_periods=1).mean() + 1e-9)
    df_calc['market_condition'] = 0
    df_calc.loc[(df_calc['rsi'] > RSI_OVERBOUGHT) | (df_calc['stoch_rsi_k'] > STOCH_RSI_OVERBOUGHT), 'market_condition'] = 1
    df_calc.loc[(df_calc['rsi'] < RSI_OVERSOLD) | (df_calc['stoch_rsi_k'] < STOCH_RSI_OVERSOLD), 'market_condition'] = -1
    ema_fast_trend = df_calc['Close'].ewm(span=EMA_FAST_PERIOD, adjust=False).mean()
    ema_slow_trend = df_calc['Close'].ewm(span=EMA_SLOW_PERIOD, adjust=False).mean()
    df_calc['price_vs_ema50'] = (df_calc['Close'] / ema_fast_trend) - 1
    df_calc['price_vs_ema200'] = (df_calc['Close'] / ema_slow_trend) - 1
    df_calc['returns'] = df_calc['Close'].pct_change()
    merged_df = pd.merge(df_calc, btc_df[['btc_returns']], left_index=True, right_index=True, how='left').fillna(0)
    df_calc['btc_correlation'] = merged_df['returns'].rolling(window=BTC_CORR_PERIOD).corr(merged_df['btc_returns'])
    df_calc['hour_of_day'] = df_calc.index.hour
    df_calc = calculate_candlestick_patterns(df_calc)
    return df_calc

# ---------------------- ÙØ¦Ø© Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Backtesting.py ----------------------
class MLStrategy(Strategy):
    ml_model = None
    scaler = None
    feature_names = None

    def init(self):
        pass

    def next(self):
        if self.position:
            return

        try:
            features = self.data.df.loc[self.data.index[-1], self.feature_names]
            if features.isnull().any():
                return
        except (KeyError, IndexError):
            return

        features_df = pd.DataFrame([features])
        features_scaled_np = self.scaler.transform(features_df)
        features_scaled_df = pd.DataFrame(features_scaled_np, columns=self.feature_names)
        
        prediction = self.ml_model.predict(features_scaled_df)[0]
        prediction_proba = self.ml_model.predict_proba(features_scaled_df)[0]
        
        try:
            class_1_index = list(self.ml_model.classes_).index(1)
            prob_for_class_1 = prediction_proba[class_1_index]
        except ValueError:
            return

        if prediction == 1 and prob_for_class_1 >= MODEL_CONFIDENCE_THRESHOLD:
            current_atr = self.data.atr[-1]
            if pd.isna(current_atr) or current_atr == 0:
                return

            # --- âœ¨ Ø§Ù„ØªØ­Ø³ÙŠÙ†: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø±ØµÙŠØ¯ ÙƒØ§ÙÙ Ù‚Ø¨Ù„ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ¯Ø§ÙˆÙ„ ---
            # Ù†ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø±ØµÙŠØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠ Ø£ÙƒØ¨Ø± Ù…Ù† Ø­Ø¬Ù… Ø§Ù„ØµÙÙ‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
            if self.equity < TRADE_AMOUNT_USDT:
                return # Ø§Ù„Ø®Ø±ÙˆØ¬ Ù…Ù† Ø§Ù„Ø¯Ø§Ù„Ø© Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ø±ØµÙŠØ¯ ÙƒØ§ÙÙŠØ§Ù‹

            current_price = self.data.Close[-1]
            size_as_fraction = TRADE_AMOUNT_USDT / self.equity

            # Ù†Ø³ØªØ®Ø¯Ù… 0.99 ÙƒÙ‡Ø§Ù…Ø´ Ø£Ù…Ø§Ù† Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø­Ø¬Ù… Ø§Ù„ØµÙÙ‚Ø© Ù„Ø§ ÙŠØªØ¬Ø§ÙˆØ² Ø§Ù„Ø±ØµÙŠØ¯
            if size_as_fraction > 0 and size_as_fraction < 0.99:
                stop_loss_price = current_price - (current_atr * ATR_SL_MULTIPLIER)
                take_profit_price = current_price + (current_atr * ATR_TP_MULTIPLIER)
                self.buy(size=size_as_fraction, sl=stop_loss_price, tp=take_profit_price)

# ---------------------- ÙƒØªÙ„Ø© Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ----------------------
def run_backtest():
    """Ù‡Ø°Ù‡ Ù‡ÙŠ ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©ØŒ ÙˆØªØ¹Ù…Ù„ Ø§Ù„Ø¢Ù† ÙÙŠ Ø®ÙŠØ· Ù…Ù†ÙØµÙ„."""
    global conn
    logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© {BASE_ML_MODEL_NAME}...")
    
    init_db()
    # ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ§Ø­ Ù‚Ø¨Ù„ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©
    if not conn:
        logger.critical("âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ Ø¨Ø¯ÙˆÙ† Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        return

    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, 'crypto_list.txt')
        with open(file_path, 'r', encoding='utf-8') as f:
            symbols_to_test = [line.strip().upper() + "USDT" for line in f if line.strip() and not line.startswith('#')]
    except FileNotFoundError:
        logger.error("âŒ Ù…Ù„Ù 'crypto_list.txt' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ø³ÙŠØªÙ… Ø§Ù„Ø®Ø±ÙˆØ¬.")
        return

    logger.info(f"â„¹ï¸ Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª BTC Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ù„ÙØªØ±Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ (Out-of-Sample: {OUT_OF_SAMPLE_OFFSET_DAYS} days)...")
    btc_df_full = fetch_historical_data(BTC_SYMBOL, SIGNAL_GENERATION_TIMEFRAME, BACKTEST_PERIOD_DAYS + 10, out_of_sample_period_days=OUT_OF_SAMPLE_OFFSET_DAYS)
    if btc_df_full is None:
        logger.critical("âŒ ÙØ´Ù„ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª BTC. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©."); return
    btc_df_full['btc_returns'] = btc_df_full['Close'].pct_change()

    all_stats = []
    
    for symbol in tqdm(symbols_to_test, desc="Backtesting Symbols"):
        logger.info(f"\n--- â³ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ù…Ø²: {symbol} ---")
        
        model_bundle = load_ml_model_bundle_from_db(symbol)
        if not model_bundle:
            logger.warning(f"âš ï¸ ØªØ®Ø·ÙŠ {symbol}: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬.")
            continue
        
        df_15m = fetch_historical_data(symbol, SIGNAL_GENERATION_TIMEFRAME, BACKTEST_PERIOD_DAYS, out_of_sample_period_days=OUT_OF_SAMPLE_OFFSET_DAYS)
        df_4h = fetch_historical_data(symbol, HIGHER_TIMEFRAME, BACKTEST_PERIOD_DAYS * 5, out_of_sample_period_days=OUT_OF_SAMPLE_OFFSET_DAYS)
        
        if df_15m is None or df_15m.empty or df_4h is None or df_4h.empty:
            logger.warning(f"âš ï¸ ØªØ®Ø·ÙŠ {symbol}: Ø¨ÙŠØ§Ù†Ø§Øª ØªØ§Ø±ÙŠØ®ÙŠØ© ØºÙŠØ± ÙƒØ§ÙÙŠØ©.")
            continue
            
        sr_levels = fetch_sr_levels_from_db(symbol)
        ichimoku_data = fetch_ichimoku_features_from_db(symbol, SIGNAL_GENERATION_TIMEFRAME)

        logger.info(f"Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ù€ {symbol}...")
        
        data = create_all_features(df_15m, btc_df_full)
        
        delta_4h = df_4h['Close'].diff()
        gain_4h = delta_4h.clip(lower=0).ewm(com=RSI_PERIOD - 1, adjust=False).mean()
        loss_4h = -delta_4h.clip(upper=0).ewm(com=RSI_PERIOD - 1, adjust=False).mean()
        df_4h['rsi_4h'] = 100 - (100 / (1 + (gain_4h / loss_4h.replace(0, 1e-9))))
        ema_fast_4h = df_4h['Close'].ewm(span=EMA_FAST_PERIOD, adjust=False).mean()
        df_4h['price_vs_ema50_4h'] = (df_4h['Close'] / ema_fast_4h) - 1
        mtf_features = df_4h[['rsi_4h', 'price_vs_ema50_4h']]
        data = data.join(mtf_features, how='left').fillna(method='ffill')

        data = calculate_sr_features(data, sr_levels)

        if not ichimoku_data.empty:
            data = data.join(ichimoku_data, how='left')
            data = calculate_ichimoku_based_features(data)
        
        data.replace([np.inf, -np.inf], np.nan, inplace=True)
        data.dropna(inplace=True)
        
        if data.empty:
            logger.warning(f"âš ï¸ ØªØ®Ø·ÙŠ {symbol}: DataFrame ÙØ§Ø±Øº Ø¨Ø¹Ø¯ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª.")
            continue

        logger.info(f"Ø¬Ø§Ø±ÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ Ù„Ù€ {symbol}...")
        bt = Backtest(
            data,
            MLStrategy,
            cash=INITIAL_CASH,
            commission=COMMISSION,
            exclusive_orders=True
        )
        
        stats = bt.run(
            ml_model=model_bundle['model'],
            scaler=model_bundle['scaler'],
            feature_names=model_bundle['feature_names']
        )
        
        logger.info(f"\n--- Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ Ù„Ù€ {symbol} ---")
        print(stats)
        all_stats.append(stats)
        
        del data, df_15m, df_4h, sr_levels, ichimoku_data, model_bundle
        gc.collect()

    logger.info("\n\n--- ğŸ Ù…Ù„Ø®Øµ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ ğŸ ---")
    if all_stats:
        summary_df = pd.DataFrame(all_stats)
        summary_df.index = [s['_strategy'] for s in all_stats]
        print(summary_df[[
            'Duration', 'Return [%]', 'Buy & Hold Return [%]', 'Win Rate [%]', 
            'Profit Factor', 'Sharpe Ratio', 'Sortino Ratio', '# Trades'
        ]])
        
        # --- START: Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ ---
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù‚Ø¨Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
        required_cols = ['Equity Final [$]', 'Start Equity [$]', '# Trades', 'Win Rate [%]', 'Profit Factor']
        if all(col in summary_df.columns for col in required_cols):
            total_trades = summary_df['# Trades'].sum()
            # ØªØµØ­ÙŠØ­ Ø§Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ Ù…Ù† 'Equity Start [$]' Ø¥Ù„Ù‰ 'Start Equity [$]'
            total_profit = summary_df['Equity Final [$]'].sum() - summary_df['Start Equity [$]'].sum()
            avg_win_rate = summary_df['Win Rate [%]'].mean()
            avg_profit_factor = summary_df['Profit Factor'].mean()
            
            print("\n--- Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© ---")
            print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ø®ØªØ¨Ø±Ø©: {len(summary_df)}")
            print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙ‚Ø§Øª: {total_trades}")
            print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ ØµØ§ÙÙŠ Ø§Ù„Ø±Ø¨Ø­/Ø§Ù„Ø®Ø³Ø§Ø±Ø©: ${total_profit:,.2f}")
            print(f"Ù…ØªÙˆØ³Ø· Ù†Ø³Ø¨Ø© Ø§Ù„Ø±Ø¨Ø­: {avg_win_rate:.2f}%")
            print(f"Ù…ØªÙˆØ³Ø· Ø¹Ø§Ù…Ù„ Ø§Ù„Ø±Ø¨Ø­: {avg_profit_factor:.2f}")
        else:
            logger.warning("\n--- âš ï¸ ØªØ¹Ø°Ø± Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© ---")
            logger.warning("ÙˆØ§Ø­Ø¯ Ø£Ùˆ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ('Equity Final [$]', 'Start Equity [$]') ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬.")
            logger.warning(f"Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ØªØ§Ø­Ø© Ù‡ÙŠ: {list(summary_df.columns)}")
        # --- END: Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ ---
            
    else:
        print("Ù„Ù… ÙŠØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø£ÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø®Ù„ÙÙŠ Ø¨Ù†Ø¬Ø§Ø­.")
        
    if conn:
        conn.close()
    logger.info("âœ… Ø§Ù†ØªÙ‡Ù‰ Ø®ÙŠØ· Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ.")


if __name__ == "__main__":
    # ØªØ´ØºÙŠÙ„ ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ ÙÙŠ Ø®ÙŠØ· Ù…Ù†ÙØµÙ„ Ø­ØªÙ‰ Ù„Ø§ ØªÙ…Ù†Ø¹ Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨ Ù…Ù† Ø§Ù„Ø¨Ø¯Ø¡
    backtest_thread = threading.Thread(target=run_backtest, name="run_backtest", daemon=True)
    backtest_thread.start()
    
    # ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨ Ù„Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø©
    # Render Ø³ÙŠÙˆÙØ± Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© PORT
    port = int(os.environ.get("PORT", 10000))
    # Ø§Ø³ØªØ®Ø¯Ù… '0.0.0.0' Ù„Ø¬Ø¹Ù„ Ø§Ù„Ø®Ø§Ø¯Ù… Ù…ØªØ§Ø­Ù‹Ø§ Ø®Ø§Ø±Ø¬ÙŠÙ‹Ø§
    app.run(host='0.0.0.0', port=port)
