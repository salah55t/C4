# ŸÖŸÑŸÅ c4_complete_v28.py - ÿßŸÑŸÜÿ≥ÿÆÿ© ÿßŸÑŸÉÿßŸÖŸÑÿ© V28import timeimport osimport jsonimport loggingimport requestsimport numpy as npimport pandas as pdimport psycopg2import pickleimport redisimport reimport gcimport randomfrom decimal import Decimal, ROUND_DOWNfrom urllib.parse import urlparsefrom psycopg2 import sql, OperationalError, InterfaceErrorfrom psycopg2.extras import RealDictCursorfrom binance.client import Clientfrom binance import ThreadedWebsocketManagerfrom binance.exceptions import BinanceAPIExceptionfrom flask import Flask, request, Response, jsonify, render_template_stringfrom flask_cors import CORSfrom threading import Thread, Lockfrom datetime import datetime, timedelta, timezonefrom decouple import configfrom typing import List, Dict, Optional, Any, Set, Tuplefrom sklearn.preprocessing import StandardScalerfrom collections import dequeimport warnings# --- ÿ™ÿ¨ÿßŸáŸÑ ÿßŸÑÿ™ÿ≠ÿ∞Ÿäÿ±ÿßÿ™ ---warnings.simplefilter(action='ignore', category=FutureWarning)warnings.simplefilter(action='ignore', category=UserWarning)# ---------------------- ÿ•ÿπÿØÿßÿØ ŸÜÿ∏ÿßŸÖ ÿßŸÑÿ™ÿ≥ÿ¨ŸäŸÑ ----------------------logging.basicConfig(    level=logging.INFO,    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',    handlers=[        logging.FileHandler('crypto_bot_v28_complete_logs.log', encoding='utf-8'),        logging.StreamHandler()    ])logger = logging.getLogger('CryptoBotV28_Complete')# ---------------------- ÿ™ÿ≠ŸÖŸäŸÑ ŸÖÿ™ÿ∫Ÿäÿ±ÿßÿ™ ÿßŸÑÿ®Ÿäÿ¶ÿ© ----------------------try:    API_KEY: str = config('BINANCE_API_KEY')    API_SECRET: str = config('BINANCE_API_SECRET')    TELEGRAM_TOKEN: str = config('TELEGRAM_BOT_TOKEN')    CHAT_ID: str = config('TELEGRAM_CHAT_ID')    DB_URL: str = config('DATABASE_URL')    WEBHOOK_URL: Optional[str] = config('WEBHOOK_URL', default=None)    REDIS_URL: str = config('REDIS_URL', default='redis://localhost:6379/0')except Exception as e:    logger.critical(f"‚ùå ŸÅÿ¥ŸÑ ÿ≠ÿßÿ≥ŸÖ ŸÅŸä ÿ™ÿ≠ŸÖŸäŸÑ ŸÖÿ™ÿ∫Ÿäÿ±ÿßÿ™ ÿßŸÑÿ®Ÿäÿ¶ÿ© ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©: {e}")    exit(1)# ---------------------- ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ±ÿßÿ™ ÿßŸÑÿπÿßŸÖÿ© ----------------------is_trading_enabled: bool = Falsetrading_status_lock = Lock()force_momentum_strategy: bool = Falseforce_momentum_lock = Lock()RISK_PER_TRADE_PERCENT: float = 1.0BASE_ML_MODEL_NAME: str = 'LightGBM_Scalping_V8_With_Momentum'MODEL_FOLDER: str = 'V8'SIGNAL_GENERATION_TIMEFRAME: str = '15m'TIMEFRAMES_FOR_TREND_LIGHTS: List[str] = ['15m', '1h', '4h']SIGNAL_GENERATION_LOOKBACK_DAYS: int = 30REDIS_PRICES_HASH_NAME: str = "crypto_bot_current_prices_v8"DIRECT_API_CHECK_INTERVAL: int = 10TRADING_FEE_PERCENT: float = 0.1STATS_TRADE_SIZE_USDT: float = 10.0BTC_SYMBOL: str = 'BTCUSDT'SYMBOL_PROCESSING_BATCH_SIZE: int = 50ADX_PERIOD: int = 14; RSI_PERIOD: int = 14; ATR_PERIOD: int = 14EMA_FAST_PERIOD: int = 50; EMA_SLOW_PERIOD: int = 200REL_VOL_PERIOD: int = 30; MOMENTUM_PERIOD: int = 12; EMA_SLOPE_PERIOD: int = 5MAX_OPEN_TRADES: int = 4BUY_CONFIDENCE_THRESHOLD = 0.80MIN_CONFIDENCE_INCREASE_FOR_UPDATE = 0.05ATR_FALLBACK_SL_MULTIPLIER: float = 1.5ATR_FALLBACK_TP_MULTIPLIER: float = 2.2USE_TRAILING_STOP_LOSS: bool = TrueTRAILING_ACTIVATION_PROFIT_PERCENT: float = 1.0TRAILING_DISTANCE_PERCENT: float = 0.8LAST_PEAK_UPDATE_TIME: Dict[int, float] = {}PEAK_UPDATE_COOLDOWN: int = 60USE_PEAK_FILTER: bool = TruePEAK_CHECK_PERIOD: int = 50PULLBACK_THRESHOLD_PCT: float = 0.988BREAKOUT_ALLOWANCE_PCT: float = 1.003DYNAMIC_FILTER_ANALYSIS_INTERVAL: int = 300ORDER_BOOK_DEPTH_LIMIT: int = 100ORDER_BOOK_WALL_MULTIPLIER: float = 10.0ORDER_BOOK_ANALYSIS_RANGE_PCT: float = 0.02# ---------------------- ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ±ÿßÿ™ ÿßŸÑÿπÿßŸÑŸÖŸäÿ© ----------------------conn: Optional[psycopg2.extensions.connection] = Noneclient: Optional[Client] = Noneredis_client: Optional[redis.Redis] = Noneml_models_cache: Dict[str, Any] = {}exchange_info_map: Dict[str, Any] = {}validated_symbols_to_scan: List[str] = []open_signals_cache: Dict[str, Dict] = {}signal_cache_lock = Lock()notifications_cache = deque(maxlen=50)notifications_lock = Lock()signals_pending_closure: Set[int] = set()closure_lock = Lock()last_api_check_time = time.time()rejection_logs_cache = deque(maxlen=100)rejection_logs_lock = Lock()last_market_state_check = 0current_market_state: Dict[str, Any] = {"overall_regime": "INITIALIZING", "trend_details_by_tf": {}, "last_updated": None}market_state_lock = Lock()dynamic_filter_profile_cache: Dict[str, Any] = {}last_dynamic_filter_analysis_time: float = 0dynamic_filter_lock = Lock()REJECTION_REASONS_AR = {    "Filters Not Loaded": "ÿßŸÑŸÅŸÑÿßÿ™ÿ± ÿ∫Ÿäÿ± ŸÖÿ≠ŸÖŸÑÿ©",    "Low Volatility": "ÿ™ŸÇŸÑÿ® ŸÖŸÜÿÆŸÅÿ∂ ÿ¨ÿØÿßŸã",    "BTC Correlation": "ÿßÿ±ÿ™ÿ®ÿßÿ∑ ÿ∂ÿπŸäŸÅ ÿ®ÿßŸÑÿ®Ÿäÿ™ŸÉŸàŸäŸÜ",    "RRR Filter": "ŸÜÿ≥ÿ®ÿ© ÿßŸÑŸÖÿÆÿßÿ∑ÿ±ÿ©/ÿßŸÑÿπÿßÿ¶ÿØ ÿ∫Ÿäÿ± ŸÉÿßŸÅŸäÿ©",    "Reversal Volume Filter": "ŸÅŸàŸÑŸäŸàŸÖ ÿßŸÑÿßŸÜÿπŸÉÿßÿ≥ ÿ∂ÿπŸäŸÅ",    "Momentum/Strength Filter": "ŸÅŸÑÿ™ÿ± ÿßŸÑÿ≤ÿÆŸÖ ŸàÿßŸÑŸÇŸàÿ©",    "Peak/Pullback Filter": "ŸÅŸÑÿ™ÿ± ÿßŸÑŸÇŸÖÿ©/ÿßŸÑÿ™ÿµÿ≠Ÿäÿ≠",    "Invalid ATR for TP/SL": "ATR ÿ∫Ÿäÿ± ÿµÿßŸÑÿ≠ ŸÑÿ≠ÿ≥ÿßÿ® ÿßŸÑÿ£ŸáÿØÿßŸÅ",    "Reversal Signal Rejected by ML Model": "ŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿ¢ŸÑŸä ÿ±ŸÅÿ∂ ÿ•ÿ¥ÿßÿ±ÿ© ÿßŸÑÿßŸÜÿπŸÉÿßÿ≥",    "Invalid Position Size": "ÿ≠ÿ¨ŸÖ ÿßŸÑÿµŸÅŸÇÿ© ÿ∫Ÿäÿ± ÿµÿßŸÑÿ≠ (ÿßŸÑŸàŸÇŸÅ ÿ™ÿ≠ÿ™ ÿßŸÑÿØÿÆŸàŸÑ)",    "Lot Size Adjustment Failed": "ŸÅÿ¥ŸÑ ÿ∂ÿ®ÿ∑ ÿ≠ÿ¨ŸÖ ÿßŸÑÿπŸÇÿØ (LOT_SIZE)",    "Min Notional Filter": "ŸÇŸäŸÖÿ© ÿßŸÑÿµŸÅŸÇÿ© ÿ£ŸÇŸÑ ŸÖŸÜ ÿßŸÑÿ≠ÿØ ÿßŸÑÿ£ÿØŸÜŸâ",    "Insufficient Balance": "ÿßŸÑÿ±ÿµŸäÿØ ÿ∫Ÿäÿ± ŸÉÿßŸÅŸç",    "Order Book Fetch Failed": "ŸÅÿ¥ŸÑ ÿ¨ŸÑÿ® ÿØŸÅÿ™ÿ± ÿßŸÑÿ∑ŸÑÿ®ÿßÿ™",    "Order Book Imbalance": "ÿßÿÆÿ™ŸÑÿßŸÑ ÿ™Ÿàÿßÿ≤ŸÜ ÿØŸÅÿ™ÿ± ÿßŸÑÿ∑ŸÑÿ®ÿßÿ™ (ÿ∂ÿ∫ÿ∑ ÿ®Ÿäÿπ)",    "Large Sell Wall Detected": "ÿ™ŸÖ ŸÉÿ¥ŸÅ ÿ¨ÿØÿßÿ± ÿ®Ÿäÿπ ÿ∂ÿÆŸÖ",}# ---------------------- ÿØŸàÿßŸÑ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ----------------------def init_db(retries: int = 5, delay: int = 5) -> None:    global conn    logger.info("[DB] Initializing database connection...")    db_url_to_use = DB_URL    if 'postgres' in db_url_to_use and 'sslmode' not in db_url_to_use:        separator = '&' if '?' in db_url_to_use else '?'        db_url_to_use += f"{separator}sslmode=require"        for attempt in range(retries):        try:            conn = psycopg2.connect(db_url_to_use, connect_timeout=15, cursor_factory=RealDictCursor)            conn.autocommit = False                        with conn.cursor() as cur:                cur.execute("""                    CREATE TABLE IF NOT EXISTS signals (                        id SERIAL PRIMARY KEY, symbol TEXT NOT NULL, entry_price DOUBLE PRECISION NOT NULL,                        target_price DOUBLE PRECISION NOT NULL, stop_loss DOUBLE PRECISION NOT NULL,                        status TEXT DEFAULT 'open', closing_price DOUBLE PRECISION, closed_at TIMESTAMP,                        profit_percentage DOUBLE PRECISION, strategy_name TEXT, signal_details JSONB,                        current_peak_price DOUBLE PRECISION, is_real_trade BOOLEAN DEFAULT FALSE,                        quantity DOUBLE PRECISION, order_id TEXT                    );                """)                cur.execute("CREATE INDEX IF NOT EXISTS idx_signals_status ON signals (status);")                cur.execute("""                    CREATE TABLE IF NOT EXISTS notifications (                        id SERIAL PRIMARY KEY, timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),                        type TEXT NOT NULL, message TEXT NOT NULL, is_read BOOLEAN DEFAULT FALSE                    );                """)            conn.commit()            logger.info("‚úÖ [DB] Database connection and schema are up-to-date.")            return        except Exception as e:            logger.error(f"‚ùå [DB] Error during initialization (Attempt {attempt + 1}/{retries}): {e}")            if conn: conn.rollback()            if attempt < retries - 1: time.sleep(delay)            else: logger.critical("‚ùå [DB] Failed to connect to the database.")def check_db_connection() -> bool:    global conn    if conn is None or conn.closed != 0:        logger.warning("[DB] Connection is closed, attempting to reconnect...")        init_db()    try:        if conn and conn.closed == 0:            with conn.cursor() as cur: cur.execute("SELECT 1;")            return True        return False    except (OperationalError, InterfaceError) as e:        logger.error(f"‚ùå [DB] Connection lost: {e}. Reconnecting...")        try:            init_db()            return conn is not None and conn.closed == 0        except Exception as retry_e:            logger.error(f"‚ùå [DB] Reconnect failed: {retry_e}")            return False    return Falsedef log_and_notify(level: str, message: str, notification_type: str):    log_methods = {'info': logger.info, 'warning': logger.warning, 'error': logger.error, 'critical': logger.critical}    log_methods.get(level.lower(), logger.info)(message)    if not check_db_connection() or not conn: return    try:        new_notification = {"timestamp": datetime.now(timezone.utc).isoformat(), "type": notification_type, "message": message}        with notifications_lock: notifications_cache.appendleft(new_notification)        with conn.cursor() as cur: cur.execute("INSERT INTO notifications (type, message) VALUES (%s, %s);", (notification_type, message))        conn.commit()    except Exception as e:        logger.error(f"‚ùå [Notify DB] Failed to save notification: {e}")        if conn: conn.rollback()def log_rejection(symbol: str, reason_key: str, details: Optional[Dict] = None):    reason_ar = REJECTION_REASONS_AR.get(reason_key, reason_key)    log_message = f"üö´ [REJECTED] {symbol} | Reason: {reason_key} | Details: {details or {}}"    logger.info(log_message)    with rejection_logs_lock:        rejection_logs_cache.appendleft({            "timestamp": datetime.now(timezone.utc).isoformat(),            "symbol": symbol,            "reason": reason_ar,            "details": details or {}        })def init_redis() -> None:    global redis_client    logger.info("[Redis] Initializing Redis connection...")    try:        redis_client = redis.from_url(REDIS_URL, decode_responses=True)        redis_client.ping()        logger.info("‚úÖ [Redis] Successfully connected to Redis server.")    except redis.exceptions.ConnectionError as e:        logger.critical(f"‚ùå [Redis] Failed to connect to Redis: {e}")        exit(1)# ---------------------- ÿØŸàÿßŸÑ Binance ŸàÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ----------------------def get_exchange_info_map() -> None:    global exchange_info_map    if not client: return    logger.info("‚ÑπÔ∏è [Exchange Info] Fetching exchange trading rules...")    try:        info = client.get_exchange_info()        exchange_info_map = {s['symbol']: s for s in info['symbols']}        logger.info(f"‚úÖ [Exchange Info] Loaded rules for {len(exchange_info_map)} symbols.")    except Exception as e:        logger.error(f"‚ùå [Exchange Info] Could not fetch exchange info: {e}")def get_validated_symbols(filename: str = 'crypto_list.txt') -> List[str]:    if not client: return []    try:        script_dir = os.path.dirname(os.path.abspath(__file__))        file_path = os.path.join(script_dir, filename)        with open(file_path, 'r', encoding='utf-8') as f:            raw_symbols = {line.strip().upper() for line in f if line.strip() and not line.startswith('#')}        formatted = {f"{s}USDT" if not s.endswith('USDT') else s for s in raw_symbols}                if not exchange_info_map: get_exchange_info_map()        active = {s for s, info in exchange_info_map.items() if info.get('quoteAsset') == 'USDT' and info.get('status') == 'TRADING'}        validated = sorted(list(formatted.intersection(active)))        logger.info(f"‚úÖ [Validation] Bot will monitor {len(validated)} symbols.")        return validated    except Exception as e:        logger.error(f"‚ùå [Validation] Error during symbol validation: {e}", exc_info=True)        return []def fetch_historical_data(symbol: str, interval: str, days: int) -> Optional[pd.DataFrame]:    if not client: return None    try:        limit = int((days * 24 * 60) / int(re.sub('[a-zA-Z]', '', interval)))        klines = client.get_historical_klines(symbol, interval, limit=min(limit, 1000))        if not klines: return None        df = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_volume', 'trades', 'taker_buy_base', 'taker_buy_quote', 'ignore'])        df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]        for col in ['open', 'high', 'low', 'close', 'volume']:            df[col] = pd.to_numeric(df[col], errors='coerce')        df = df.astype({'open': np.float32, 'high': np.float32, 'low': np.float32, 'close': np.float32, 'volume': np.float32})        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)        df.set_index('timestamp', inplace=True)        return df.dropna()    except BinanceAPIException as e:        logger.error(f"‚ùå [Data] Binance API Error for {symbol}: {e}")        if e.code == -1003:            logger.critical("IP BANNED! Waiting for 60 seconds before trying to continue.")            time.sleep(60)        return None    except Exception as e:        logger.error(f"‚ùå [Data] Error fetching historical data for {symbol}: {e}")        return Nonedef analyze_order_book(symbol: str, entry_price: float) -> Optional[Dict[str, Any]]:    if not client: return None    try:        order_book = client.get_order_book(symbol=symbol, limit=ORDER_BOOK_DEPTH_LIMIT)                bids = pd.DataFrame(order_book['bids'], columns=['price', 'qty'], dtype=float)        asks = pd.DataFrame(order_book['asks'], columns=['price', 'qty'], dtype=float)        price_range = entry_price * ORDER_BOOK_ANALYSIS_RANGE_PCT        relevant_bids_vol = bids[bids['price'] >= entry_price - price_range]['qty'].sum()        relevant_asks_vol = asks[asks['price'] <= entry_price + price_range]['qty'].sum()        bid_ask_ratio = relevant_bids_vol / relevant_asks_vol if relevant_asks_vol > 0 else float('inf')        avg_ask_qty = asks['qty'].mean()        sell_wall_threshold = avg_ask_qty * ORDER_BOOK_WALL_MULTIPLIER        nearby_asks = asks[asks['price'].between(entry_price, entry_price * 1.05)]        large_sell_walls = nearby_asks[nearby_asks['qty'] > sell_wall_threshold]        analysis_result = {            "bid_ask_ratio": bid_ask_ratio,            "has_large_sell_wall": not large_sell_walls.empty,            "wall_details": large_sell_walls.to_dict('records') if not large_sell_walls.empty else []        }        logger.info(f"üìñ [{symbol}] Order Book Analysis: Ratio={bid_ask_ratio:.2f}, Has Wall={analysis_result['has_large_sell_wall']}")        return analysis_result    except Exception as e:        logger.error(f"‚ùå [{symbol}] Failed to fetch or analyze order book: {e}")        log_rejection(symbol, "Order Book Fetch Failed", {"error": str(e)})        return None# ---------------------- ÿØŸàÿßŸÑ ÿ≠ÿ≥ÿßÿ® ÿßŸÑŸÖŸäÿ≤ÿßÿ™ ----------------------def calculate_features(df: pd.DataFrame, btc_df: Optional[pd.DataFrame]) -> pd.DataFrame:    df_calc = df.copy()        # ATR    high_low = df_calc['high'] - df_calc['low']    high_close = (df_calc['high'] - df_calc['close'].shift()).abs()    low_close = (df_calc['low'] - df_calc['close'].shift()).abs()    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)    df_calc['atr'] = tr.ewm(span=ATR_PERIOD, adjust=False).mean()        # ADX    up_move = df_calc['high'].diff()    down_move = -df_calc['low'].diff()    plus_dm = pd.Series(np.where((up_move > down_move) & (up_move > 0), up_move, 0.0), index=df_calc.index)    minus_dm = pd.Series(np.where((down_move > up_move) & (down_move > 0), down_move, 0.0), index=df_calc.index)    plus_di = 100 * plus_dm.ewm(span=ADX_PERIOD, adjust=False).mean() / df_calc['atr'].replace(0, 1e-9)    minus_di = 100 * minus_dm.ewm(span=ADX_PERIOD, adjust=False).mean() / df_calc['atr'].replace(0, 1e-9)    dx = 100 * (abs(plus_di - minus_di) / (plus_di + minus_di).replace(0, 1e-9))    df_calc['adx'] = dx.ewm(span=ADX_PERIOD, adjust=False).mean()        # RSI    delta = df_calc['close'].diff()    gain = delta.clip(lower=0).ewm(com=RSI_PERIOD - 1, adjust=False).mean()    loss = -delta.clip(upper=0).ewm(com=RSI_PERIOD - 1, adjust=False).mean()    df_calc['rsi'] = 100 - (100 / (1 + (gain / loss.replace(0, 1e-9))))        # Volume metrics    df_calc['relative_volume'] = df_calc['volume'] / (df_calc['volume'].rolling(window=REL_VOL_PERIOD, min_periods=1).mean() + 1e-9)    df_calc['price_vs_ema50'] = (df_calc['close'] / df_calc['close'].ewm(span=EMA_FAST_PERIOD, adjust=False).mean()) - 1    df_calc['price_vs_ema200'] = (df_calc['close'] / df_calc['close'].ewm(span=EMA_SLOW_PERIOD, adjust=False).mean()) - 1        # BTC Correlation    if btc_df is not None and not btc_df.empty:        merged_df = pd.merge(df_calc, btc_df[['btc_returns']], left_index=True, right_index=True, how='left').fillna(0)        df_calc['btc_correlation'] = df_calc['close'].pct_change().rolling(window=30).corr(merged_df['btc_returns'])    else:        df_calc['btc_correlation'] = 0.0            # ROC and slope    df_calc[f'roc_{MOMENTUM_PERIOD}'] = (df_calc['close'] / df_calc['close'].shift(MOMENTUM_PERIOD) - 1) * 100    ema_slope = df_calc['close'].ewm(span=EMA_SLOPE_PERIOD, adjust=False).mean()    df_calc[f'ema_slope_{EMA_SLOPE_PERIOD}'] = (ema_slope - ema_slope.shift(1)) / ema_slope.shift(1).replace(0, 1e-9) * 100        return df_calc.astype('float32', errors='ignore')# ---------------------- ÿ£ŸÜÿ∏ŸÖÿ© ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÖÿ™ŸÇÿØŸÖÿ© ----------------------class EnhancedTrendAnalyzer:    @staticmethod    def get_enhanced_trend(df: pd.DataFrame, symbol: str, btc_df: pd.DataFrame) -> Dict[str, Any]:        if df is None or len(df) < 50 or btc_df is None:            return {"trend": "Uncertain", "strength": 0, "confidence": 0}                try:            close_series = df['close']            btc_close = btc_df['close']                        symbol_returns = close_series.pct_change()            btc_returns = btc_close.pct_change()            relative_strength = (symbol_returns - btc_returns).tail(20).mean() * 100                        exp1 = close_series.ewm(span=12, adjust=False).mean()            exp2 = close_series.ewm(span=26, adjust=False).mean()            macd = exp1 - exp2            signal_line = macd.ewm(span=9, adjust=False).mean()                        high_low = df['high'] - df['low']            high_close = (df['high'] - close_series.shift()).abs()            low_close = (df['low'] - close_series.shift()).abs()            tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)            atr = tr.ewm(span=ADX_PERIOD, adjust=False).mean()                        up_move = df['high'].diff()            down_move = -df['low'].diff()            plus_dm = pd.Series(np.where((up_move > down_move) & (up_move > 0), up_move, 0.0), index=df.index)            minus_dm = pd.Series(np.where((down_move > up_move) & (down_move > 0), down_move, 0.0), index=df.index)            plus_di = 100 * plus_dm.ewm(span=ADX_PERIOD, adjust=False).mean() / atr.replace(0, 1e-9)            minus_di = 100 * minus_dm.ewm(span=ADX_PERIOD, adjust=False).mean() / atr.replace(0, 1e-9)            dx = 100 * (abs(plus_di - minus_di) / (plus_di + minus_di).replace(0, 1e-9))            adx = dx.ewm(span=ADX_PERIOD, adjust=False).mean().iloc[-1]                        volume = df['volume']            avg_volume = volume.rolling(window=20).mean()            current_ratio = volume.iloc[-1] / avg_volume.iloc[-1] if avg_volume.iloc[-1] > 0 else 1                        close = df['close']            high = df['high']            low = df['low']            pivot = (high.iloc[-1] + low.iloc[-1] + close.iloc[-1]) / 3            r1 = 2 * pivot - low.iloc[-1]            s1 = 2 * pivot - high.iloc[-1]            current_price = close.iloc[-1]                        trend_score = 0            if macd.iloc[-1] > signal_line.iloc[-1]:                trend_score += 0.25            else:                trend_score -= 0.25                        adx_contribution = min(adx / 50, 1) * (1 if relative_strength > 0 else -1) * 0.30            trend_score += adx_contribution                        volume_score = np.sign(df['close'].pct_change().tail(5).mean() * volume.pct_change().tail(5).mean()) * min(abs(df['close'].pct_change().tail(5).mean() * 100), 1)            trend_score += volume_score * 0.15                        if current_price > pivot:                trend_score += 0.10            else:                trend_score -= 0.10                        if abs(trend_score) >= 0.7:                trend = "Strong Uptrend" if trend_score > 0 else "Strong Downtrend"            elif abs(trend_score) >= 0.3:                trend = "Uptrend" if trend_score > 0 else "Downtrend"            else:                trend = "Ranging"                        return {                "trend": trend,                "strength": abs(trend_score),                "confidence": min(abs(trend_score) * 100, 100),                "relative_strength": relative_strength,                "adx": float(adx),                "volume_score": float(volume_score),                "support": float(s1),                "resistance": float(r1)            }                    except Exception as e:            logger.error(f"‚ùå [Enhanced Trend] Error for {symbol}: {e}")            return {"trend": "Uncertain", "strength": 0, "confidence": 0}class MarketConditionsAnalyzer:    def __init__(self):        self.conditions_cache = {}        self.last_analysis = 0        def analyze_conditions(self) -> Dict[str, Any]:        current_time = time.time()        if current_time - self.last_analysis < 300:            return self.conditions_cache                try:            conditions = {                'volatility_regime': self._get_volatility_regime(),                'volume_regime': self._get_volume_regime(),                'correlation_regime': self._get_correlation_regime(),                'sentiment_score': self._get_sentiment_score(),                'session_type': self._get_session_type()            }                        self.conditions_cache = conditions            self.last_analysis = current_time            return conditions                    except Exception as e:            logger.error(f"‚ùå [Market Conditions] Error: {e}")            return self._get_default_conditions()        def _get_volatility_regime(self) -> str:        try:            btc_data = fetch_historical_data(BTC_SYMBOL, '1h', 7)            if btc_data is None:                return "normal"                        returns = btc_data['close'].pct_change()            volatility = returns.rolling(24).std().iloc[-1] * np.sqrt(24) * 100                        if volatility < 2: return "low"            elif volatility < 5: return "normal"            elif volatility < 10: return "high"            else: return "extreme"        except: return "normal"        def _get_volume_regime(self) -> str:        try:            btc_data = fetch_historical_data(BTC_SYMBOL, '1h', 7)            if btc_data is None: return "normal"                        volume = btc_data['volume']            avg_volume = volume.rolling(24).mean().iloc[-1]            current_volume = volume.iloc[-1]            ratio = current_volume / avg_volume if avg_volume > 0 else 1                        if ratio < 0.7: return "low"            elif ratio < 1.3: return "normal"            elif ratio < 2: return "high"            else: return "spike"        except: return "normal"        def _get_correlation_regime(self) -> str:        try:            major_coins = ['ETHUSDT', 'ADAUSDT', 'SOLUSDT', 'DOTUSDT']            correlations = []            btc_data = fetch_historical_data(BTC_SYMBOL, '1h', 2)            if btc_data is None: return "normal"                        btc_returns = btc_data['close'].pct_change().dropna()            for coin in major_coins:                data = fetch_historical_data(coin, '1h', 2)                if data is not None:                    coin_returns = data['close'].pct_change().dropna()                    if len(coin_returns) > 0:                        corr = btc_returns.tail(10).corr(coin_returns.tail(10))                        correlations.append(abs(corr))                        avg_correlation = np.mean(correlations) if correlations else 0.5            if avg_correlation < 0.3: return "low"            elif avg_correlation < 0.7: return "normal"            else: return "high"        except: return "normal"        def _get_sentiment_score(self) -> float:        try:            fear_greed = get_fear_and_greed_index()            return (fear_greed['value'] - 50) / 50 if fear_greed['value'] != -1 else 0        except: return 0        def _get_session_type(self) -> str:        active_sessions, liquidity_state, _ = get_session_state()        return liquidity_state        def _get_default_conditions(self) -> Dict[str, Any]:        return {'volatility_regime': 'normal', 'volume_regime': 'normal', 'correlation_regime': 'normal', 'sentiment_score': 0, 'session_type': 'NORMAL_LIQUIDITY'}class EnhancedFilterSystem:    def __init__(self):        self.analyzer = MarketConditionsAnalyzer()        def generate_filters(self) -> Dict[str, Any]:        conditions = self.analyzer.analyze_conditions()        base_profile = {            "adx": 25.0, "rel_vol": 0.4, "rsi_range": (52, 88), "roc": 0.05,            "slope": 0.01, "min_rrr": 1.4, "min_volatility_pct": 0.35,            "min_btc_correlation": 0.4, "min_bid_ask_ratio": 1.15        }                # ÿ™ÿ∑ÿ®ŸäŸÇ ÿßŸÑÿ™ÿπÿØŸäŸÑÿßÿ™        if conditions['volatility_regime'] == "low":            base_profile['min_volatility_pct'] *= 0.7            base_profile['min_rrr'] *= 1.2        elif conditions['volatility_regime'] == "high":            base_profile['min_volatility_pct'] *= 1.3            base_profile['min_rrr'] *= 0.8                if conditions['volume_regime'] == "low":            base_profile['rel_vol'] *= 0.5        elif conditions['volume_regime'] == "high":            base_profile['rel_vol'] *= 1.2                if conditions['correlation_regime'] == "high":            base_profile['min_btc_correlation'] = max(0.7, base_profile['min_btc_correlation'])        elif conditions['correlation_regime'] == "low":            base_profile['min_btc_correlation'] = max(0.2, base_profile['min_btc_correlation'])                sentiment = conditions['sentiment_score']        if sentiment > 0.5:            base_profile['rsi_range'] = (55, 85)        elif sentiment < -0.5:            base_profile['rsi_range'] = (35, 65)                return {            "name": f"ŸÅŸÑÿßÿ™ÿ± ŸÖÿ™ŸÉŸäŸÅÿ© - {conditions['volatility_regime']}",            "description": f"ŸÜÿ∏ÿßŸÖ ŸÖÿ™ŸÉŸäŸÅ: {conditions['volatility_regime']}/{conditions['volume_regime']}",            "strategy": "MOMENTUM",            "filters": base_profile,            "conditions": conditions        }# ---------------------- ÿØŸàÿßŸÑ ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© ----------------------class EnhancedTradingStrategy:    def __init__(self, symbol: str):        self.symbol = symbol        model_bundle = self._load_ml_model(symbol)        self.ml_model, self.scaler, self.feature_names = (model_bundle.get('model'), model_bundle.get('scaler'), model_bundle.get('feature_names')) if model_bundle else (None, None, None)    def _load_ml_model(self, symbol: str) -> Optional[Dict[str, Any]]:        model_name = f"{BASE_ML_MODEL_NAME}_{symbol}"        if model_name in ml_models_cache:            return ml_models_cache[model_name]                script_dir = os.path.dirname(os.path.abspath(__file__))        model_dir_path = os.path.join(script_dir, MODEL_FOLDER)        model_path = os.path.join(model_dir_path, f"{model_name}.pkl")                if not os.path.exists(model_path):            return None                try:            with open(model_path, 'rb') as f:                model_bundle = pickle.load(f)            if 'model' in model_bundle and 'scaler' in model_bundle and 'feature_names' in model_bundle:                ml_models_cache[model_name] = model_bundle                return model_bundle            return None        except Exception as e:            logger.error(f"‚ùå [ML Model] Error loading model for {symbol}: {e}")            return None    def get_features(self, df_15m: pd.DataFrame, df_4h: pd.DataFrame, btc_df: pd.DataFrame) -> Optional[pd.DataFrame]:        if self.feature_names is None: return None        try:            df_featured = calculate_features(df_15m, btc_df)            df_4h_features = calculate_features(df_4h, None)            df_4h_features = df_4h_features.rename(columns=lambda c: f"{c}_4h", inplace=False)            required_4h_cols = ['rsi_4h', 'price_vs_ema50_4h']            df_featured = df_featured.join(df_4h_features[required_4h_cols], how='outer')            df_featured.fillna(method='ffill', inplace=True)            for col in self.feature_names:                if col not in df_featured.columns: df_featured[col] = 0.0            df_featured.replace([np.inf, -np.inf], np.nan, inplace=True)            return df_featured.dropna(subset=self.feature_names)        except Exception as e:            logger.error(f"‚ùå [{self.symbol}] Feature engineering failed: {e}", exc_info=True)            return None    def generate_buy_signal(self, df_features: pd.DataFrame) -> Optional[Dict[str, Any]]:        if not all([self.ml_model, self.scaler, self.feature_names]) or df_features.empty: return None        try:            last_row_ordered_df = df_features.iloc[[-1]][self.feature_names]            features_scaled_np = self.scaler.transform(last_row_ordered_df)            features_scaled_df = pd.DataFrame(features_scaled_np, columns=self.feature_names)            prediction = self.ml_model.predict(features_scaled_df)[0]            if prediction != 1: return None                        prediction_proba = self.ml_model.predict_proba(features_scaled_df)            confidence = float(np.max(prediction_proba[0]))            logger.debug(f"‚ÑπÔ∏è [{self.symbol}] ML Model predicted 'BUY' with {confidence:.2%} confidence.")            return {'prediction': int(prediction), 'confidence': confidence}        except Exception as e:            logger.warning(f"‚ö†Ô∏è [{self.symbol}] ML Signal Generation Error: {e}")            return Nonedef find_crazy_reversal_signal(df_featured: pd.DataFrame) -> Optional[Dict[str, Any]]:    try:        if len(df_featured) < 30: return None                last_candle = df_featured.iloc[-1]        if last_candle['close'] <= last_candle['kc_upper']:            return None                lookback_period = 25        relevant_data = df_featured.iloc[-lookback_period:-1]                price_low_idx = relevant_data['low'].idxmin()        price_low_val = relevant_data.loc[price_low_idx, 'low']        rsi_at_price_low = relevant_data.loc[price_low_idx, 'rsi']        current_price_low = last_candle['low']        if current_price_low <= price