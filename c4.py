import time
import os
import json
import logging
import requests
import numpy as np
import pandas as pd
import psycopg2
from psycopg2 import sql, OperationalError, InterfaceError # Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¢Ù…Ù†Ø© ÙˆØ£Ø®Ø·Ø§Ø¡ Ù…Ø­Ø¯Ø¯Ø©
from psycopg2.extras import RealDictCursor # Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙƒÙ‚ÙˆØ§Ù…ÙŠØ³
from binance.client import Client
from binance import ThreadedWebsocketManager
from binance.exceptions import BinanceAPIException, BinanceRequestException # Ø£Ø®Ø·Ø§Ø¡ Binance Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
from flask import Flask, request, Response
from threading import Thread
from datetime import datetime, timedelta
from decouple import config
from typing import List, Dict, Optional, Tuple, Any, Union # Ù„Ø¥Ø¶Ø§ÙØ© Type Hinting

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ----------------------
logging.basicConfig(
    level=logging.INFO, # ÙŠÙ…ÙƒÙ† ØªØºÙŠÙŠØ± Ù‡Ø°Ø§ Ø¥Ù„Ù‰ logging.DEBUG Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø³Ø¬Ù„Ø§Øª Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', # Ø¥Ø¶Ø§ÙØ© Ø§Ø³Ù… Ø§Ù„Ù…Ø³Ø¬Ù„
    handlers=[
        logging.FileHandler('crypto_bot_elliott_fib.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø³Ù… Ù…Ø­Ø¯Ø¯ Ù„Ù„Ù…Ø³Ø¬Ù„ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø¬Ø°Ø±
logger = logging.getLogger('CryptoBot')

# ---------------------- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ© ----------------------
try:
    API_KEY: str = config('BINANCE_API_KEY')
    API_SECRET: str = config('BINANCE_API_SECRET')
    TELEGRAM_TOKEN: str = config('TELEGRAM_BOT_TOKEN')
    CHAT_ID: str = config('TELEGRAM_CHAT_ID')
    DB_URL: str = config('DATABASE_URL')
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© None Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ù…ØªØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§
    WEBHOOK_URL: Optional[str] = config('WEBHOOK_URL', default=None)
except Exception as e:
     logger.critical(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {e}")
     exit(1) # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø±Ù…Ø² Ø®Ø±ÙˆØ¬ ØºÙŠØ± ØµÙØ±ÙŠ Ù„Ù„Ø¥Ø´Ø§Ø±Ø© Ø¥Ù„Ù‰ Ø®Ø·Ø£

logger.info(f"Binance API Key: {'Ù…ØªÙˆÙØ±' if API_KEY else 'ØºÙŠØ± Ù…ØªÙˆÙØ±'}")
logger.info(f"Telegram Token: {TELEGRAM_TOKEN[:10]}...{'*' * (len(TELEGRAM_TOKEN)-10)}")
logger.info(f"Telegram Chat ID: {CHAT_ID}")
logger.info(f"Database URL: {'Ù…ØªÙˆÙØ±' if DB_URL else 'ØºÙŠØ± Ù…ØªÙˆÙØ±'}")
logger.info(f"Webhook URL: {WEBHOOK_URL if WEBHOOK_URL else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'}")

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø«ÙˆØ§Ø¨Øª ÙˆØ§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© (Ù…Ø¹Ø¯Ù„Ø© Ù„Ù„ÙØ­Øµ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± 15 Ø¯Ù‚ÙŠÙ‚Ø©) ----------------------
TRADE_VALUE: float = 10.0         # Default trade value in USDT (Keep small for testing)
MAX_OPEN_TRADES: int = 5          # Maximum number of open trades simultaneously (Increased slightly for scalping)
SIGNAL_GENERATION_TIMEFRAME: str = '15m' # Timeframe for signal generation (Changed to 15m)
SIGNAL_GENERATION_LOOKBACK_DAYS: int = 7 # Increased historical data lookback for 15m timeframe
SIGNAL_TRACKING_TIMEFRAME: str = '15m' # Timeframe for signal tracking and target updates (Changed to 15m)
SIGNAL_TRACKING_LOOKBACK_DAYS: int = 3   # Increased historical data lookback in days for signal tracking

# --- New Constants for Multi-Timeframe Confirmation ---
CONFIRMATION_TIMEFRAME: str = '30m' # Larger timeframe for trend confirmation (Changed to 30m)
CONFIRMATION_LOOKBACK_DAYS: int = 14 # Historical data lookback for confirmation timeframe (Increased for 30m)

# --- Parameters for Improved Entry Point ---
ENTRY_POINT_EMA_PROXIMITY_PCT: float = 0.002 # Price must be within this % of signal timeframe EMA_SHORT (Increased tolerance slightly)
ENTRY_POINT_RECENT_CANDLE_LOOKBACK: int = 2 # Look back this many candles on signal timeframe for bullish sign (Reduced lookback)

# =============================================================================
# --- Indicator Parameters (Adjusted for 15m Signal and 30m Confirmation) ---
# =============================================================================
RSI_PERIOD: int = 14 # Standard RSI period
RSI_OVERSOLD: int = 30
RSI_OVERBOUGHT: int = 70
EMA_SHORT_PERIOD: int = 13 # Adjusted for 15m
EMA_LONG_PERIOD: int = 34 # Adjusted for 15m
VWMA_PERIOD: int = 21 # Adjusted for 15m
SWING_ORDER: int = 3 # Not used in current strategy logic
FIB_LEVELS_TO_CHECK: List[float] = [0.382, 0.5, 0.618] # Not used in current strategy logic
FIB_TOLERANCE: float = 0.005 # Not used in current strategy logic
LOOKBACK_FOR_SWINGS: int = 50 # Not used in current strategy logic
ENTRY_ATR_PERIOD: int = 14 # Adjusted for 15m
ENTRY_ATR_MULTIPLIER: float = 1.75 # ATR Multiplier for initial target (Adjusted slightly)
BOLLINGER_WINDOW: int = 20 # Standard Bollinger period
BOLLINGER_STD_DEV: int = 2 # Standard Bollinger std dev
MACD_FAST: int = 12 # Standard MACD fast period
MACD_SLOW: int = 26 # Standard MACD slow period
MACD_SIGNAL: int = 9 # Standard MACD signal period
ADX_PERIOD: int = 14 # Standard ADX period
SUPERTREND_PERIOD: int = 10 # Standard Supertrend period
SUPERTREND_MULTIPLIER: float = 3.0 # Adjusted Supertrend multiplier slightly

# --- Parameters for Dynamic Target Update ---
DYNAMIC_TARGET_APPROACH_PCT: float = 0.003 # Percentage proximity to target to trigger re-evaluation (e.g., 0.3%) (Increased slightly)
DYNAMIC_TARGET_EXTENSION_ATR_MULTIPLIER: float = 1.0 # ATR multiplier for extending the target (Increased)
MAX_DYNAMIC_TARGET_UPDATES: int = 3 # Maximum number of times a target can be dynamically updated for a single signal (Increased)
MIN_ADX_FOR_DYNAMIC_UPDATE: int = 25 # Minimum ADX value to consider dynamic target update (Increased slightly)

MIN_PROFIT_MARGIN_PCT: float = 1.5 # Increased minimum profit margin
MIN_VOLUME_15M_USDT: float = 500000.0 # Increased minimum volume check (using 15m data now)

RECENT_EMA_CROSS_LOOKBACK: int = 3 # Adjusted for 15m
MIN_ADX_TREND_STRENGTH: int = 25 # Increased minimum ADX trend strength for essential condition
MACD_HIST_INCREASE_CANDLES: int = 2 # Reduced lookback for MACD Hist increase
OBV_INCREASE_CANDLES: int = 2 # Reduced lookback for OBV increase
# =============================================================================

# Global variables
conn: Optional[psycopg2.extensions.connection] = None
cur: Optional[psycopg2.extensions.cursor] = None
client: Optional[Client] = None
ticker_data: Dict[str, float] = {}

# ---------------------- Binance Client Setup ----------------------
try:
    logger.info("â„¹ï¸ [Binance] ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ Binance...")
    client = Client(API_KEY, API_SECRET)
    client.ping()
    server_time = client.get_server_time()
    logger.info(f"âœ… [Binance] ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ Binance Ø¨Ù†Ø¬Ø§Ø­. ÙˆÙ‚Øª Ø§Ù„Ø®Ø§Ø¯Ù…: {datetime.fromtimestamp(server_time['serverTime']/1000)}")
except (BinanceRequestException, BinanceAPIException) as binance_err:
     logger.critical(f"âŒ [Binance] Ø®Ø·Ø£ ÙÙŠ Binance API/Ø§Ù„Ø·Ù„Ø¨: {binance_err}")
     exit(1)
except Exception as e:
    logger.critical(f"âŒ [Binance] ÙØ´Ù„ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ Binance: {e}", exc_info=True)
    exit(1)

# ---------------------- Additional Indicator Functions ----------------------
def get_fear_greed_index() -> str:
    classification_translation_ar = {
        "Extreme Fear": "Ø®ÙˆÙ Ø´Ø¯ÙŠØ¯", "Fear": "Ø®ÙˆÙ", "Neutral": "Ù…Ø­Ø§ÙŠØ¯",
        "Greed": "Ø¬Ø´Ø¹", "Extreme Greed": "Ø¬Ø´Ø¹ Ø´Ø¯ÙŠØ¯",
    }
    url = "https://api.alternative.me/fng/"
    logger.debug(f"â„¹ï¸ [Indicators] Ø¬Ù„Ø¨ Ù…Ø¤Ø´Ø± Ø§Ù„Ø®ÙˆÙ ÙˆØ§Ù„Ø¬Ø´Ø¹ Ù…Ù† {url}...")
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        data = response.json()
        value = int(data["data"][0]["value"])
        classification_en = data["data"][0]["value_classification"]
        classification_ar = classification_translation_ar.get(classification_en, classification_en)
        logger.debug(f"âœ… [Indicators] Ù…Ø¤Ø´Ø± Ø§Ù„Ø®ÙˆÙ ÙˆØ§Ù„Ø¬Ø´Ø¹: {value} ({classification_ar})")
        return f"{value} ({classification_ar})"
    except requests.exceptions.RequestException as e:
         logger.error(f"âŒ [Indicators] Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ù…Ø¤Ø´Ø± Ø§Ù„Ø®ÙˆÙ ÙˆØ§Ù„Ø¬Ø´Ø¹: {e}")
         return "N/A (Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ©)"
    except Exception as e:
        logger.error(f"âŒ [Indicators] Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ù…Ø¤Ø´Ø± Ø§Ù„Ø®ÙˆÙ ÙˆØ§Ù„Ø¬Ø´Ø¹: {e}", exc_info=True)
        return "N/A (Ø®Ø·Ø£)"

def fetch_historical_data(symbol: str, interval: str, days: int) -> Optional[pd.DataFrame]:
    """ÙŠØ¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ø²ÙˆØ¬ Ù…Ø¹ÙŠÙ† ÙˆØ¥Ø·Ø§Ø± Ø²Ù…Ù†ÙŠ."""
    if not client:
        logger.error(f"âŒ [Data] Ø¹Ù…ÙŠÙ„ Binance ØºÙŠØ± Ù…Ù‡ÙŠØ£ Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol}.")
        return None
    try:
        start_dt = datetime.utcnow() - timedelta(days=days + 1)
        start_str = start_dt.strftime("%Y-%m-%d %H:%M:%S")
        logger.debug(f"â„¹ï¸ [Data] Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª {interval} Ù„Ù€ {symbol} Ù…Ù†Ø° {start_str} (Ø­Ø¯ Ø£Ù‚ØµÙ‰ 1000 Ø´Ù…Ø¹Ø©)...")
        klines = client.get_historical_klines(symbol, interval, start_str, limit=1000)
        if not klines:
            logger.warning(f"âš ï¸ [Data] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ§Ø±ÙŠØ®ÙŠØ© ({interval}) Ù„Ù€ {symbol}.")
            return None
        df = pd.DataFrame(klines, columns=[
            'timestamp', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_volume', 'trades', 'taker_buy_base', 'taker_buy_quote', 'ignore'
        ])
        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        df = df[numeric_cols]
        df.dropna(subset=numeric_cols, inplace=True)
        if df.empty:
            logger.warning(f"âš ï¸ [Data] DataFrame Ù„Ù€ {symbol} ÙØ§Ø±Øº Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ù‚ÙŠÙ… NaN.")
            return None
        logger.debug(f"âœ… [Data] ØªÙ… Ø¬Ù„Ø¨ {len(df)} Ø´Ù…Ø¹Ø© ({interval}) Ù„Ù€ {symbol}.")
        return df
    except (BinanceAPIException, BinanceRequestException) as binance_err:
         logger.error(f"âŒ [Data] Ø®Ø·Ø£ Binance Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol}: {binance_err}")
         return None
    except Exception as e:
        logger.error(f"âŒ [Data] Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ù€ {symbol}: {e}", exc_info=True)
        return None

def calculate_ema(series: pd.Series, span: int) -> pd.Series:
    """ÙŠØ­Ø³Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ØªØ­Ø±Ùƒ Ø§Ù„Ø£Ø³ÙŠ (EMA)."""
    if series is None or series.isnull().all() or len(series) < span:
        logger.debug(f"âš ï¸ [Indicators] Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ø­Ø³Ø§Ø¨ EMA span={span}.")
        return pd.Series(index=series.index if series is not None else None, dtype=float)
    ema = series.ewm(span=span, adjust=False).mean()
    logger.debug(f"âœ… [Indicators] ØªÙ… Ø­Ø³Ø§Ø¨ EMA span={span}.")
    return ema

def calculate_vwma(df: pd.DataFrame, period: int) -> pd.Series:
    """ÙŠØ­Ø³Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ØªØ­Ø±Ùƒ Ø§Ù„Ù…Ø±Ø¬Ø­ Ø¨Ø§Ù„Ø­Ø¬Ù… (VWMA)."""
    df_calc = df.copy()
    if not all(col in df_calc.columns for col in ['close', 'volume']) or df_calc[['close', 'volume']].isnull().all().any() or len(df_calc) < period:
        logger.debug(f"âš ï¸ [Indicators] Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ø­Ø³Ø§Ø¨ VWMA period={period}.")
        return pd.Series(index=df_calc.index, dtype=float)
    df_calc['price_volume'] = df_calc['close'] * df_calc['volume']
    rolling_price_volume_sum = df_calc['price_volume'].rolling(window=period, min_periods=period).sum()
    rolling_volume_sum = df_calc['volume'].rolling(window=period, min_periods=period).sum()
    vwma = rolling_price_volume_sum / rolling_volume_sum.replace(0, np.nan)
    logger.debug(f"âœ… [Indicators] ØªÙ… Ø­Ø³Ø§Ø¨ VWMA period={period}.")
    return vwma

def get_btc_trend_4h() -> str:
    """ÙŠØ­Ø³Ø¨ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± 4 Ø³Ø§Ø¹Ø§Øª."""
    logger.debug("â„¹ï¸ [Indicators] Ø­Ø³Ø§Ø¨ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± 4 Ø³Ø§Ø¹Ø§Øª...")
    try:
        df = fetch_historical_data("BTCUSDT", interval=Client.KLINE_INTERVAL_4HOUR, days=10)
        if df is None or df.empty or len(df) < 51: # Ensure enough data for EMA50
            logger.warning("âš ï¸ [Indicators] Ø¨ÙŠØ§Ù†Ø§Øª BTC/USDT 4H ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§ØªØ¬Ø§Ù‡.")
            return "N/A (Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©)"
        df['close'] = pd.to_numeric(df['close'], errors='coerce')
        df.dropna(subset=['close'], inplace=True)
        if len(df) < 50:
            logger.warning("âš ï¸ [Indicators] Ø¨ÙŠØ§Ù†Ø§Øª BTC/USDT 4H ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© NaN.")
            return "N/A (Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©)"
        ema20 = calculate_ema(df['close'], 20).iloc[-1]
        ema50 = calculate_ema(df['close'], 50).iloc[-1]
        current_close = df['close'].iloc[-1]
        if pd.isna(ema20) or pd.isna(ema50) or pd.isna(current_close):
            logger.warning("âš ï¸ [Indicators] Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ EMA20/EMA50 Ù„Ù€ BTC/USDT 4H.")
            return "N/A (Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­Ø³Ø§Ø¨)"
        diff_ema20_pct = abs(current_close - ema20) / current_close if current_close > 0 else 0
        if current_close > ema20 > ema50: trend = "ØµØ¹ÙˆØ¯ ğŸ“ˆ"
        elif current_close < ema20 < ema50: trend = "Ù‡Ø¨ÙˆØ· ğŸ“‰"
        elif diff_ema20_pct < 0.005: trend = "Ø§Ø³ØªÙ‚Ø±Ø§Ø± ğŸ”„" # Sideways
        else: trend = "ØªØ°Ø¨Ø°Ø¨ ğŸ”€" # Volatile
        logger.debug(f"âœ… [Indicators] Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† 4H: {trend}")
        return trend
    except Exception as e:
        logger.error(f"âŒ [Indicators] Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø­Ø³Ø§Ø¨ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± 4 Ø³Ø§Ø¹Ø§Øª: {e}", exc_info=True)
        return "N/A (Ø®Ø·Ø£)"

# ---------------------- Database Connection Setup ----------------------
def init_db(retries: int = 5, delay: int = 5) -> None:
    """ØªÙ‡ÙŠØ¦Ø© Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©."""
    global conn, cur
    logger.info("[DB] Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    for attempt in range(retries):
        try:
            logger.info(f"[DB] Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ (Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}/{retries})..." )
            conn = psycopg2.connect(DB_URL, connect_timeout=10, cursor_factory=RealDictCursor)
            conn.autocommit = False
            cur = conn.cursor()
            logger.info("âœ… [DB] ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")

            logger.info("[DB] Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†/Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ 'signals'...")
            cur.execute("""
                CREATE TABLE IF NOT EXISTS signals (
                    id SERIAL PRIMARY KEY,
                    symbol TEXT NOT NULL,
                    entry_price DOUBLE PRECISION NOT NULL,
                    initial_target DOUBLE PRECISION NOT NULL,
                    initial_stop_loss DOUBLE PRECISION DEFAULT 0.0,
                    current_target DOUBLE PRECISION NOT NULL,
                    current_stop_loss DOUBLE PRECISION DEFAULT 0.0,
                    r2_score DOUBLE PRECISION,
                    volume_15m DOUBLE PRECISION,
                    achieved_target BOOLEAN DEFAULT FALSE,
                    hit_stop_loss BOOLEAN DEFAULT FALSE,
                    closing_price DOUBLE PRECISION,
                    closed_at TIMESTAMP,
                    sent_at TIMESTAMP DEFAULT NOW(),
                    profit_percentage DOUBLE PRECISION,
                    profitable_stop_loss BOOLEAN DEFAULT FALSE,
                    is_trailing_active BOOLEAN DEFAULT FALSE,
                    strategy_name TEXT,
                    signal_details JSONB,
                    last_trailing_update_price DOUBLE PRECISION,
                    time_to_target_seconds BIGINT,
                    dynamic_updates_count INTEGER DEFAULT 0
                );""")
            conn.commit()
            logger.info("âœ… [DB] ØªÙ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†/Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ 'signals'.")

            columns_to_add = {
                "time_to_target_seconds": "BIGINT",
                "dynamic_updates_count": "INTEGER DEFAULT 0"
            }
            cur.execute("SELECT column_name FROM information_schema.columns WHERE table_name = 'signals' AND table_schema = 'public';")
            existing_columns = {row['column_name'] for row in cur.fetchall()}

            for col_name, col_type in columns_to_add.items():
                if col_name not in existing_columns:
                    logger.info(f"[DB] Ø¥Ø¶Ø§ÙØ© Ø¹Ù…ÙˆØ¯ '{col_name}' Ø¥Ù„Ù‰ Ø¬Ø¯ÙˆÙ„ 'signals'...")
                    cur.execute(sql.SQL(f"ALTER TABLE signals ADD COLUMN IF NOT EXISTS {col_name} {col_type};"))
                    conn.commit()
                    logger.info(f"âœ… [DB] ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø¹Ù…ÙˆØ¯ '{col_name}'.")

            logger.info("[DB] Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†/Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ 'market_dominance'...")
            cur.execute("""
                CREATE TABLE IF NOT EXISTS market_dominance (
                    id SERIAL PRIMARY KEY,
                    recorded_at TIMESTAMP DEFAULT NOW(),
                    btc_dominance DOUBLE PRECISION,
                    eth_dominance DOUBLE PRECISION
                );
            """)
            conn.commit()
            logger.info("âœ… [DB] ØªÙ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†/Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ 'market_dominance'.")
            logger.info("âœ… [DB] ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ø§Ø¬Ø­Ø©.")
            return
        except OperationalError as op_err:
            logger.error(f"âŒ [DB] Ø®Ø·Ø£ ØªØ´ØºÙŠÙ„ÙŠ (Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}): {op_err}")
            if conn: conn.rollback()
            if attempt == retries - 1: raise op_err
            time.sleep(delay)
        except Exception as e:
            logger.critical(f"âŒ [DB] ÙØ´Ù„ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}): {e}", exc_info=True)
            if conn: conn.rollback()
            if attempt == retries - 1: raise e
            time.sleep(delay)
    logger.critical("âŒ [DB] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©.")
    exit(1)

def check_db_connection() -> bool:
    """ÙŠØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ø´Ø·Ù‹Ø§ ÙˆÙŠØ¹ÙŠØ¯ ØªÙ‡ÙŠØ¦ØªÙ‡ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±."""
    global conn, cur
    try:
        if conn is None or conn.closed != 0:
            logger.warning("âš ï¸ [DB] Ø§Ù„Ø§ØªØµØ§Ù„ Ù…ØºÙ„Ù‚/ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙ‡ÙŠØ¦Ø©...")
            init_db()
            return True
        else:
             with conn.cursor() as check_cur: # Use a temporary cursor
                  check_cur.execute("SELECT 1;")
                  check_cur.fetchone()
             return True
    except (OperationalError, InterfaceError) as e:
        logger.error(f"âŒ [DB] ÙÙ‚Ø¯Ø§Ù† Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ({e}). Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙ‡ÙŠØ¦Ø©...")
        try: init_db(); return True
        except Exception as recon_err: logger.error(f"âŒ [DB] ÙØ´Ù„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§ØªØµØ§Ù„: {recon_err}"); return False
    except Exception as e:
        logger.error(f"âŒ [DB] Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„: {e}", exc_info=True)
        try: init_db(); return True
        except Exception as recon_err: logger.error(f"âŒ [DB] ÙØ´Ù„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§ØªØµØ§Ù„: {recon_err}"); return False

def convert_np_values(obj: Any) -> Any:
    """ÙŠØ­ÙˆÙ„ Ù‚ÙŠÙ… NumPy Ø¥Ù„Ù‰ Ø£Ù†ÙˆØ§Ø¹ Python Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„Ù„ØªØ³Ù„Ø³Ù„."""
    if isinstance(obj, dict): return {k: convert_np_values(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [convert_np_values(item) for item in obj]
    elif isinstance(obj, np.ndarray): return obj.tolist()
    elif isinstance(obj, (np.integer, np.int_)): return int(obj)
    elif isinstance(obj, (np.floating, np.float64)): return float(obj)
    elif isinstance(obj, (np.bool_)): return bool(obj)
    elif pd.isna(obj): return None
    else: return obj

# ---------------------- Reading and Validating Symbols List ----------------------
def get_crypto_symbols(filename: str = 'crypto_list.txt') -> List[str]:
    """ÙŠÙ‚Ø±Ø£ ÙˆÙŠØªØ­Ù‚Ù‚ Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ø±Ù…ÙˆØ² Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø´ÙØ±Ø©."""
    raw_symbols: List[str] = []
    logger.info(f"â„¹ï¸ [Data] Ù‚Ø±Ø§Ø¡Ø© Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ù…Ù† '{filename}'...")
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, filename)
        if not os.path.exists(file_path): file_path = os.path.abspath(filename)
        if not os.path.exists(file_path): logger.error(f"âŒ [Data] Ø§Ù„Ù…Ù„Ù '{filename}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯."); return []
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_symbols = [f"{line.strip().upper().replace('USDT', '')}USDT" for line in f if line.strip() and not line.startswith('#')]
        raw_symbols = sorted(list(set(raw_symbols)))
        logger.info(f"â„¹ï¸ [Data] ØªÙ… Ù‚Ø±Ø§Ø¡Ø© {len(raw_symbols)} Ø±Ù…Ø²Ù‹Ø§ Ù…Ù† '{file_path}'.")
    except Exception as e: logger.error(f"âŒ [Data] Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù '{filename}': {e}", exc_info=True); return []
    if not raw_symbols: logger.warning("âš ï¸ [Data] Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ù…ÙˆØ² ÙØ§Ø±ØºØ©."); return []

    if not client: logger.error("âŒ [Data Validation] Ø¹Ù…ÙŠÙ„ Binance ØºÙŠØ± Ù…Ù‡ÙŠØ£."); return raw_symbols
    try:
        logger.info("â„¹ï¸ [Data Validation] Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ² Ù…Ù† Binance API...")
        exchange_info = client.get_exchange_info()
        valid_trading_usdt_symbols = {s['symbol'] for s in exchange_info['symbols'] if s.get('quoteAsset') == 'USDT' and s.get('status') == 'TRADING' and s.get('isSpotTradingAllowed') is True}
        validated_symbols = [symbol for symbol in raw_symbols if symbol in valid_trading_usdt_symbols]
        removed_count = len(raw_symbols) - len(validated_symbols)
        if removed_count > 0: logger.warning(f"âš ï¸ [Data Validation] ØªÙ… Ø¥Ø²Ø§Ù„Ø© {removed_count} Ø±Ù…Ø²Ù‹Ø§ ØºÙŠØ± ØµØ§Ù„Ø­.")
        logger.info(f"âœ… [Data Validation] Ø§Ø³ØªØ®Ø¯Ø§Ù… {len(validated_symbols)} Ø±Ù…Ø²Ù‹Ø§ ØµØ§Ù„Ø­Ù‹Ø§.")
        return validated_symbols
    except Exception as api_err:
         logger.error(f"âŒ [Data Validation] Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø±Ù…ÙˆØ² Binance: {api_err}", exc_info=True)
         return raw_symbols

# ---------------------- WebSocket Management for Ticker Prices ----------------------
def handle_ticker_message(msg: Union[List[Dict[str, Any]], Dict[str, Any]]) -> None:
    """ÙŠØ¹Ø§Ù„Ø¬ Ø±Ø³Ø§Ø¦Ù„ WebSocket Ù„Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„ÙÙˆØ±ÙŠØ©."""
    global ticker_data
    try:
        if isinstance(msg, list):
            for ticker_item in msg:
                symbol = ticker_item.get('s')
                price_str = ticker_item.get('c')
                if symbol and 'USDT' in symbol and price_str:
                    try: ticker_data[symbol] = float(price_str)
                    except ValueError: logger.warning(f"âš ï¸ [WS] Ø³Ø¹Ø± ØºÙŠØ± ØµØ§Ù„Ø­ Ù„Ù€ {symbol}: '{price_str}'")
        elif isinstance(msg, dict) and msg.get('stream') and msg.get('data'): # Handle combined streams format
            for ticker_item in msg.get('data', []):
                symbol = ticker_item.get('s')
                price_str = ticker_item.get('c')
                if symbol and 'USDT' in symbol and price_str:
                    try: ticker_data[symbol] = float(price_str)
                    except ValueError: logger.warning(f"âš ï¸ [WS] Ø³Ø¹Ø± ØºÙŠØ± ØµØ§Ù„Ø­ Ù„Ù€ {symbol} ÙÙŠ Ø§Ù„Ø¨Ø« Ø§Ù„Ù…Ø¯Ù…Ø¬: '{price_str}'")
        elif isinstance(msg, dict) and msg.get('e') == 'error':
            logger.error(f"âŒ [WS] Ø®Ø·Ø£ Ù…Ù† WebSocket: {msg.get('m', 'Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙØ§ØµÙŠÙ„')}")
    except Exception as e: logger.error(f"âŒ [WS] Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªÙŠÙƒØ±: {e}", exc_info=True)

def run_ticker_socket_manager() -> None:
    """ÙŠØ¯ÙŠØ± Ø§ØªØµØ§Ù„ WebSocket Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„ØªÙŠÙƒØ±."""
    while True:
        try:
            logger.info("â„¹ï¸ [WS] Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ù…Ø¯ÙŠØ± WebSocket Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„ØªÙŠÙƒØ±...")
            twm = ThreadedWebsocketManager(api_key=API_KEY, api_secret=API_SECRET)
            twm.start()
            stream_name = twm.start_miniticker_socket(callback=handle_ticker_message)
            logger.info(f"âœ… [WS] ØªÙ… Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø¨Ø« WebSocket: {stream_name}")
            twm.join()
            logger.warning("âš ï¸ [WS] ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ù…Ø¯ÙŠØ± WebSocket. Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ØºÙŠÙ„...")
        except Exception as e: logger.error(f"âŒ [WS] Ø®Ø·Ø£ ÙØ§Ø¯Ø­ ÙÙŠ Ù…Ø¯ÙŠØ± WebSocket: {e}. Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ØºÙŠÙ„ ÙÙŠ 15 Ø«Ø§Ù†ÙŠØ©...", exc_info=True)
        time.sleep(15)

# ---------------------- Technical Indicator Functions ----------------------
def calculate_rsi_indicator(df: pd.DataFrame, period: int = RSI_PERIOD) -> pd.DataFrame:
    """ÙŠØ­Ø³Ø¨ Ù…Ø¤Ø´Ø± Ø§Ù„Ù‚ÙˆØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ© (RSI)."""
    df = df.copy()
    if 'close' not in df.columns or df['close'].isnull().all() or len(df) < period:
        logger.debug(f"âš ï¸ [Indicators] Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ø­Ø³Ø§Ø¨ RSI period={period}.")
        df['rsi'] = np.nan; return df
    delta = df['close'].diff()
    gain = delta.clip(lower=0).ewm(com=period - 1, adjust=False).mean()
    loss = -delta.clip(upper=0).ewm(com=period - 1, adjust=False).mean()
    rs = gain / loss.replace(0, np.nan)
    df['rsi'] = (100 - (100 / (1 + rs))).ffill().fillna(50)
    logger.debug(f"âœ… [Indicators] ØªÙ… Ø­Ø³Ø§Ø¨ RSI period={period}.")
    return df

def calculate_atr_indicator(df: pd.DataFrame, period: int = ENTRY_ATR_PERIOD) -> pd.DataFrame:
    """ÙŠØ­Ø³Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ (ATR)."""
    df = df.copy()
    if not all(col in df.columns for col in ['high', 'low', 'close']) or df[['high', 'low', 'close']].isnull().all().any() or len(df) < period + 1:
        logger.debug(f"âš ï¸ [Indicators] Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ø­Ø³Ø§Ø¨ ATR period={period}.")
        df['atr'] = np.nan; return df
    high_low = df['high'] - df['low']
    high_close_prev = (df['high'] - df['close'].shift(1)).abs()
    low_close_prev = (df['low'] - df['close'].shift(1)).abs()
    tr = pd.concat([high_low, high_close_prev, low_close_prev], axis=1).max(axis=1, skipna=False)
    df['atr'] = tr.ewm(span=period, adjust=False).mean()
    logger.debug(f"âœ… [Indicators] ØªÙ… Ø­Ø³Ø§Ø¨ ATR period={period}.")
    return df

def calculate_bollinger_bands(df: pd.DataFrame, window: int = BOLLINGER_WINDOW, num_std: int = BOLLINGER_STD_DEV) -> pd.DataFrame:
    """ÙŠØ­Ø³Ø¨ Ù†Ø·Ø§Ù‚Ø§Øª Ø¨ÙˆÙ„ÙŠÙ†Ø¬Ø± (Bollinger Bands)."""
    df = df.copy()
    if 'close' not in df.columns or df['close'].isnull().all() or len(df) < window:
        logger.debug(f"âš ï¸ [Indicators] Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ø­Ø³Ø§Ø¨ Bollinger Bands window={window}.")
        df['bb_middle'] = np.nan; df['bb_upper'] = np.nan; df['bb_lower'] = np.nan; return df
    df['bb_middle'] = df['close'].rolling(window=window).mean()
    df['bb_std'] = df['close'].rolling(window=window).std()
    df['bb_upper'] = df['bb_middle'] + num_std * df['bb_std']
    df['bb_lower'] = df['bb_middle'] - num_std * df['bb_std']
    logger.debug(f"âœ… [Indicators] ØªÙ… Ø­Ø³Ø§Ø¨ Bollinger Bands window={window}.")
    return df

def calculate_macd(df: pd.DataFrame, fast: int = MACD_FAST, slow: int = MACD_SLOW, signal: int = MACD_SIGNAL) -> pd.DataFrame:
    """ÙŠØ­Ø³Ø¨ Ù…Ø¤Ø´Ø± MACD."""
    df = df.copy()
    min_len = max(fast, slow, signal)
    if 'close' not in df.columns or df['close'].isnull().all() or len(df) < min_len:
        logger.debug(f"âš ï¸ [Indicators] Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ø­Ø³Ø§Ø¨ MACD fast={fast}, slow={slow}, signal={signal}.")
        df['macd'] = np.nan; df['macd_signal'] = np.nan; df['macd_hist'] = np.nan; return df
    ema_fast = calculate_ema(df['close'], fast)
    ema_slow = calculate_ema(df['close'], slow)
    df['macd'] = ema_fast - ema_slow
    df['macd_signal'] = calculate_ema(df['macd'], signal)
    df['macd_hist'] = df['macd'] - df['macd_signal']
    logger.debug(f"âœ… [Indicators] ØªÙ… Ø­Ø³Ø§Ø¨ MACD fast={fast}, slow={slow}, signal={signal}.")
    return df

def calculate_adx(df: pd.DataFrame, period: int = ADX_PERIOD) -> pd.DataFrame:
    """ÙŠØ­Ø³Ø¨ Ù…Ø¤Ø´Ø± ADX."""
    df_calc = df.copy() # Work on a copy
    if not all(col in df_calc.columns for col in ['high', 'low', 'close']) or df_calc[['high', 'low', 'close']].isnull().all().any() or len(df_calc) < period * 2:
        logger.debug(f"âš ï¸ [Indicators] Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ø­Ø³Ø§Ø¨ ADX period={period}.")
        df_calc['adx'] = np.nan; df_calc['di_plus'] = np.nan; df_calc['di_minus'] = np.nan; return df_calc
    df_calc['tr'] = pd.concat([df_calc['high'] - df_calc['low'], abs(df_calc['high'] - df_calc['close'].shift(1)), abs(df_calc['low'] - df_calc['close'].shift(1))], axis=1).max(axis=1, skipna=False)
    df_calc['up_move'] = df_calc['high'] - df_calc['high'].shift(1)
    df_calc['down_move'] = df_calc['low'].shift(1) - df_calc['low']
    df_calc['+dm'] = np.where((df_calc['up_move'] > df_calc['down_move']) & (df_calc['up_move'] > 0), df_calc['up_move'], 0)
    df_calc['-dm'] = np.where((df_calc['down_move'] > df_calc['up_move']) & (df_calc['down_move'] > 0), df_calc['down_move'], 0)
    alpha = 1 / period
    df_calc['tr_smooth'] = df_calc['tr'].ewm(alpha=alpha, adjust=False).mean()
    df_calc['+dm_smooth'] = df_calc['+dm'].ewm(alpha=alpha, adjust=False).mean()
    df_calc['-dm_smooth'] = df_calc['-dm'].ewm(alpha=alpha, adjust=False).mean()
    df_calc['di_plus'] = np.where(df_calc['tr_smooth'] > 0, 100 * (df_calc['+dm_smooth'] / df_calc['tr_smooth']), 0)
    df_calc['di_minus'] = np.where(df_calc['tr_smooth'] > 0, 100 * (df_calc['-dm_smooth'] / df_calc['tr_smooth']), 0)
    di_sum = df_calc['di_plus'] + df_calc['di_minus']
    df_calc['dx'] = np.where(di_sum > 0, 100 * abs(df_calc['di_plus'] - df_calc['di_minus']) / di_sum, 0)
    df_calc['adx'] = df_calc['dx'].ewm(alpha=alpha, adjust=False).mean()
    logger.debug(f"âœ… [Indicators] ØªÙ… Ø­Ø³Ø§Ø¨ ADX period={period}.")
    return df_calc[['adx', 'di_plus', 'di_minus']]

def calculate_vwap(df: pd.DataFrame) -> pd.DataFrame:
    """ÙŠØ­Ø³Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ù…Ø±Ø¬Ø­ Ø¨Ø§Ù„Ø­Ø¬Ù… (VWAP)."""
    df = df.copy()
    if not all(col in df.columns for col in ['high', 'low', 'close', 'volume']) or df[['high', 'low', 'close', 'volume']].isnull().all().any() or not isinstance(df.index, pd.DatetimeIndex):
        logger.debug("âš ï¸ [Indicators] Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ø­Ø³Ø§Ø¨ VWAP.")
        df['vwap'] = np.nan; return df
    try:
        df['date'] = df.index.date
        df['typical_price'] = (df['high'] + df['low'] + df['close']) / 3
        df['tp_vol'] = df['typical_price'] * df['volume']
        df['cum_tp_vol'] = df.groupby('date')['tp_vol'].cumsum()
        df['cum_volume'] = df.groupby('date')['volume'].cumsum()
        df['vwap'] = np.where(df['cum_volume'] > 0, df['cum_tp_vol'] / df['cum_volume'], np.nan)
        df['vwap'] = df['vwap'].bfill()
        df.drop(columns=['date', 'typical_price', 'tp_vol', 'cum_tp_vol', 'cum_volume'], inplace=True, errors='ignore')
        logger.debug("âœ… [Indicators] ØªÙ… Ø­Ø³Ø§Ø¨ VWAP.")
    except Exception as e: logger.error(f"âŒ [Indicator VWAP] Ø®Ø·Ø£: {e}"); df['vwap'] = np.nan
    return df

def calculate_obv(df: pd.DataFrame) -> pd.DataFrame:
    """ÙŠØ­Ø³Ø¨ Ø­Ø¬Ù… Ø§Ù„ØªÙˆØ§Ø²Ù† (OBV)."""
    df = df.copy()
    if not all(col in df.columns for col in ['close', 'volume']) or df[['close', 'volume']].isnull().all().any() or not pd.api.types.is_numeric_dtype(df['close']) or not pd.api.types.is_numeric_dtype(df['volume']):
        logger.debug("âš ï¸ [Indicators] Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ø­Ø³Ø§Ø¨ OBV.")
        df['obv'] = np.nan; return df
    obv = np.zeros(len(df), dtype=np.float64)
    close = df['close'].values; volume = df['volume'].values
    close_diff = df['close'].diff().values
    for i in range(1, len(df)):
        if np.isnan(close[i]) or np.isnan(volume[i]) or np.isnan(close_diff[i]): obv[i] = obv[i-1]; continue
        if close_diff[i] > 0: obv[i] = obv[i-1] + volume[i]
        elif close_diff[i] < 0: obv[i] = obv[i-1] - volume[i]
        else: obv[i] = obv[i-1]
    df['obv'] = obv
    logger.debug("âœ… [Indicators] ØªÙ… Ø­Ø³Ø§Ø¨ OBV.")
    return df

def calculate_supertrend(df: pd.DataFrame, period: int = SUPERTREND_PERIOD, multiplier: float = SUPERTREND_MULTIPLIER) -> pd.DataFrame:
    """ÙŠØ­Ø³Ø¨ Ù…Ø¤Ø´Ø± Supertrend."""
    df_st = df.copy()
    if not all(col in df_st.columns for col in ['high', 'low', 'close']) or df_st[['high', 'low', 'close']].isnull().all().any():
        logger.debug(f"âš ï¸ [Indicators] Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ø­Ø³Ø§Ø¨ Supertrend period={period}, multiplier={multiplier}.")
        df_st['supertrend'] = np.nan; df_st['supertrend_trend'] = 0; return df_st
    df_st = calculate_atr_indicator(df_st, period=period) # Use Supertrend's own period for ATR
    if 'atr' not in df_st.columns or df_st['atr'].isnull().all() or len(df_st) < period:
        logger.debug(f"âš ï¸ [Indicators] Ø¨ÙŠØ§Ù†Ø§Øª ATR ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ø­Ø³Ø§Ø¨ Supertrend period={period}.")
        df_st['supertrend'] = np.nan; df_st['supertrend_trend'] = 0; return df_st
    hl2 = (df_st['high'] + df_st['low']) / 2
    df_st['basic_ub'] = hl2 + multiplier * df_st['atr']
    df_st['basic_lb'] = hl2 - multiplier * df_st['atr']
    df_st['final_ub'] = 0.0; df_st['final_lb'] = 0.0
    df_st['supertrend'] = np.nan; df_st['supertrend_trend'] = 0
    close = df_st['close'].values; basic_ub = df_st['basic_ub'].values; basic_lb = df_st['basic_lb'].values
    final_ub = df_st['final_ub'].values; final_lb = df_st['final_lb'].values
    st = df_st['supertrend'].values; st_trend = df_st['supertrend_trend'].values
    for i in range(1, len(df_st)):
        if pd.isna(basic_ub[i]) or pd.isna(basic_lb[i]) or pd.isna(close[i]):
            final_ub[i] = final_ub[i-1]; final_lb[i] = final_lb[i-1]; st[i] = st[i-1]; st_trend[i] = st_trend[i-1]; continue
        if basic_ub[i] < final_ub[i-1] or close[i-1] > final_ub[i-1]: final_ub[i] = basic_ub[i]
        else: final_ub[i] = final_ub[i-1]
        if basic_lb[i] > final_lb[i-1] or close[i-1] < final_lb[i-1]: final_lb[i] = basic_lb[i]
        else: final_lb[i] = final_lb[i-1]
        if st_trend[i-1] == -1:
            if close[i] <= final_ub[i]: st[i] = final_ub[i]; st_trend[i] = -1
            else: st[i] = final_lb[i]; st_trend[i] = 1
        elif st_trend[i-1] == 1:
            if close[i] >= final_lb[i]: st[i] = final_lb[i]; st_trend[i] = 1
            else: st[i] = final_ub[i]; st_trend[i] = -1
        else: # Initial state
            if close[i] > final_ub[i]: st[i] = final_lb[i]; st_trend[i] = 1
            elif close[i] < final_lb[i]: st[i] = final_ub[i]; st_trend[i] = -1
            else: st[i] = np.nan; st_trend[i] = 0 # Or use previous if available
    df_st['final_ub'] = final_ub; df_st['final_lb'] = final_lb
    df_st['supertrend'] = st; df_st['supertrend_trend'] = st_trend
    df_st.drop(columns=['basic_ub', 'basic_lb', 'final_ub', 'final_lb'], inplace=True, errors='ignore')
    logger.debug(f"âœ… [Indicators] ØªÙ… Ø­Ø³Ø§Ø¨ Supertrend period={period}, multiplier={multiplier}.")
    return df_st

# ---------------------- Candlestick Patterns ----------------------
def is_bullish_candle(row: pd.Series) -> bool:
    """ÙŠØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø´Ù…Ø¹Ø© ØµØ¹ÙˆØ¯ÙŠØ©."""
    o, c = row.get('open'), row.get('close')
    return pd.notna(o) and pd.notna(c) and c > o

def is_hammer(row: pd.Series) -> int:
    """ÙŠØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø´Ù…Ø¹Ø© Ù†Ù…Ø· Ù…Ø·Ø±Ù‚Ø© (Hammer)."""
    o, h, l, c = row.get('open'), row.get('high'), row.get('low'), row.get('close')
    if pd.isna([o, h, l, c]).any():
        return 0
    body = abs(c - o)
    candle_range = h - l
    if candle_range == 0:
        return 0
    lower_shadow = min(o, c) - l
    upper_shadow = h - max(o, c)
    is_small_body = body < (candle_range * 0.35)
    is_long_lower_shadow = lower_shadow >= 1.8 * body if body > 0 else lower_shadow > candle_range * 0.6
    is_small_upper_shadow = upper_shadow <= body * 0.6 if body > 0 else upper_shadow < candle_range * 0.15
    return 100 if is_small_body and is_long_lower_shadow and is_small_upper_shadow else 0

def is_shooting_star(row: pd.Series) -> int:
    """ÙŠØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø´Ù…Ø¹Ø© Ù†Ù…Ø· Ø´Ù‡Ø§Ø¨ Ø³Ø§Ù‚Ø· (Shooting Star)."""
    o, h, l, c = row.get('open'), row.get('high'), row.get('low'), row.get('close')
    if pd.isna([o, h, l, c]).any():
        return 0
    body = abs(c - o)
    candle_range = h - l
    if candle_range == 0:
        return 0
    lower_shadow = min(o, c) - l
    upper_shadow = h - max(o, c)
    is_small_body = body < (candle_range * 0.35)
    is_long_upper_shadow = upper_shadow >= 1.8 * body if body > 0 else upper_shadow < candle_range * 0.6
    is_small_lower_shadow = lower_shadow <= body * 0.6 if body > 0 else lower_shadow < candle_range * 0.15
    return -100 if is_small_body and is_long_upper_shadow and is_small_lower_shadow else 0

def compute_engulfing(df: pd.DataFrame, idx: int) -> int:
    """ÙŠØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ù†Ù…Ø· Ø§Ø¨ØªÙ„Ø§Ø¹ÙŠ (Engulfing)."""
    if idx == 0: return 0
    prev = df.iloc[idx - 1]; curr = df.iloc[idx]
    if pd.isna([prev['close'], prev['open'], curr['close'], curr['open']]).any() or abs(prev['close'] - prev['open']) < (prev['high'] - prev['low']) * 0.1: return 0 # Prev is doji-like
    is_bullish = (prev['close'] < prev['open'] and curr['close'] > curr['open'] and curr['open'] <= prev['close'] and curr['close'] >= prev['open'])
    is_bearish = (prev['close'] > prev['open'] and curr['close'] < curr['open'] and curr['open'] >= prev['close'] and curr['close'] <= prev['open'])
    if is_bullish: return 100
    if is_bearish: return -100
    return 0

def detect_candlestick_patterns(df: pd.DataFrame) -> pd.DataFrame:
    """ÙŠÙƒØªØ´Ù Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø´Ù…ÙˆØ¹ Ø§Ù„ÙŠØ§Ø¨Ø§Ù†ÙŠØ©."""
    df = df.copy()
    logger.debug("â„¹ï¸ [Indicators] Ø§ÙƒØªØ´Ø§Ù Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø´Ù…ÙˆØ¹ Ø§Ù„ÙŠØ§Ø¨Ø§Ù†ÙŠØ©...")
    df['Hammer'] = df.apply(is_hammer, axis=1)
    df['ShootingStar'] = df.apply(is_shooting_star, axis=1)
    df['Engulfing'] = [compute_engulfing(df, i) for i in range(len(df))]
    df['BullishCandleSignal'] = df.apply(lambda row: 1 if (row['Hammer'] == 100 or row['Engulfing'] == 100) else 0, axis=1)
    df['BearishCandleSignal'] = df.apply(lambda row: 1 if (row['ShootingStar'] == -100 or row['Engulfing'] == -100) else 0, axis=1)
    logger.debug("âœ… [Indicators] ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø´Ù…ÙˆØ¹ Ø§Ù„ÙŠØ§Ø¨Ø§Ù†ÙŠØ©.")
    return df

# ---------------------- Other Helper Functions ----------------------
def fetch_recent_volume(symbol: str, interval: str = '15m') -> float:
    """ÙŠØ¬Ù„Ø¨ Ø­Ø¬Ù… Ø§Ù„ØªØ¯Ø§ÙˆÙ„ Ù„Ø¢Ø®Ø± ÙØªØ±Ø© Ø²Ù…Ù†ÙŠØ© Ù…Ø­Ø¯Ø¯Ø©."""
    if not client:
        logger.error(f"âŒ [Data Volume] Ø¹Ù…ÙŠÙ„ Binance ØºÙŠØ± Ù…Ù‡ÙŠØ£ Ù„Ù€ {symbol}."); return 0.0
    try:
        logger.debug(f"â„¹ï¸ [Data Volume] Ø¬Ù„Ø¨ Ø­Ø¬Ù… Ø§Ù„ØªØ¯Ø§ÙˆÙ„ Ù„Ø¢Ø®Ø± {interval} Ù„Ù€ {symbol}...")
        # Fetch klines for the specified interval
        klines = client.get_klines(symbol=symbol, interval=interval, limit=1)
        if not klines or len(klines) < 1:
            logger.warning(f"âš ï¸ [Data Volume] Ø¨ÙŠØ§Ù†Ø§Øª {interval} ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ù€ {symbol}."); return 0.0
        # Use quote asset volume (index 7) for USDT volume
        volume_usdt = float(klines[0][7]) if len(klines[0]) > 7 and klines[0][7] else 0.0
        logger.debug(f"âœ… [Data Volume] Ø§Ù„Ø³ÙŠÙˆÙ„Ø© Ù„Ø¢Ø®Ø± {interval} Ù„Ù€ {symbol}: {volume_usdt:.2f} USDT")
        return volume_usdt
    except Exception as e:
        logger.error(f"âŒ [Data Volume] Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø­Ø¬Ù… Ø§Ù„ØªØ¯Ø§ÙˆÙ„ Ù„Ù€ {symbol}: {e}", exc_info=True)
        return 0.0

# ---------------------- Performance Report Function ----------------------
def generate_performance_report() -> str:
    """ÙŠÙ†Ø´Ø¦ ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ø´Ø§Ù…Ù„."""
    logger.info("â„¹ï¸ [Report] Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡...")
    if not check_db_connection() or not conn or not cur:
        logger.error("âŒ [Report] Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±.")
        return "âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±ØŒ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."
    try:
        with conn.cursor() as report_cur: # Uses RealDictCursor
            report_cur.execute("SELECT COUNT(*) AS count FROM signals WHERE achieved_target = FALSE AND hit_stop_loss = FALSE;")
            open_signals_count = (report_cur.fetchone() or {}).get('count', 0)
            report_cur.execute("""
                SELECT
                    COUNT(*) AS total_closed,
                    COUNT(CASE WHEN profit_percentage > 0 THEN 1 END) AS winning_signals,
                    COUNT(CASE WHEN profit_percentage <= 0 THEN 1 END) AS losing_signals, -- Includes break-even as losing for simplicity here
                    COALESCE(SUM(profit_percentage), 0) AS total_profit_pct_sum,
                    COALESCE(AVG(profit_percentage), 0) AS avg_profit_pct,
                    COALESCE(SUM(CASE WHEN profit_percentage > 0 THEN profit_percentage ELSE 0 END), 0) AS gross_profit_pct_sum,
                    COALESCE(SUM(CASE WHEN profit_percentage < 0 THEN profit_percentage ELSE 0 END), 0) AS gross_loss_pct_sum,
                    COALESCE(AVG(CASE WHEN profit_percentage > 0 THEN profit_percentage END), 0) AS avg_win_pct,
                    COALESCE(AVG(CASE WHEN profit_percentage < 0 THEN profit_percentage END), 0) AS avg_loss_pct,
                    COALESCE(AVG(time_to_target_seconds), 0) AS avg_time_to_target_seconds
                FROM signals
                WHERE achieved_target = TRUE; -- Only count achieved targets for win/loss stats now
            """)
            closed_stats = report_cur.fetchone() or {}
            total_closed = closed_stats.get('total_closed', 0) # This now means total targets hit
            winning_signals = closed_stats.get('winning_signals', 0) # Should be same as total_closed if only target hits are counted
            losing_signals = 0 # Explicitly set to 0 as stop loss is removed

            total_profit_pct_sum = closed_stats.get('total_profit_pct_sum', 0.0)
            gross_profit_pct_sum = closed_stats.get('gross_profit_pct_sum', 0.0)
            gross_loss_pct_sum = 0.0 # No losses from stop loss
            avg_win_pct = closed_stats.get('avg_win_pct', 0.0)
            avg_loss_pct = 0.0 # No losses from stop loss
            avg_time_to_target_seconds = closed_stats.get('avg_time_to_target_seconds', 0.0)

            total_profit_usd = (total_profit_pct_sum / 100.0) * TRADE_VALUE
            gross_profit_usd = (gross_profit_pct_sum / 100.0) * TRADE_VALUE
            gross_loss_usd = 0.0

            win_rate = (winning_signals / total_closed) * 100 if total_closed > 0 else 0.0
            profit_factor = float('inf') if gross_loss_pct_sum == 0 else abs(gross_profit_pct_sum / gross_loss_pct_sum)

            avg_time_to_target_formatted = "N/A"
            if avg_time_to_target_seconds > 0:
                avg_time_delta = timedelta(seconds=avg_time_to_target_seconds)
                hours, remainder = divmod(avg_time_delta.total_seconds(), 3600)
                minutes, seconds = divmod(remainder, 60)
                avg_time_to_target_formatted = f"{int(hours)} Ø³, {int(minutes)} Ø¯, {int(seconds)} Ø«"


        report = (
            f"ğŸ“Š *ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø´Ø§Ù…Ù„ (Ø¨Ø¯ÙˆÙ† ÙˆÙ‚Ù Ø®Ø³Ø§Ø±Ø©):*\n"
            f"_(Ø§ÙØªØ±Ø§Ø¶ Ø­Ø¬Ù… Ø§Ù„ØµÙÙ‚Ø©: ${TRADE_VALUE:,.2f})_\n"
            f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n"
            f"ğŸ“ˆ Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙØªÙˆØ­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹: *{open_signals_count}*\n"
            f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n"
            f"ğŸ¯ *Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø­Ù‚Ù‚Ø©:*\n"
            f"  â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø­Ù‚Ù‚Ø©: *{total_closed}*\n"
            f"  â³ Ù…ØªÙˆØ³Ø· Ø§Ù„ÙˆÙ‚Øª Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù‡Ø¯Ù: *{avg_time_to_target_formatted}*\n"
            f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n"
            f"ğŸ’° *Ø§Ù„Ø±Ø¨Ø­ÙŠØ© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© (Ù…Ù† Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ù…Ø­Ù‚Ù‚Ø©):*\n"
            f"  â€¢ ØµØ§ÙÙŠ Ø§Ù„Ø±Ø¨Ø­: *{total_profit_pct_sum:+.2f}%* (â‰ˆ *${total_profit_usd:+.2f}*)\n"
            f"  â€¢ Ù…ØªÙˆØ³Ø· Ø§Ù„ØµÙÙ‚Ø© Ø§Ù„Ø±Ø§Ø¨Ø­Ø©: *{avg_win_pct:+.2f}%*\n"
            f"  â€¢ Ø¹Ø§Ù…Ù„ Ø§Ù„Ø±Ø¨Ø­: *{'âˆ' if profit_factor == float('inf') else f'{profit_factor:.2f}'}*\n"
            f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n"
            f"â„¹ï¸ *Ù…Ù„Ø§Ø­Ø¸Ø©: ØªÙ… Ø¥Ø²Ø§Ù„Ø© ÙˆÙ‚Ù Ø§Ù„Ø®Ø³Ø§Ø±Ø©. Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ØªØ¹ÙƒØ³ ÙÙ‚Ø· Ø§Ù„ØµÙÙ‚Ø§Øª Ø§Ù„ØªÙŠ ÙˆØµÙ„Øª Ø¥Ù„Ù‰ Ø§Ù„Ù‡Ø¯Ù.*\n"
            f"ğŸ•°ï¸ _Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ù…Ø­Ø¯Ø« Ø­ØªÙ‰: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}_"
        )
        logger.info("âœ… [Report] ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡.")
        return report
    except psycopg2.Error as db_err:
        logger.error(f"âŒ [Report] Ø®Ø·Ø£ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø«Ù†Ø§Ø¡ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {db_err}")
        if conn: conn.rollback()
        return "âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø«Ù†Ø§Ø¡ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±."
    except Exception as e:
        logger.error(f"âŒ [Report] Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {e}", exc_info=True)
        return "âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±."

# ---------------------- Trading Strategy -------------------
class ScalpingTradingStrategy:
    def __init__(self, symbol: str):
        self.symbol = symbol
        # Required columns for signal timeframe (15m) indicators - Removed VWAP, BB, OBV
        self.required_cols_signal_indicators = [
            'open', 'high', 'low', 'close', 'volume',
            f'ema_{EMA_SHORT_PERIOD}', f'ema_{EMA_LONG_PERIOD}', 'vwma',
            'rsi', 'atr', # Removed bb_upper, bb_lower, bb_middle
            'macd', 'macd_signal', 'macd_hist',
            'adx', 'di_plus', 'di_minus',
            # Removed vwap, obv
            'supertrend', 'supertrend_trend',
            'BullishCandleSignal', 'BearishCandleSignal' # Keep candle patterns for entry quality check
        ]
        # Required columns for confirmation timeframe (30m) indicators
        self.required_cols_confirmation_indicators = [
             'open', 'high', 'low', 'close',
             f'ema_{EMA_SHORT_PERIOD}', f'ema_{EMA_LONG_PERIOD}',
             'macd', 'macd_signal', 'macd_hist',
             'adx', 'di_plus', 'di_minus',
             'supertrend', 'supertrend_trend'
        ]
        # Required columns for buy signal generation - Removed VWAP, BB, OBV
        self.required_cols_buy_signal = [
            'close', f'ema_{EMA_SHORT_PERIOD}', f'ema_{EMA_LONG_PERIOD}', 'vwma',
            'rsi', 'atr', 'macd', 'macd_signal', 'macd_hist',
            'supertrend', 'supertrend_trend', 'adx', 'di_plus', 'di_minus',
            # Removed vwap, bb_upper, obv
            'BullishCandleSignal'
        ]
        # Adjusted weights for remaining optional conditions to increase sensitivity
        self.condition_weights = {
            'rsi_ok': 1.0, # Increased weight
            'bullish_candle': 2.0, # Increased weight
            # Removed 'not_bb_extreme': 0.5,
            # Removed 'obv_rising': 1.0,
            'rsi_filter_breakout': 1.5, # Increased weight
            'macd_filter_breakout': 1.5, # Increased weight
            'macd_hist_increasing': 4.0, # Increased weight
            # Removed 'obv_increasing_recent': 3.0,
            # Removed 'above_vwap': 1.0
        }
        # Essential conditions remain the same
        self.essential_conditions = [
            'price_above_emas_and_vwma', 'ema_short_above_ema_long',
            'supertrend_up', 'macd_positive_or_cross', 'adx_trending_bullish_strong',
        ]
        self.total_possible_score = sum(self.condition_weights.values())
        self.min_score_threshold_pct = 0.65 # Slightly lowered threshold for increased sensitivity
        self.min_signal_score = self.total_possible_score * self.min_score_threshold_pct

    def populate_indicators(self, df: pd.DataFrame, timeframe: str) -> Optional[pd.DataFrame]:
        """Populates indicators for a given dataframe and timeframe."""
        logger.debug(f"â„¹ï¸ [Strategy {self.symbol}] Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø±Ø§Øª Ø¥Ø·Ø§Ø± {timeframe}...")
        # Adjust min_len_required based on timeframe and indicators used
        min_len_required = max(EMA_LONG_PERIOD, VWMA_PERIOD, RSI_PERIOD, ENTRY_ATR_PERIOD, BOLLINGER_WINDOW, MACD_SLOW, ADX_PERIOD*2, SUPERTREND_PERIOD) + 5
        if timeframe == CONFIRMATION_TIMEFRAME:
             min_len_required = max(EMA_LONG_PERIOD, MACD_SLOW, ADX_PERIOD*2, SUPERTREND_PERIOD) + 5

        if len(df) < min_len_required:
            logger.warning(f"âš ï¸ [Strategy {self.symbol}] DataFrame Ø¥Ø·Ø§Ø± {timeframe} Ù‚ØµÙŠØ± Ø¬Ø¯Ù‹Ø§ ({len(df)} < {min_len_required}).")
            return None

        try:
            df_calc = df.copy()
            # Calculate indicators relevant to both timeframes or specific to one
            df_calc[f'ema_{EMA_SHORT_PERIOD}'] = calculate_ema(df_calc['close'], EMA_SHORT_PERIOD)
            df_calc[f'ema_{EMA_LONG_PERIOD}'] = calculate_ema(df_calc['close'], EMA_LONG_PERIOD)
            df_calc = calculate_macd(df_calc, MACD_FAST, MACD_SLOW, MACD_SIGNAL)
            adx_df = calculate_adx(df_calc, ADX_PERIOD); df_calc = df_calc.join(adx_df)
            df_calc = calculate_supertrend(df_calc, SUPERTREND_PERIOD, SUPERTREND_MULTIPLIER)

            if timeframe == SIGNAL_GENERATION_TIMEFRAME:
                # Add indicators specific to the signal timeframe (15m) - Removed VWAP, BB, OBV
                df_calc = calculate_atr_indicator(df_calc, ENTRY_ATR_PERIOD)
                df_calc['vwma'] = calculate_vwma(df_calc, VWMA_PERIOD)
                df_calc = calculate_rsi_indicator(df_calc, RSI_PERIOD)
                # df_calc = calculate_bollinger_bands(df_calc, BOLLINGER_WINDOW, BOLLINGER_STD_DEV) # Removed
                # df_calc = calculate_vwap(df_calc) # Removed
                # df_calc = calculate_obv(df_calc) # Removed
                df_calc = detect_candlestick_patterns(df_calc)
                required_cols = self.required_cols_signal_indicators
            elif timeframe == CONFIRMATION_TIMEFRAME:
                 required_cols = self.required_cols_confirmation_indicators
            else:
                 logger.error(f"âŒ [Strategy {self.symbol}] Ø¥Ø·Ø§Ø± Ø²Ù…Ù†ÙŠ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ '{timeframe}' Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª.")
                 return None

            missing_cols = [col for col in required_cols if col not in df_calc.columns]
            if missing_cols:
                logger.error(f"âŒ [Strategy {self.symbol}] Ø£Ø¹Ù…Ø¯Ø© Ù…Ø¤Ø´Ø±Ø§Øª Ø¥Ø·Ø§Ø± {timeframe} Ù…ÙÙ‚ÙˆØ¯Ø©: {missing_cols}")
                return None

            # Drop rows with NaNs only for the required columns for this timeframe
            df_cleaned = df_calc.dropna(subset=required_cols).copy()
            if df_cleaned.empty:
                logger.warning(f"âš ï¸ [Strategy {self.symbol}] DataFrame Ø¥Ø·Ø§Ø± {timeframe} ÙØ§Ø±Øº Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ù‚ÙŠÙ… NaN.")
                return None

            logger.debug(f"âœ… [Strategy {self.symbol}] ØªÙ… Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø±Ø§Øª Ø¥Ø·Ø§Ø± {timeframe}.")
            return df_cleaned
        except Exception as e:
            logger.error(f"âŒ [Strategy {self.symbol}] Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø±Ø§Øª Ø¥Ø·Ø§Ø± {timeframe}: {e}", exc_info=True)
            return None

    def check_confirmation_conditions(self) -> Tuple[bool, Dict[str, Any]]:
        """Checks for bullish trend confirmation on the larger timeframe (30m)."""
        logger.debug(f"â„¹ï¸ [Strategy {self.symbol}] Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø´Ø±ÙˆØ· Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± {CONFIRMATION_TIMEFRAME}...")
        df_conf = fetch_historical_data(self.symbol, interval=CONFIRMATION_TIMEFRAME, days=CONFIRMATION_LOOKBACK_DAYS)
        confirmation_details = {}

        if df_conf is None or df_conf.empty:
            logger.warning(f"âš ï¸ [Strategy {self.symbol}] ØªØ¹Ø°Ø± Ø¬Ù„Ø¨ Ø£Ùˆ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø·Ø§Ø± {CONFIRMATION_TIMEFRAME} Ù„Ù„ØªØ£ÙƒÙŠØ¯.")
            confirmation_details['Status'] = f"ÙØ´Ù„: Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø·Ø§Ø± {CONFIRMATION_TIMEFRAME}"
            return False, confirmation_details

        df_conf_processed = self.populate_indicators(df_conf, CONFIRMATION_TIMEFRAME)

        if df_conf_processed is None or df_conf_processed.empty:
            logger.warning(f"âš ï¸ [Strategy {self.symbol}] ØªØ¹Ø°Ø± Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ù„ØªØ£ÙƒÙŠØ¯ Ø¥Ø·Ø§Ø± {CONFIRMATION_TIMEFRAME}.")
            confirmation_details['Status'] = f"ÙØ´Ù„: Ø®Ø·Ø£ ÙÙŠ Ù…Ø¤Ø´Ø±Ø§Øª Ø¥Ø·Ø§Ø± {CONFIRMATION_TIMEFRAME}"
            return False, confirmation_details

        last_row_conf = df_conf_processed.iloc[-1]
        logger.debug(f"â„¹ï¸ [Strategy {self.symbol}] Ø¢Ø®Ø± Ø´Ù…Ø¹Ø© Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± {CONFIRMATION_TIMEFRAME}: Close={last_row_conf['close']:.4f}, EMA_Short={last_row_conf[f'ema_{EMA_SHORT_PERIOD}']:.4f}, EMA_Long={last_row_conf[f'ema_{EMA_LONG_PERIOD}']:.4f}, Supertrend_Trend={last_row_conf.get('supertrend_trend')}, MACD_Hist={last_row_conf.get('macd_hist'):.4f}, ADX={last_row_conf.get('adx'):.1f}, DI+={last_row_conf.get('di_plus'):.1f}, DI-={last_row_conf.get('di_minus'):.1f}")


        # Confirmation Conditions: Price above EMAs, Supertrend up, MACD bullish, ADX trending
        price_above_emas_conf = (pd.notna(last_row_conf['close']) and
                                 pd.notna(last_row_conf[f'ema_{EMA_SHORT_PERIOD}']) and
                                 pd.notna(last_row_conf[f'ema_{EMA_LONG_PERIOD}']) and
                                 last_row_conf['close'] > last_row_conf[f'ema_{EMA_SHORT_PERIOD}'] and
                                 last_row_conf['close'] > last_row_conf[f'ema_{EMA_LONG_PERIOD}'])
        confirmation_details['Price_Above_EMAs_Conf'] = f"Ø§Ø¬ØªØ§Ø² ({last_row_conf['close']:.4f} > {last_row_conf[f'ema_{EMA_SHORT_PERIOD}']:.4f}, {last_row_conf['close']:.4f} > {last_row_conf[f'ema_{EMA_LONG_PERIOD}']:.4f})" if price_above_emas_conf else f"ÙØ´Ù„ ({last_row_conf['close']:.4f} Ù„ÙŠØ³ ÙÙˆÙ‚ EMA {EMA_SHORT_PERIOD}/{EMA_LONG_PERIOD})"

        supertrend_up_conf = (pd.notna(last_row_conf['supertrend_trend']) and last_row_conf['supertrend_trend'] == 1)
        confirmation_details['SuperTrend_Conf'] = "Ø§Ø¬ØªØ§Ø² (Ø§ØªØ¬Ø§Ù‡ ØµØ§Ø¹Ø¯)" if supertrend_up_conf else "ÙØ´Ù„ (Ù„ÙŠØ³ Ø§ØªØ¬Ø§Ù‡ ØµØ§Ø¹Ø¯)"

        macd_bullish_conf = (pd.notna(last_row_conf['macd_hist']) and last_row_conf['macd_hist'] > 0)
        confirmation_details['MACD_Conf'] = f"Ø§Ø¬ØªØ§Ø² (Hist > 0: {last_row_conf['macd_hist']:.4f})" if macd_bullish_conf else f"ÙØ´Ù„ (Hist <= 0: {last_row_conf.get('macd_hist', np.nan):.4f})"

        adx_trending_bullish_conf = (pd.notna(last_row_conf['adx']) and last_row_conf['adx'] > MIN_ADX_TREND_STRENGTH and
                                     pd.notna(last_row_conf['di_plus']) and pd.notna(last_row_conf['di_minus']) and
                                     last_row_conf['di_plus'] > last_row_conf['di_minus'])
        confirmation_details['ADX_DI_Conf'] = f"Ø§Ø¬ØªØ§Ø² (ADX:{last_row_conf['adx']:.1f}, DI+>DI-)" if adx_trending_bullish_conf else f"ÙØ´Ù„ (ADX <= {MIN_ADX_TREND_STRENGTH} Ø£Ùˆ DI+ <= DI-)"

        all_confirmed = price_above_emas_conf and supertrend_up_conf and macd_bullish_conf and adx_trending_bullish_conf

        confirmation_details['Status'] = "Ù…Ø¤ÙƒØ¯" if all_confirmed else "ÙØ´Ù„ Ø§Ù„ØªØ£ÙƒÙŠØ¯"
        logger.debug(f"âœ… [Strategy {self.symbol}] Ø­Ø§Ù„Ø© ØªØ£ÙƒÙŠØ¯ Ø¥Ø·Ø§Ø± {CONFIRMATION_TIMEFRAME}: {confirmation_details['Status']}")

        return all_confirmed, confirmation_details

    def check_entry_point_quality(self, df_processed_signal: pd.DataFrame) -> Tuple[bool, Dict[str, Any]]:
        """Checks if the current price offers a good entry point on the signal timeframe (15m)."""
        logger.debug(f"â„¹ï¸ [Strategy {self.symbol}] Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± {SIGNAL_GENERATION_TIMEFRAME}...")
        entry_point_details = {}

        if df_processed_signal is None or df_processed_signal.empty or len(df_processed_signal) < ENTRY_POINT_RECENT_CANDLE_LOOKBACK + 1:
            logger.warning(f"âš ï¸ [Strategy {self.symbol}] Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø·Ø§Ø± {SIGNAL_GENERATION_TIMEFRAME} ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„.")
            entry_point_details['Status'] = f"ÙØ´Ù„: Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø·Ø§Ø± {SIGNAL_GENERATION_TIMEFRAME} ØºÙŠØ± ÙƒØ§ÙÙŠØ©"
            return False, entry_point_details

        last_row_signal = df_processed_signal.iloc[-1]
        recent_df_signal = df_processed_signal.iloc[-ENTRY_POINT_RECENT_CANDLE_LOOKBACK-1:]

        if recent_df_signal[['close', 'open', f'ema_{EMA_SHORT_PERIOD}']].isnull().values.any():
             logger.warning(f"âš ï¸ [Strategy {self.symbol}] Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø·Ø§Ø± {SIGNAL_GENERATION_TIMEFRAME} Ø§Ù„Ø£Ø®ÙŠØ±Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ NaN Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„.")
             entry_point_details['Status'] = f"ÙØ´Ù„: NaN ÙÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø·Ø§Ø± {SIGNAL_GENERATION_TIMEFRAME} Ø§Ù„Ø£Ø®ÙŠØ±Ø©"
             return False, entry_point_details

        current_price = last_row_signal['close']
        ema_short_signal = last_row_signal[f'ema_{EMA_SHORT_PERIOD}']

        # Condition 1: Price is close to the signal timeframe EMA_SHORT
        price_near_ema_short = abs(current_price - ema_short_signal) / ema_short_signal <= ENTRY_POINT_EMA_PROXIMITY_PCT if ema_short_signal > 0 else False
        entry_point_details['Price_Near_EMA_Short_SignalTF'] = f"Ø§Ø¬ØªØ§Ø² (Ø¶Ù…Ù† {ENTRY_POINT_EMA_PROXIMITY_PCT*100:.2f}%)" if price_near_ema_short else f"ÙØ´Ù„ (Ø§Ù„Ù…Ø³Ø§ÙØ©: {abs(current_price - ema_short_signal) / ema_short_signal * 100:.2f}%)"
        logger.debug(f"â„¹ï¸ [Strategy {self.symbol}] ØªØ­Ù‚Ù‚ Ø§Ù„Ù‚Ø±Ø¨ Ù…Ù† EMA {EMA_SHORT_PERIOD} ({SIGNAL_GENERATION_TIMEFRAME}): Ø§Ù„Ø³Ø¹Ø± {current_price:.4f}, EMA {EMA_SHORT_PERIOD} {ema_short_signal:.4f}, Ù‚Ø±ÙŠØ¨: {price_near_ema_short}")


        # Condition 2: Last candle is bullish or a hammer
        last_candle_bullish_or_hammer = is_bullish_candle(last_row_signal) or is_hammer(last_row_signal) == 100
        entry_point_details['Last_Candle_Bullish_or_Hammer_SignalTF'] = "Ø§Ø¬ØªØ§Ø²" if last_candle_bullish_or_hammer else "ÙØ´Ù„"
        logger.debug(f"â„¹ï¸ [Strategy {self.symbol}] ØªØ­Ù‚Ù‚ Ø§Ù„Ø´Ù…Ø¹Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© ({SIGNAL_GENERATION_TIMEFRAME}): ØµØ¹ÙˆØ¯ÙŠØ© Ø£Ùˆ Ù…Ø·Ø±Ù‚Ø©: {last_candle_bullish_or_hammer}")


        # Combine conditions for a good entry point
        is_good_entry = price_near_ema_short and last_candle_bullish_or_hammer

        entry_point_details['Status'] = "Ù†Ù‚Ø·Ø© Ø¯Ø®ÙˆÙ„ Ø¬ÙŠØ¯Ø©" if is_good_entry else "Ù†Ù‚Ø·Ø© Ø¯Ø®ÙˆÙ„ Ù„ÙŠØ³Øª Ù…Ø«Ø§Ù„ÙŠØ©"
        logger.debug(f"âœ… [Strategy {self.symbol}] Ø­Ø§Ù„Ø© Ø¬ÙˆØ¯Ø© Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± {SIGNAL_GENERATION_TIMEFRAME}: {entry_point_details['Status']}")

        return is_good_entry, entry_point_details


    def generate_buy_signal(self, df_processed: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """ÙŠÙˆÙ„Ø¯ Ø¥Ø´Ø§Ø±Ø© Ø´Ø±Ø§Ø¡ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø´Ø±ÙˆØ· Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©."""
        logger.debug(f"â„¹ï¸ [Strategy {self.symbol}] Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø¥Ø´Ø§Ø±Ø© Ø´Ø±Ø§Ø¡...")
        min_signal_data_len = max(RECENT_EMA_CROSS_LOOKBACK, MACD_HIST_INCREASE_CANDLES, OBV_INCREASE_CANDLES, ENTRY_POINT_RECENT_CANDLE_LOOKBACK) + 1
        if df_processed is None or df_processed.empty or len(df_processed) < min_signal_data_len:
            logger.warning(f"âš ï¸ [Strategy {self.symbol}] DataFrame Ù‚ØµÙŠØ± Ø¬Ø¯Ù‹Ø§ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©."); return None
        missing_cols = [col for col in self.required_cols_buy_signal if col not in df_processed.columns]
        if missing_cols: logger.warning(f"âš ï¸ [Strategy {self.symbol}] Ø£Ø¹Ù…Ø¯Ø© Ù…ÙÙ‚ÙˆØ¯Ø© Ù„Ù„Ø¥Ø´Ø§Ø±Ø©: {missing_cols}."); return None

        # --- Step 1: Check Multi-Timeframe Confirmation (30m) ---
        is_confirmed_on_larger_tf, confirmation_details = self.check_confirmation_conditions()
        if not is_confirmed_on_larger_tf:
             logger.debug(f"â„¹ï¸ [Strategy {self.symbol}] ÙØ´Ù„ Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± {CONFIRMATION_TIMEFRAME}. Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©.")
             return None # Do not generate signal if larger timeframe is not confirmed
        logger.debug(f"âœ… [Strategy {self.symbol}] ØªÙ… Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± {CONFIRMATION_TIMEFRAME}. Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©.")


        # --- Step 2: Check Entry Point Quality on Signal Timeframe (15m) ---
        is_good_entry_point, entry_point_details = self.check_entry_point_quality(df_processed)
        if not is_good_entry_point:
             logger.debug(f"â„¹ï¸ [Strategy {self.symbol}] Ø¬ÙˆØ¯Ø© Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± {SIGNAL_GENERATION_TIMEFRAME} Ù„ÙŠØ³Øª Ù…Ø«Ø§Ù„ÙŠØ©. Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©.")
             return None # Do not generate signal if entry point is not ideal
        logger.debug(f"âœ… [Strategy {self.symbol}] Ø¬ÙˆØ¯Ø© Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± {SIGNAL_GENERATION_TIMEFRAME} Ø¬ÙŠØ¯Ø©. Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©.")


        # --- Step 3: Proceed with Signal Generation if Confirmed and Entry is Good ---
        btc_trend = get_btc_trend_4h()
        if "Ù‡Ø¨ÙˆØ·" in btc_trend: logger.info(f"â„¹ï¸ [Strategy {self.symbol}] Ø§Ù„ØªØ¯Ø§ÙˆÙ„ Ù…ØªÙˆÙ‚Ù (Ø§ØªØ¬Ø§Ù‡ BTC Ù‡Ø§Ø¨Ø·: {btc_trend})."); return None
        if "N/A" in btc_trend: logger.warning(f"âš ï¸ [Strategy {self.symbol}] Ø§ØªØ¬Ø§Ù‡ BTC ØºÙŠØ± Ù…ØªØ§Ø­ØŒ ØªÙ… ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø´Ø±Ø·.")

        last_row = df_processed.iloc[-1]; recent_df = df_processed.iloc[-min_signal_data_len:]
        if recent_df[self.required_cols_buy_signal].isnull().values.any():
            logger.warning(f"âš ï¸ [Strategy {self.symbol}] Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ NaN ÙÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©."); return None

        essential_passed = True; failed_essential_conditions = []; signal_details = {}
        # Mandatory Conditions Check (on 15m timeframe)
        if not (pd.notna(last_row[f'ema_{EMA_SHORT_PERIOD}']) and pd.notna(last_row[f'ema_{EMA_LONG_PERIOD}']) and pd.notna(last_row['vwma']) and last_row['close'] > last_row[f'ema_{EMA_SHORT_PERIOD}'] and last_row['close'] > last_row[f'ema_{LONG_PERIOD}'] and last_row['close'] > last_row['vwma']):
            essential_passed = False; failed_essential_conditions.append('Ø§Ù„Ø³Ø¹Ø± ÙÙˆÙ‚ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù…ØªØ­Ø±ÙƒØ© Ùˆ VWMA'); signal_details['Price_MA_Alignment_SignalTF'] = 'ÙØ´Ù„: Ø§Ù„Ø³Ø¹Ø± Ù„ÙŠØ³ ÙÙˆÙ‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù…ØªØ­Ø±ÙƒØ© Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± Ø§Ù„Ø¥Ø´Ø§Ø±Ø©'
        else: signal_details['Price_MA_Alignment_SignalTF'] = 'Ø§Ø¬ØªØ§Ø²: Ø§Ù„Ø³Ø¹Ø± ÙÙˆÙ‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù…ØªØ­Ø±ÙƒØ© Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± Ø§Ù„Ø¥Ø´Ø§Ø±Ø©'

        # Corrected typo: EMA_LONG_PERIOD instead of LONG_PERIOD
        if not (pd.notna(last_row[f'ema_{EMA_SHORT_PERIOD}']) and pd.notna(last_row[f'ema_{EMA_LONG_PERIOD}']) and last_row[f'ema_{EMA_SHORT_PERIOD}'] > last_row[f'ema_{EMA_LONG_PERIOD}']):
            essential_passed = False; failed_essential_conditions.append('EMA Ø§Ù„Ù‚ØµÙŠØ± > EMA Ø§Ù„Ø·ÙˆÙŠÙ„'); signal_details['EMA_Order_SignalTF'] = 'ÙØ´Ù„: EMA Ø§Ù„Ù‚ØµÙŠØ± Ù„ÙŠØ³ ÙÙˆÙ‚ EMA Ø§Ù„Ø·ÙˆÙŠÙ„ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± Ø§Ù„Ø¥Ø´Ø§Ø±Ø©'
        else: signal_details['EMA_Order_SignalTF'] = 'Ø§Ø¬ØªØ§Ø²: EMA Ø§Ù„Ù‚ØµÙŠØ± ÙÙˆÙ‚ EMA Ø§Ù„Ø·ÙˆÙŠÙ„ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± Ø§Ù„Ø¥Ø´Ø§Ø±Ø©'

        if not (pd.notna(last_row['supertrend']) and last_row['close'] > last_row['supertrend'] and last_row['supertrend_trend'] == 1):
            essential_passed = False; failed_essential_conditions.append('SuperTrend ØµØ§Ø¹Ø¯'); signal_details['SuperTrend_SignalTF'] = 'ÙØ´Ù„: SuperTrend Ù„ÙŠØ³ ØµØ§Ø¹Ø¯Ù‹Ø§ Ø£Ùˆ Ø§Ù„Ø³Ø¹Ø± Ù„ÙŠØ³ ÙÙˆÙ‚Ù‡ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± Ø§Ù„Ø¥Ø´Ø§Ø±Ø©'
        else: signal_details['SuperTrend_SignalTF'] = 'Ø§Ø¬ØªØ§Ø²: SuperTrend ØµØ§Ø¹Ø¯ ÙˆØ§Ù„Ø³Ø¹Ø± ÙÙˆÙ‚Ù‡ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± Ø§Ù„Ø¥Ø´Ø§Ø±Ø©'

        if not (pd.notna(last_row['macd_hist']) and (last_row['macd_hist'] > 0 or (pd.notna(last_row['macd']) and pd.notna(last_row['macd_signal']) and last_row['macd'] > last_row['macd_signal']))):
            essential_passed = False; failed_essential_conditions.append('MACD ØµØ¹ÙˆØ¯ÙŠ'); signal_details['MACD_SignalTF'] = 'ÙØ´Ù„: MACD Hist Ù„ÙŠØ³ Ø¥ÙŠØ¬Ø§Ø¨ÙŠÙ‹Ø§ ÙˆÙ„Ø§ ÙŠÙˆØ¬Ø¯ ØªÙ‚Ø§Ø·Ø¹ ØµØ¹ÙˆØ¯ÙŠ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± Ø§Ù„Ø¥Ø´Ø§Ø±Ø©'
        else: signal_details['MACD_SignalTF'] = 'Ø§Ø¬ØªØ§Ø²: MACD Hist Ø¥ÙŠØ¬Ø§Ø¨ÙŠ Ø£Ùˆ ÙŠÙˆØ¬Ø¯ ØªÙ‚Ø§Ø·Ø¹ ØµØ¹ÙˆØ¯ÙŠ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± Ø§Ù„Ø¥Ø´Ø§Ø±Ø©'

        if not (pd.notna(last_row['adx']) and pd.notna(last_row['di_plus']) and pd.notna(last_row['di_minus']) and last_row['adx'] > MIN_ADX_TREND_STRENGTH and last_row['di_plus'] > last_row['di_minus']):
            essential_passed = False; failed_essential_conditions.append(f'ADX Ù‚ÙˆÙŠ ØµØ¹ÙˆØ¯ÙŠ (>{MIN_ADX_TREND_STRENGTH})'); signal_details['ADX_DI_SignalTF'] = f'ÙØ´Ù„: Ù„ÙŠØ³ Ù‚ÙˆÙŠÙ‹Ø§ ØµØ¹ÙˆØ¯ÙŠÙ‹Ø§ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± Ø§Ù„Ø¥Ø´Ø§Ø±Ø© (ADX <= {MIN_ADX_TREND_STRENGTH} Ø£Ùˆ DI+ <= DI-)'
        else: signal_details['ADX_DI_SignalTF'] = f'Ø§Ø¬ØªØ§Ø²: Ù‚ÙˆÙŠ ØµØ¹ÙˆØ¯ÙŠÙ‹Ø§ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± Ø§Ù„Ø¥Ø´Ø§Ø±Ø© (ADX:{last_row["adx"]:.1f}, DI+>DI-)'


        if not essential_passed:
            logger.debug(f"â„¹ï¸ [Strategy {self.symbol}] Ø´Ø±ÙˆØ· {SIGNAL_GENERATION_TIMEFRAME} Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ© ÙØ´Ù„Øª: {', '.join(failed_essential_conditions)}.");
            # Include essential conditions check results in details even if failed
            signal_details['Essential_Conditions_SignalTF_Status'] = 'ÙØ´Ù„'
            signal_details['Failed_Essential_Conditions_SignalTF'] = failed_essential_conditions
            return None

        signal_details['Essential_Conditions_SignalTF_Status'] = 'Ø§Ø¬ØªØ§Ø²'
        current_score = 0.0 # Optional Conditions Scoring (Updated based on remaining conditions)

        if pd.notna(last_row['rsi']) and RSI_OVERSOLD < last_row['rsi'] < RSI_OVERBOUGHT : current_score += self.condition_weights.get('rsi_ok', 0); signal_details['RSI_Basic_SignalTF'] = f'Ù…Ù‚Ø¨ÙˆÙ„ ({RSI_OVERSOLD}<{last_row["rsi"]:.1f}<{RSI_OVERBOUGHT}) (+{self.condition_weights.get("rsi_ok",0)})'
        else: signal_details['RSI_Basic_SignalTF'] = f'RSI ({last_row.get("rsi", np.nan):.1f}) ØºÙŠØ± Ù…Ù‚Ø¨ÙˆÙ„ (0)'

        if last_row.get('BullishCandleSignal', 0) == 1: current_score += self.condition_weights.get('bullish_candle', 0); signal_details['Candle_SignalTF'] = f'Ù†Ù…Ø· Ø´Ù…Ø¹Ø© ØµØ¹ÙˆØ¯ÙŠ (+{self.condition_weights.get("bullish_candle",0)})'
        else: signal_details['Candle_SignalTF'] = 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Ù…Ø· Ø´Ù…Ø¹Ø© ØµØ¹ÙˆØ¯ÙŠ (0)'

        # Removed: if pd.notna(last_row['bb_upper']) and last_row['close'] < last_row['bb_upper'] * 0.995 : current_score += self.condition_weights.get('not_bb_extreme', 0); signal_details['Bollinger_Basic_SignalTF'] = f'Ù„ÙŠØ³ Ø¹Ù†Ø¯ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø¹Ù„ÙˆÙŠ (+{self.condition_weights.get("not_bb_extreme",0)})'
        # Removed: else: signal_details['Bollinger_Basic_SignalTF'] = 'Ø¹Ù†Ø¯ Ø£Ùˆ ÙÙˆÙ‚ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø¹Ù„ÙˆÙŠ (0)'

        # Removed: if len(df_processed) >= 2 and pd.notna(df_processed.iloc[-2]['obv']) and pd.notna(last_row['obv']) and last_row['obv'] > df_processed.iloc[-2]['obv']: current_score += self.condition_weights.get('obv_rising', 0); signal_details['OBV_Last_SignalTF'] = f'ÙŠØ±ØªÙØ¹ ÙÙŠ Ø¢Ø®Ø± Ø´Ù…Ø¹Ø© (+{self.condition_weights.get("obv_rising",0)})'
        # Removed: else: signal_details['OBV_Last_SignalTF'] = 'Ù„Ø§ ÙŠØ±ØªÙØ¹ ÙÙŠ Ø¢Ø®Ø± Ø´Ù…Ø¹Ø© (0)'

        if pd.notna(last_row['rsi']) and 50 <= last_row['rsi'] <= 80: current_score += self.condition_weights.get('rsi_filter_breakout', 0); signal_details['RSI_Filter_Breakout_SignalTF'] = f'RSI ({last_row["rsi"]:.1f}) ÙÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„ØµØ¹ÙˆØ¯ÙŠ (50-80) (+{self.condition_weights.get("rsi_filter_breakout",0)})'
        else: signal_details['RSI_Filter_Breakout_SignalTF'] = f'RSI ({last_row.get("rsi", np.nan):.1f}) Ù„ÙŠØ³ ÙÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„ØµØ¹ÙˆØ¯ÙŠ (0)'

        if pd.notna(last_row['macd_hist']) and last_row['macd_hist'] > 0: current_score += self.condition_weights.get('macd_filter_breakout', 0); signal_details['MACD_Filter_Breakout_SignalTF'] = f'MACD Hist Ø¥ÙŠØ¬Ø§Ø¨ÙŠ ({last_row["macd_hist"]:.4f}) (+{self.condition_weights.get("macd_filter_breakout",0)})'
        else: signal_details['MACD_Filter_Breakout_SignalTF'] = 'MACD Hist Ù„ÙŠØ³ Ø¥ÙŠØ¬Ø§Ø¨ÙŠÙ‹Ø§ (0)'

        if len(recent_df) >= MACD_HIST_INCREASE_CANDLES + 1 and not recent_df['macd_hist'].iloc[-MACD_HIST_INCREASE_CANDLES-1:].isnull().any() and np.all(np.diff(recent_df['macd_hist'].iloc[-MACD_HIST_INCREASE_CANDLES-1:]) > 0): current_score += self.condition_weights.get('macd_hist_increasing', 0); signal_details['MACD_Hist_Increasing_SignalTF'] = f'MACD Hist ÙŠØ²Ø¯Ø§Ø¯ ({MACD_HIST_INCREASE_CANDLES} Ø´Ù…Ø¹Ø©) (+{self.condition_weights.get("macd_hist_increasing",0)})'
        else: signal_details['MACD_Hist_Increasing_SignalTF'] = f'MACD Hist Ù„Ø§ ÙŠØ²Ø¯Ø§Ø¯ ({MACD_HIST_INCREASE_CANDLES} Ø´Ù…Ø¹Ø©) (0)'

        # Removed: if len(recent_df) >= OBV_INCREASE_CANDLES + 1 and not recent_df['obv'].iloc[-OBV_INCREASE_CANDLES-1:].isnull().any() and np.all(np.diff(recent_df['obv'].iloc[-OBV_INCREASE_CANDLES-1:]) > 0): current_score += self.condition_weights.get('obv_increasing_recent', 0); signal_details['OBV_Increasing_Recent_SignalTF'] = f'OBV ÙŠØ²Ø¯Ø§Ø¯ ({OBV_INCREASE_CANDLES} Ø´Ù…Ø¹Ø©) (+{self.condition_weights.get("obv_increasing_recent",0)})'
        # Removed: else: signal_details['OBV_Increasing_Recent_SignalTF'] = f'OBV Ù„Ø§ ÙŠØ²Ø¯Ø§Ø¯ ({OBV_INCREASE_CANDLES} Ø´Ù…Ø¹Ø©) (0)'

        # Removed: if pd.notna(last_row['vwap']) and last_row['close'] > last_row['vwap']: current_score += self.condition_weights.get('above_vwap', 0); signal_details['VWAP_Daily'] = f'ÙÙˆÙ‚ VWAP Ø§Ù„ÙŠÙˆÙ…ÙŠ (+{self.condition_weights.get("above_vwap",0)})'
        # Removed: else: signal_details['VWAP_Daily'] = 'ØªØ­Øª VWAP Ø§Ù„ÙŠÙˆÙ…ÙŠ (0)'


        if current_score < self.min_signal_score:
            logger.debug(f"â„¹ï¸ [Strategy {self.symbol}] Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±ÙŠØ© Ù…Ù†Ø®ÙØ¶Ø© Ø¬Ø¯Ù‹Ø§ ({current_score:.2f} < {self.min_signal_score:.2f}). Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©.");
            signal_details['Optional_Score_Status'] = f'ÙØ´Ù„: Ø§Ù„Ù†Ù‚Ø§Ø· {current_score:.2f} Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ {self.min_signal_score:.2f}'
            return None

        signal_details['Optional_Score_Status'] = f'Ø§Ø¬ØªØ§Ø²: Ø§Ù„Ù†Ù‚Ø§Ø· {current_score:.2f}'

        # Fetch volume for the signal timeframe (15m)
        volume_recent = fetch_recent_volume(self.symbol, interval=SIGNAL_GENERATION_TIMEFRAME)
        if volume_recent < MIN_VOLUME_15M_USDT:
            logger.info(f"â„¹ï¸ [Strategy {self.symbol}] Ø§Ù„Ø³ÙŠÙˆÙ„Ø© Ù…Ù†Ø®ÙØ¶Ø© ({volume_recent:,.0f} < {MIN_VOLUME_15M_USDT:,.0f}). Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©.");
            signal_details['Liquidity_Check'] = f'ÙØ´Ù„: Ø§Ù„Ø³ÙŠÙˆÙ„Ø© {volume_recent:,.0f} Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ {MIN_VOLUME_15M_USDT:,.0f}'
            return None

        signal_details['Liquidity_Check'] = f'Ø§Ø¬ØªØ§Ø²: Ø§Ù„Ø³ÙŠÙˆÙ„Ø© {volume_recent:,.0f}'

        current_price = last_row['close']; current_atr = last_row.get('atr')
        if pd.isna(current_atr) or current_atr <= 0:
            logger.warning(f"âš ï¸ [Strategy {self.symbol}] ATR ØºÙŠØ± ØµØ§Ù„Ø­ ({current_atr}). Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©.");
            signal_details['ATR_Check'] = f'ÙØ´Ù„: ATR ØºÙŠØ± ØµØ§Ù„Ø­ ({current_atr})'
            return None

        signal_details['ATR_Check'] = f'Ø§Ø¬ØªØ§Ø²: ATR ØµØ§Ù„Ø­ ({current_atr:.4f})'

        initial_target = current_price + (ENTRY_ATR_MULTIPLIER * current_atr)
        initial_stop_loss = 0.0 # Stop Loss is removed

        profit_margin_pct = ((initial_target / current_price) - 1) * 100 if current_price > 0 else 0
        if profit_margin_pct < MIN_PROFIT_MARGIN_PCT:
            logger.info(f"â„¹ï¸ [Strategy {self.symbol}] Ù‡Ø§Ù…Ø´ Ø§Ù„Ø±Ø¨Ø­ Ù…Ù†Ø®ÙØ¶ Ø¬Ø¯Ù‹Ø§ ({profit_margin_pct:.2f}% < {MIN_PROFIT_MARGIN_PCT:.2f}%). Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©.");
            signal_details['Profit_Margin_Check'] = f'ÙØ´Ù„: Ù‡Ø§Ù…Ø´ Ø§Ù„Ø±Ø¨Ø­ {profit_margin_pct:.2f}% Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ {MIN_PROFIT_MARGIN_PCT:.2f}%'
            return None

        signal_details['Profit_Margin_Check'] = f'Ø§Ø¬ØªØ§Ø²: Ù‡Ø§Ù…Ø´ Ø§Ù„Ø±Ø¨Ø­ {profit_margin_pct:.2f}%'

        # Include confirmation and entry point details in the signal details
        signal_details['Confirmation_Details'] = confirmation_details
        signal_details['Entry_Point_Details_SignalTF'] = entry_point_details


        signal_output = {
            'symbol': self.symbol, 'entry_price': float(f"{current_price:.8g}"),
            'initial_target': float(f"{initial_target:.8g}"),
            'initial_stop_loss': initial_stop_loss, # Stop loss removed
            'current_target': float(f"{initial_target:.8g}"),
            'current_stop_loss': initial_stop_loss, # Stop loss removed
            'r2_score': float(f"{current_score:.2f}"),
            'strategy_name': 'Scalping_Momentum_Trend_MultiTF_EnhancedEntry_V2', # Updated strategy name
            'signal_details': signal_details, 'volume_15m': volume_recent, # Renamed volume key to reflect interval
            'trade_value': TRADE_VALUE, 'total_possible_score': float(f"{self.total_possible_score:.2f}")
        }
        logger.info(f"âœ… [Strategy {self.symbol}] ØªÙ… ØªØ£ÙƒÙŠØ¯ Ø¥Ø´Ø§Ø±Ø© Ø´Ø±Ø§Ø¡. Ø§Ù„Ø³Ø¹Ø±: {current_price:.6f}, Ø§Ù„Ù†Ù‚Ø§Ø·: {current_score:.2f}, ATR: {current_atr:.6f}")
        return signal_output

    def analyze_target_continuation(self, df_processed: pd.DataFrame, current_price: float, current_target: float) -> Optional[float]:
        """Analyzes if the target should be extended."""
        logger.debug(f"â„¹ï¸ [Strategy {self.symbol}] ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªÙ…Ø±Ø§Ø±ÙŠØ© Ø§Ù„Ù‡Ø¯Ù...")
        if df_processed is None or df_processed.empty:
            logger.debug(f"âš ï¸ [Strategy {self.symbol}] Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªÙ…Ø±Ø§Ø±ÙŠØ© Ø§Ù„Ù‡Ø¯Ù.")
            return None
        last_row = df_processed.iloc[-1]

        # Conditions for continuation (example: strong momentum) - Using signal timeframe indicators
        macd_hist_ok = pd.notna(last_row['macd_hist']) and last_row['macd_hist'] > 0.1 # Example: histogram still positive and strong
        adx_ok = pd.notna(last_row['adx']) and last_row['adx'] > MIN_ADX_FOR_DYNAMIC_UPDATE and pd.notna(last_row['di_plus']) and pd.notna(last_row['di_minus']) and last_row['di_plus'] > last_row['di_minus']
        rsi_ok = pd.notna(last_row['rsi']) and last_row['rsi'] < (RSI_OVERBOUGHT + 10) # Allow slightly higher RSI for continuation

        logger.debug(f"â„¹ï¸ [Strategy {self.symbol}] ØªØ­Ù‚Ù‚ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ: MACD Hist={last_row.get('macd_hist', np.nan):.4f} (Ù…Ù‚Ø¨ÙˆÙ„:{macd_hist_ok}), ADX={last_row.get('adx', np.nan):.1f} (Ù…Ù‚Ø¨ÙˆÙ„:{adx_ok}), RSI={last_row.get('rsi', np.nan):.1f} (Ù…Ù‚Ø¨ÙˆÙ„:{rsi_ok})")

        if macd_hist_ok and adx_ok and rsi_ok:
            current_atr = last_row.get('atr')
            if pd.notna(current_atr) and current_atr > 0:
                new_target = current_target + (current_atr * DYNAMIC_TARGET_EXTENSION_ATR_MULTIPLIER)
                logger.info(f"ğŸ¯ [Strategy {self.symbol}] Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø© Ø¹Ù„Ù‰ ØªÙ…Ø¯ÙŠØ¯ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ. Ø§Ù„Ù‚Ø¯ÙŠÙ…: {current_target:.6f}, Ø§Ù„Ø¬Ø¯ÙŠØ¯: {new_target:.6f}")
                return new_target
            else:
                logger.warning(f"âš ï¸ [Strategy {self.symbol}] ATR ØºÙŠØ± ØµØ§Ù„Ø­ Ù„ØªÙ…Ø¯ÙŠØ¯ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ.")
        else:
            logger.debug(f"â„¹ï¸ [Strategy {self.symbol}] Ø§Ù„Ø´Ø±ÙˆØ· ØºÙŠØ± Ù…Ø³ØªÙˆÙØ§Ø© Ù„ØªÙ…Ø¯ÙŠØ¯ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ.")
        return None

# ---------------------- Telegram Functions ----------------------
def send_telegram_message(target_chat_id: str, text: str, reply_markup: Optional[Dict] = None, parse_mode: str = 'Markdown', disable_web_page_preview: bool = True, timeout: int = 20) -> Optional[Dict]:
    """ÙŠØ±Ø³Ù„ Ø±Ø³Ø§Ù„Ø© Ø¥Ù„Ù‰ Telegram."""
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    payload = {'chat_id': str(target_chat_id), 'text': text, 'parse_mode': parse_mode, 'disable_web_page_preview': disable_web_page_preview}
    if reply_markup: payload['reply_markup'] = json.dumps(convert_np_values(reply_markup))
    logger.debug(f"â„¹ï¸ [Telegram] Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø¥Ù„Ù‰ {target_chat_id}...")
    try:
        response = requests.post(url, json=payload, timeout=timeout)
        response.raise_for_status()
        logger.info(f"âœ… [Telegram] ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø¥Ù„Ù‰ {target_chat_id}.")
        return response.json()
    except Exception as e: logger.error(f"âŒ [Telegram] ÙØ´Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø¥Ù„Ù‰ {target_chat_id}: {e}", exc_info=True); return None

def send_telegram_alert(signal_data: Dict[str, Any], timeframe: str) -> None:
    """ÙŠØ±Ø³Ù„ ØªÙ†Ø¨ÙŠÙ‡ Ø¥Ø´Ø§Ø±Ø© ØªØ¯Ø§ÙˆÙ„ Ø¥Ù„Ù‰ Telegram."""
    logger.debug(f"â„¹ï¸ [Telegram Alert] ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡ Ù„Ù€ {signal_data.get('symbol', 'N/A')}")
    try:
        entry_price = float(signal_data['entry_price']); target_price = float(signal_data['initial_target'])
        symbol = signal_data['symbol']; strategy_name = signal_data.get('strategy_name', 'N/A')
        signal_score = signal_data.get('r2_score', 0.0); total_possible_score = signal_data.get('total_possible_score', 10.0)
        volume_signal_tf = signal_data.get('volume_15m', 0.0); # Renamed key
        trade_value_signal = signal_data.get('trade_value', TRADE_VALUE)

        # Extract and format confirmation and entry point details
        confirmation_details = signal_data.get('signal_details', {}).get('Confirmation_Details', {})
        entry_point_details = signal_data.get('signal_details', {}).get('Entry_Point_Details_SignalTF', {}) # Renamed key

        confirmation_text = "\n".join([f"    - {k.replace('_', ' ').title()}: {v}" for k,v in confirmation_details.items()])
        entry_point_text = "\n".join([f"    - {k.replace('_', ' ').title()}: {v}" for k,v in entry_point_details.items()])

        # Construct a readable string from other signal_details for conditions met
        other_signal_details_text = "\n".join([f"  - {k.replace('_', ' ').title()}: {v}" for k,v in signal_data.get('signal_details', {}).items() if k not in ['Confirmation_Details', 'Entry_Point_Details_SignalTF'] and ('Ø§Ø¬ØªØ§Ø²' in str(v) or 'ÙØ´Ù„' not in str(v) or '+' in str(v) or '(0)' not in str(v))]) # Updated key


        profit_pct = ((target_price / entry_price) - 1) * 100 if entry_price > 0 else 0
        profit_usdt = trade_value_signal * (profit_pct / 100)

        generation_time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')


        safe_symbol = symbol.replace('_', '\\_').replace('*', '\\*').replace('[', '\\[').replace('`', '\\`')
        fear_greed = get_fear_greed_index(); btc_trend = get_btc_trend_4h()

        message = (
            f"ğŸ’¡ *Ø¥Ø´Ø§Ø±Ø© ØªØ¯Ø§ÙˆÙ„ Ø¬Ø¯ÙŠØ¯Ø© ({strategy_name.replace('_', ' ').title()})* ğŸ’¡\n"
            f"ğŸ•°ï¸ *ÙˆÙ‚Øª Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙˆØµÙŠØ©:* {generation_time_str}\n" # ÙˆÙ‚Øª Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙˆØµÙŠØ©
            f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n"
            f"ğŸª™ **Ø§Ù„Ø²ÙˆØ¬:** `{safe_symbol}`\n"
            f"ğŸ“ˆ **Ù†ÙˆØ¹ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©:** Ø´Ø±Ø§Ø¡ (Ø·ÙˆÙŠÙ„)\n"
            f"ğŸ•°ï¸ **Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù„Ø¥Ø´Ø§Ø±Ø©:** {timeframe}\n"
            f"ğŸ“Š **Ù‚ÙˆØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© (Ø§Ù„Ù†Ù‚Ø§Ø·):** *{signal_score:.1f} / {total_possible_score:.1f}*\n"
            f"ğŸ’§ **Ø§Ù„Ø³ÙŠÙˆÙ„Ø© ({timeframe}):** {volume_signal_tf:,.0f} USDT\n" # Updated volume interval
            f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n"
            f"â¡ï¸ **Ø³Ø¹Ø± Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø§Ù„Ù…Ù‚ØªØ±Ø­:** `${entry_price:,.8g}`\n"
            f"ğŸ¯ **Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø£ÙˆÙ„ÙŠ:** `${target_price:,.8g}` ({profit_pct:+.2f}% / â‰ˆ ${profit_usdt:+.2f})\n"
            f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n"
            f"âœ… *ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø²Ù…Ù†ÙŠ Ø§Ù„Ø£ÙƒØ¨Ø± ({CONFIRMATION_TIMEFRAME}):*\n" # Updated confirmation timeframe
            f"{confirmation_text if confirmation_text else '    - Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙØ§ØµÙŠÙ„ ØªØ£ÙƒÙŠØ¯ Ù…ØªØ§Ø­Ø©.'}\n"
            f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n"
            f"ğŸ“ *Ø¬ÙˆØ¯Ø© Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ ({SIGNAL_GENERATION_TIMEFRAME}):*\n" # Updated signal timeframe
            f"{entry_point_text if entry_point_text else '    - Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙØ§ØµÙŠÙ„ Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù…ØªØ§Ø­Ø©.'}\n"
            f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n"
            f"ğŸ“‹ *ØªÙØ§ØµÙŠÙ„ Ø´Ø±ÙˆØ· Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø£Ø®Ø±Ù‰:*\n"
            f"{other_signal_details_text if other_signal_details_text else '  - Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙØ§ØµÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠØ© Ù…ØªØ§Ø­Ø© Ù„Ù„Ø´Ø±ÙˆØ·.'}\n"
            f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n"
            f"ğŸ˜¨/ğŸ¤‘ **Ù…Ø¤Ø´Ø± Ø§Ù„Ø®ÙˆÙ ÙˆØ§Ù„Ø¬Ø´Ø¹:** {fear_greed}\n"
            f"â‚¿ **Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¨ÙŠØªÙƒÙˆÙŠÙ† (4 Ø³Ø§Ø¹Ø§Øª):** {btc_trend}\n"
            f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n"
            f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        )
        reply_markup = {"inline_keyboard": [[{"text": "ğŸ“Š Ø¹Ø±Ø¶ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡", "callback_data": "get_report"}]]}
        send_telegram_message(CHAT_ID, message, reply_markup=reply_markup, parse_mode='Markdown')
        logger.info(f"âœ… [Telegram Alert] ØªÙ… Ø¥Ø±Ø³Ø§Ù„ ØªÙ†Ø¨ÙŠÙ‡ Ø¥Ø´Ø§Ø±Ø© Ù„Ù€ {symbol}.")
    except Exception as e: logger.error(f"âŒ [Telegram Alert] ÙØ´Ù„ Ø¥Ø±Ø³Ø§Ù„ ØªÙ†Ø¨ÙŠÙ‡ Ù„Ù€ {signal_data.get('symbol', 'N/A')}: {e}", exc_info=True)

def format_duration(seconds: Optional[int]) -> str:
    """ÙŠÙ†Ø³Ù‚ Ø§Ù„Ù…Ø¯Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ù…Ù† Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ø³Ù‡Ù„ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©."""
    if seconds is None or seconds < 0:
        return "ØºÙŠØ± Ù…ØªØ§Ø­"
    delta = timedelta(seconds=seconds)
    hours, remainder = divmod(delta.total_seconds(), 3600)
    minutes, secs = divmod(remainder, 60)
    return f"{int(hours)} Ø³, {int(minutes)} Ø¯, {int(secs)} Ø«"

def send_tracking_notification(details: Dict[str, Any]) -> None:
    """ÙŠØ±Ø³Ù„ Ø¥Ø´Ø¹Ø§Ø± ØªØªØ¨Ø¹ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© (Ù…Ø«Ù„ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù‡Ø¯Ù)."""
    symbol = details.get('symbol', 'N/A'); signal_id = details.get('id', 'N/A')
    notification_type = details.get('type', 'unknown'); message = ""
    safe_symbol = symbol.replace('_', '\\_').replace('*', '\\*').replace('[', '\\[').replace('`', '\\`')
    closing_price = details.get('closing_price', 0.0); profit_pct = details.get('profit_pct', 0.0)
    time_to_target_str = format_duration(details.get('time_to_target_seconds'))

    logger.debug(f"â„¹ï¸ [Notification] ØªÙ†Ø³ÙŠÙ‚ Ø¥Ø´Ø¹Ø§Ø± Ø§Ù„ØªØªØ¨Ø¹: ID={signal_id}, Ø§Ù„Ù†ÙˆØ¹={notification_type}, Ø§Ù„Ø±Ù…Ø²={symbol}")

    if notification_type == 'target_hit':
        message = (
            f"âœ… *ØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù‡Ø¯Ù (ID: {signal_id})*\n"
            f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n"
            f"ğŸª™ **Ø§Ù„Ø²ÙˆØ¬:** `{safe_symbol}`\n"
            f"ğŸ¯ **Ø³Ø¹Ø± Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ (Ø§Ù„Ù‡Ø¯Ù):** `${closing_price:,.8g}`\n"
            f"ğŸ’° **Ø§Ù„Ø±Ø¨Ø­ Ø§Ù„Ù…Ø­Ù‚Ù‚:** {profit_pct:+.2f}%\n"
            f"â±ï¸ **Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø³ØªØºØ±Ù‚ Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù‡Ø¯Ù:** {time_to_target_str}" # Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø³ØªØºØ±Ù‚
        )
        logger.info(f"âœ… [Notification] ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø¹Ø§Ø± Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù‡Ø¯Ù Ù„Ù€ {symbol} (ID: {signal_id}).")
    elif notification_type == 'target_updated_dynamically':
        old_target = details.get('old_target', 0.0)
        new_target = details.get('new_target', 0.0)
        current_price = details.get('current_price', 0.0)
        message = (
            f"ğŸ”„ *ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‡Ø¯Ù Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ§Ù‹ (ID: {signal_id})*\n"
            f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n"
            f"ğŸª™ **Ø§Ù„Ø²ÙˆØ¬:** `{safe_symbol}`\n"
            f"ğŸ“ˆ **Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ:** `${current_price:,.8g}`\n"
            f"ğŸ“Š **Ø§Ù„ØªØ­Ù„ÙŠÙ„:** Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø§Ø±ØªÙØ§Ø¹ Ø§Ù„Ø³Ø¹Ø±\n"
            f"ğŸ¹ **Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ù‚Ø¯ÙŠÙ…:** `${old_target:,.8g}`\n"
            f"ğŸ¯ **Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø¬Ø¯ÙŠØ¯:** `${new_target:,.8g}`"
        )
        logger.info(f"ğŸ”„ [Notification] ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø¹Ø§Ø± ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ù„Ù€ {symbol} (ID: {signal_id}).")
    else: logger.warning(f"âš ï¸ [Notification] Ù†ÙˆØ¹ Ø¥Ø´Ø¹Ø§Ø± ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: {notification_type} Ù„Ù€ {details}"); return

    if message: send_telegram_message(CHAT_ID, message, parse_mode='Markdown')

# ---------------------- Database Functions (Insert and Update) ----------------------
def insert_signal_into_db(signal: Dict[str, Any]) -> bool:
    """ÙŠØ¯Ø®Ù„ Ø¥Ø´Ø§Ø±Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
    if not check_db_connection() or not conn:
        logger.error(f"âŒ [DB Insert] Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø¥Ø¯Ø±Ø§Ø¬ Ø¥Ø´Ø§Ø±Ø© {signal.get('symbol', 'N/A')}.")
        return False
    symbol = signal.get('symbol', 'N/A')
    logger.debug(f"â„¹ï¸ [DB Insert] Ø¥Ø¯Ø±Ø§Ø¬ Ø¥Ø´Ø§Ø±Ø© Ù„Ù€ {symbol}...")
    try:
        signal_prepared = convert_np_values(signal)
        signal_details_json = json.dumps(signal_prepared.get('signal_details', {}))
        with conn.cursor() as cur_ins:
            insert_query = sql.SQL("""
                INSERT INTO signals
                 (symbol, entry_price, initial_target, initial_stop_loss, current_target, current_stop_loss,
                 r2_score, strategy_name, signal_details, volume_15m, sent_at)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW());
            """) # sent_at is recommendation generation time
            cur_ins.execute(insert_query, (
                signal_prepared['symbol'], signal_prepared['entry_price'],
                signal_prepared['initial_target'], signal_prepared.get('initial_stop_loss', 0.0), # Default SL to 0.0
                signal_prepared['current_target'], signal_prepared.get('current_stop_loss', 0.0), # Default SL to 0.0
                signal_prepared.get('r2_score'), signal_prepared.get('strategy_name', 'unknown'),
                signal_details_json, signal_prepared.get('volume_15m') # Using the renamed key
            ))
        conn.commit()
        logger.info(f"âœ… [DB Insert] ØªÙ… Ø¥Ø¯Ø±Ø§Ø¬ Ø¥Ø´Ø§Ø±Ø© Ù„Ù€ {symbol} (Ø§Ù„Ù†Ù‚Ø§Ø·: {signal_prepared.get('r2_score')}).")
        return True
    except psycopg2.Error as db_err: # Specific Psycopg2 error
        logger.error(f"âŒ [DB Insert] Ø®Ø·Ø£ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø«Ù†Ø§Ø¡ Ø¥Ø¯Ø±Ø§Ø¬ Ø¥Ø´Ø§Ø±Ø© Ù„Ù€ {symbol}: {db_err}")
        if conn:
            conn.rollback()
        return False
    except Exception as e: # General exception
        logger.error(f"âŒ [DB Insert] Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ø¯Ø±Ø§Ø¬ Ø¥Ø´Ø§Ø±Ø© Ù„Ù€ {symbol}: {e}", exc_info=True)
        if conn:
            conn.rollback()
        return False

# ---------------------- Open Signal Tracking Function ----------------------
def track_signals() -> None:
    """ÙŠØªØªØ¨Ø¹ Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙØªÙˆØ­Ø© ÙˆÙŠØ­Ø¯Ø« Ø­Ø§Ù„ØªÙ‡Ø§ (Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù‡Ø¯ÙØŒ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‡Ø¯Ù)."""
    logger.info("â„¹ï¸ [Tracker] Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© ØªØªØ¨Ø¹ Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙØªÙˆØ­Ø©...")
    while True:
        active_signals_summary: List[str] = []
        processed_in_cycle = 0
        try:
            if not check_db_connection() or not conn:
                logger.warning("âš ï¸ [Tracker] ØªØ®Ø·ÙŠ Ø§Ù„ØªØªØ¨Ø¹: Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."); time.sleep(15); continue

            with conn.cursor() as track_cur: # Uses RealDictCursor
                 track_cur.execute("""
                    SELECT id, symbol, entry_price, current_target, sent_at, dynamic_updates_count
                    FROM signals
                    WHERE achieved_target = FALSE AND hit_stop_loss = FALSE;
                """) # Removed stop loss fields from select
                 open_signals: List[Dict] = track_cur.fetchall()

            if not open_signals:
                logger.debug("â„¹ï¸ [Tracker] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¥Ø´Ø§Ø±Ø§Øª Ù…ÙØªÙˆØ­Ø© Ù„Ù„ØªØªØ¨Ø¹.")
                time.sleep(10); continue # Wait less if no signals

            logger.debug(f"â„¹ï¸ [Tracker] ØªØªØ¨Ø¹ {len(open_signals)} Ø¥Ø´Ø§Ø±Ø© Ù…ÙØªÙˆØ­Ø©...")

            for signal_row in open_signals:
                signal_id = signal_row['id']; symbol = signal_row['symbol']; processed_in_cycle += 1
                update_executed = False # To track if this signal was updated in the current cycle
                try:
                    entry_price = float(signal_row['entry_price'])
                    current_target = float(signal_row['current_target'])
                    sent_at_timestamp = signal_row['sent_at'] # Recommendation generation time
                    dynamic_updates_count = signal_row.get('dynamic_updates_count', 0)

                    current_price = ticker_data.get(symbol)
                    if current_price is None:
                        logger.warning(f"âš ï¸ [Tracker] {symbol}(ID:{signal_id}): Ø§Ù„Ø³Ø¹Ø± ØºÙŠØ± Ù…ØªÙˆÙØ± ÙÙŠ Ø§Ù„ØªÙŠÙƒØ±.");
                        continue # Skip if price not available

                    active_signals_summary.append(f"{symbol}({signal_id}): P={current_price:.4f} T={current_target:.4f} DynUpd={dynamic_updates_count}")
                    logger.debug(f"â„¹ï¸ [Tracker] Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© ID:{signal_id}, Ø§Ù„Ø±Ù…Ø²:{symbol}, Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ:{current_price:.4f}, Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø­Ø§Ù„ÙŠ:{current_target:.4f}")


                    update_query: Optional[sql.SQL] = None; update_params: Tuple = (); log_message: Optional[str] = None
                    notification_details: Dict[str, Any] = {'symbol': symbol, 'id': signal_id, 'current_price': current_price}

                    # 1. Check for Target Hit
                    if current_price >= current_target:
                        profit_pct = ((current_target / entry_price) - 1) * 100 if entry_price > 0 else 0
                        closed_at_time = datetime.now()
                        time_to_target_delta = closed_at_time - sent_at_timestamp
                        time_to_target_seconds = int(time_to_target_delta.total_seconds())

                        update_query = sql.SQL("""
                            UPDATE signals SET achieved_target = TRUE, closing_price = %s,
                                         closed_at = %s, profit_percentage = %s,
                                         time_to_target_seconds = %s
                            WHERE id = %s;
                        """)
                        update_params = (current_target, closed_at_time, profit_pct, time_to_target_seconds, signal_id)
                        log_message = f"ğŸ¯ [Tracker] {symbol}(ID:{signal_id}): ØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù‡Ø¯Ù Ø¹Ù†Ø¯ {current_target:.8g} (Ø§Ù„Ø±Ø¨Ø­: {profit_pct:+.2f}%). Ø§Ù„ÙˆÙ‚Øª: {format_duration(time_to_target_seconds)}."
                        notification_details.update({'type': 'target_hit', 'closing_price': current_target, 'profit_pct': profit_pct, 'time_to_target_seconds': time_to_target_seconds})
                        update_executed = True
                        logger.info(f"ğŸ¯ [Tracker] ØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù‡Ø¯Ù Ù„Ù€ {symbol} (ID: {signal_id}).")

                    # 3. Dynamic Target Update (Only if Target not hit and updates allowed)
                    elif not update_executed and dynamic_updates_count < MAX_DYNAMIC_TARGET_UPDATES and \
                         current_price >= (current_target * (1 - DYNAMIC_TARGET_APPROACH_PCT)) and \
                         current_price < current_target: # Price is near target but hasn't hit it

                        logger.info(f"ğŸ” [Tracker] {symbol}(ID:{signal_id}): Ø§Ù„Ø³Ø¹Ø± Ù‚Ø±ÙŠØ¨ Ù…Ù† Ø§Ù„Ù‡Ø¯Ù. Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‚ÙŠÙŠÙ… Ù„Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ (Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø±Ù‚Ù… {dynamic_updates_count + 1}).")
                        # Fetch data for the signal tracking timeframe (15m) for dynamic update analysis
                        df_dynamic = fetch_historical_data(symbol, interval=SIGNAL_TRACKING_TIMEFRAME, days=SIGNAL_TRACKING_LOOKBACK_DAYS)
                        if df_dynamic is not None and not df_dynamic.empty:
                            strategy = ScalpingTradingStrategy(symbol) # Re-initialize strategy to use its methods
                            # Populate indicators for the signal tracking timeframe (15m)
                            df_indicators_dynamic = strategy.populate_indicators(df_dynamic, SIGNAL_TRACKING_TIMEFRAME)
                            if df_indicators_dynamic is not None and not df_indicators_dynamic.empty:
                                new_dynamic_target = strategy.analyze_target_continuation(df_indicators_dynamic, current_price, current_target)
                                if new_dynamic_target and new_dynamic_target > current_target:
                                    update_query = sql.SQL("""
                                        UPDATE signals SET current_target = %s,
                                                       dynamic_updates_count = dynamic_updates_count + 1,
                                                       signal_details = signal_details || %s::jsonb
                                        WHERE id = %s;
                                    """)
                                    update_details_json = json.dumps({
                                        f"dynamic_update_{dynamic_updates_count+1}": {
                                            "timestamp": str(datetime.now()),
                                            "old_target": current_target,
                                            "new_target": new_dynamic_target,
                                            "price_at_update": current_price
                                        }
                                    })
                                    update_params = (new_dynamic_target, update_details_json, signal_id)
                                    log_message = f"ğŸ”„ [Tracker] {symbol}(ID:{signal_id}): ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ! Ø§Ù„Ù‚Ø¯ÙŠÙ…: {current_target:.6f}, Ø§Ù„Ø¬Ø¯ÙŠØ¯: {new_target:.6f}"
                                    notification_details.update({'type': 'target_updated_dynamically',
                                                                 'old_target': current_target,
                                                                 'new_target': new_dynamic_target,
                                                                 'current_price': current_price})
                                    update_executed = True
                                    logger.info(f"ğŸ”„ [Tracker] ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‡Ø¯Ù Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠÙ‹Ø§ Ù„Ù€ {symbol} (ID: {signal_id}).")
                                else:
                                    logger.info(f"â„¹ï¸ [Tracker] {symbol}(ID:{signal_id}): Ø§Ù„Ø´Ø±ÙˆØ· ØºÙŠØ± Ù…Ø³ØªÙˆÙØ§Ø© Ù„ØªÙ…Ø¯ÙŠØ¯ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ø£Ùˆ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù„ÙŠØ³ Ø£Ø¹Ù„Ù‰.")
                            else:
                                logger.warning(f"âš ï¸ [Tracker] {symbol}(ID:{signal_id}): ØªØ¹Ø°Ø± Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ù„Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ.")
                        else:
                            logger.warning(f"âš ï¸ [Tracker] {symbol}(ID:{signal_id}): ØªØ¹Ø°Ø± Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ.")

                    if update_executed and update_query:
                        try:
                             with conn.cursor() as update_cur:
                                 update_cur.execute(update_query, update_params)
                             conn.commit()
                             if log_message:
                                 logger.info(log_message)
                             if notification_details.get('type'):
                                 send_tracking_notification(notification_details)
                        except psycopg2.Error as db_err:
                            logger.error(f"âŒ [Tracker] {symbol}(ID:{signal_id}): Ø®Ø·Ø£ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ø¯ÙŠØ«: {db_err}")
                            if conn:
                                conn.rollback()
                        except Exception as exec_err:
                            logger.error(f"âŒ [Tracker] {symbol}(ID:{signal_id}): Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ø¯ÙŠØ«/Ø§Ù„Ø¥Ø´Ø¹Ø§Ø±: {exec_err}", exc_info=True)
                            if conn:
                                conn.rollback()
                except Exception as inner_loop_err: logger.error(f"âŒ [Tracker] {symbol}(ID:{signal_id}): Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø´Ø§Ø±Ø©: {inner_loop_err}", exc_info=True)

            if active_signals_summary: logger.debug(f"â„¹ï¸ [Tracker] Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¯ÙˆØ±Ø© ({processed_in_cycle} ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§): {'; '.join(active_signals_summary)}")
            time.sleep(5) # Increased wait time between tracking cycles
        except psycopg2.Error as db_cycle_err:
            logger.error(f"âŒ [Tracker] Ø®Ø·Ø£ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø¯ÙˆØ±Ø© Ø§Ù„ØªØªØ¨Ø¹: {db_cycle_err}. Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§ØªØµØ§Ù„...")
            if conn:
                conn.rollback()
            time.sleep(30)
            check_db_connection()
        except Exception as cycle_err:
            logger.error(f"âŒ [Tracker] Ø®Ø·Ø£ ÙÙŠ Ø¯ÙˆØ±Ø© Ø§Ù„ØªØªØ¨Ø¹: {cycle_err}", exc_info=True)
            time.sleep(30)

def get_interval_minutes(interval: str) -> int:
    """ÙŠØ­ÙˆÙ„ Ø§Ù„ÙØ§ØµÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ Ù…Ù† Ø³Ù„Ø³Ù„Ø© Ù†ØµÙŠØ© Ø¥Ù„Ù‰ Ø¯Ù‚Ø§Ø¦Ù‚."""
    if interval.endswith('m'): return int(interval[:-1])
    elif interval.endswith('h'): return int(interval[:-1]) * 60
    elif interval.endswith('d'): return int(interval[:-1]) * 60 * 24
    return 0

# ---------------------- Flask Service (Optional for Webhook) ----------------------
app = Flask(__name__)
@app.route('/')
def home() -> Response:
    """ØµÙØ­Ø© Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©."""
    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    ws_alive = ws_thread.is_alive() if 'ws_thread' in globals() and ws_thread else False
    tracker_alive = tracker_thread.is_alive() if 'tracker_thread' in globals() and tracker_thread else False
    status = "ÙŠØ¹Ù…Ù„" if ws_alive and tracker_alive else "ÙŠØ¹Ù…Ù„ Ø¬Ø²Ø¦ÙŠÙ‹Ø§"
    return Response(f"ğŸ“ˆ Crypto Signal Bot ({status}) - Ø¢Ø®Ø± ØªØ­Ù‚Ù‚: {now}", status=200, mimetype='text/plain')

@app.route('/favicon.ico')
def favicon() -> Response: return Response(status=204) # No Content

@app.route('/webhook', methods=['POST'])
def webhook() -> Tuple[str, int]:
    """Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© Webhook Ù„Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ ØªØ­Ø¯ÙŠØ«Ø§Øª Telegram."""
    if not request.is_json: logger.warning("âš ï¸ [Flask] Ø·Ù„Ø¨ Webhook Ù„ÙŠØ³ Ø¨ØµÙŠØºØ© JSON."); return "Invalid request", 400
    try:
        data = request.get_json(); logger.debug(f"â„¹ï¸ [Flask] Ø¨ÙŠØ§Ù†Ø§Øª Webhook: {json.dumps(data)[:200]}...")
        if 'callback_query' in data:
            callback_query = data['callback_query']; callback_id = callback_query['id']
            callback_data = callback_query.get('data'); message_info = callback_query.get('message')
            if not message_info or not callback_data: logger.warning(f"âš ï¸ [Flask] Callback {callback_id} ÙŠÙØªÙ‚Ø¯ Ø§Ù„Ø±Ø³Ø§Ù„Ø©/Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."); return "OK", 200
            chat_id_callback = message_info.get('chat', {}).get('id')
            if not chat_id_callback: logger.warning(f"âš ï¸ [Flask] Callback {callback_id} ÙŠÙØªÙ‚Ø¯ Ù…Ø¹Ø±Ù Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©."); return "OK", 200
            try: requests.post(f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/answerCallbackQuery", json={'callback_query_id': callback_id}, timeout=5)
            except Exception as ack_err: logger.warning(f"âš ï¸ [Flask] ÙØ´Ù„ ØªØ£ÙƒÙŠØ¯ callback {callback_id}: {ack_err}")
            if callback_data == "get_report":
                Thread(target=lambda: send_telegram_message(chat_id_callback, generate_performance_report(), parse_mode='Markdown')).start()
                logger.info(f"â„¹ï¸ [Flask] ØªÙ… ØªØ´ØºÙŠÙ„ Ø£Ù…Ø± '/report' Ù…Ù† callback query Ù„Ù€ {chat_id_callback}.")
        elif 'message' in data:
            message_data = data['message']; chat_info = message_data.get('chat'); text_msg = message_data.get('text', '').strip()
            if not chat_info or not text_msg: logger.debug("â„¹ï¸ [Flask] Ø±Ø³Ø§Ù„Ø© Ø¨Ø¯ÙˆÙ† Ø¯Ø±Ø¯Ø´Ø©/Ù†Øµ."); return "OK", 200
            chat_id_msg = chat_info['id']
            if text_msg.lower() == '/report':
                Thread(target=lambda: send_telegram_message(chat_id_msg, generate_performance_report(), parse_mode='Markdown')).start()
                logger.info(f"â„¹ï¸ [Flask] ØªÙ… ØªØ´ØºÙŠÙ„ Ø£Ù…Ø± '/report' Ù…Ù† Ø±Ø³Ø§Ù„Ø© Ù„Ù€ {chat_id_msg}.")
            elif text_msg.lower() == '/status':
                Thread(target=handle_status_command, args=(chat_id_msg,)).start()
                logger.info(f"â„¹ï¸ [Flask] ØªÙ… ØªØ´ØºÙŠÙ„ Ø£Ù…Ø± '/status' Ù…Ù† Ø±Ø³Ø§Ù„Ø© Ù„Ù€ {chat_id_msg}.")
            else:
                logger.debug(f"â„¹ï¸ [Flask] Ø±Ø³Ø§Ù„Ø© ØºÙŠØ± Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù† {chat_id_msg}: '{text_msg}'")
        return "OK", 200
    except Exception as e: logger.error(f"âŒ [Flask] Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Webhook: {e}", exc_info=True); return "Error", 500

def handle_status_command(chat_id_msg: int) -> None:
    """ÙŠØ¹Ø§Ù„Ø¬ Ø£Ù…Ø± /status Ù…Ù† Telegram."""
    logger.info(f"â„¹ï¸ [Flask Status] Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ù…Ø± /status Ù„Ù„Ø¯Ø±Ø¯Ø´Ø© {chat_id_msg}")
    status_msg = "â³ Ø¬Ù„Ø¨ Ø§Ù„Ø­Ø§Ù„Ø©..."
    msg_sent = send_telegram_message(chat_id_msg, status_msg)
    if not (msg_sent and msg_sent.get('ok')): logger.error(f"âŒ [Flask Status] ÙØ´Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ø¥Ù„Ù‰ {chat_id_msg}"); return
    message_id_to_edit = msg_sent['result']['message_id'] if msg_sent and msg_sent.get('result') else None
    if not message_id_to_edit: logger.error(f"âŒ [Flask Status] Ù„Ø§ ÙŠÙˆØ¬Ø¯ message_id Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø© ÙÙŠ {chat_id_msg}"); return

    try:
        open_count = 0
        if check_db_connection() and conn:
            with conn.cursor() as status_cur:
                status_cur.execute("SELECT COUNT(*) AS count FROM signals WHERE achieved_target = FALSE AND hit_stop_loss = FALSE;")
                open_count = (status_cur.fetchone() or {}).get('count', 0)
        ws_status = 'Ù†Ø´Ø· âœ…' if 'ws_thread' in globals() and ws_thread and ws_thread.is_alive() else 'ØºÙŠØ± Ù†Ø´Ø· âŒ'
        tracker_status = 'Ù†Ø´Ø· âœ…' if 'tracker_thread' in globals() and tracker_thread and tracker_thread.is_alive() else 'ØºÙŠØ± Ù†Ø´Ø· âŒ'
        final_status_msg = (
            f"ğŸ¤– *Ø­Ø§Ù„Ø© Ø§Ù„Ø¨ÙˆØª:*\n"
            f"- ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± (WS): {ws_status}\n"
            f"- ØªØªØ¨Ø¹ Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª: {tracker_status}\n"
            f"- Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø©: *{open_count}* / {MAX_OPEN_TRADES}\n"
            f"- ÙˆÙ‚Øª Ø§Ù„Ø®Ø§Ø¯Ù…: {datetime.now().strftime('%H:%M:%S')}"
        )
        edit_url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/editMessageText"
        edit_payload = {'chat_id': chat_id_msg, 'message_id': message_id_to_edit, 'text': final_status_msg, 'parse_mode': 'Markdown'}
        requests.post(edit_url, json=edit_payload, timeout=10).raise_for_status()
        logger.info(f"âœ… [Flask Status] ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø© Ù„Ù„Ø¯Ø±Ø¯Ø´Ø© {chat_id_msg}")
    except Exception as status_err:
        logger.error(f"âŒ [Flask Status] Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨/ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„Ø© Ù„Ù€ {chat_id_msg}: {status_err}", exc_info=True)
        send_telegram_message(chat_id_msg, "âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø­Ø§Ù„Ø©.")


def run_flask() -> None:
    """ÙŠØ´ØºÙ„ Ø®Ø§Ø¯Ù… Flask Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Webhooks."""
    if not WEBHOOK_URL:
        logger.info("â„¹ï¸ [Flask] Ø¹Ù†ÙˆØ§Ù† Webhook ØºÙŠØ± Ù…ÙƒÙˆÙ‘Ù†. Ù„Ù† ÙŠØªÙ… ØªØ´ØºÙŠÙ„ Flask.")
        return
    host = "0.0.0.0"; port = int(config('PORT', default=10000))
    logger.info(f"â„¹ï¸ [Flask] Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ ØªØ·Ø¨ÙŠÙ‚ Flask Ø¹Ù„Ù‰ {host}:{port}...")
    try:
        from waitress import serve
        logger.info("âœ… [Flask] Ø§Ø³ØªØ®Ø¯Ø§Ù… 'waitress'.")
        serve(app, host=host, port=port, threads=6)
    except ImportError:
        logger.warning("âš ï¸ [Flask] 'waitress' ØºÙŠØ± Ù…Ø«Ø¨Øª. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø§Ø¯Ù… ØªØ·ÙˆÙŠØ± Flask.")
        app.run(host=host, port=port)
    except Exception as serve_err:
        logger.critical(f"âŒ [Flask] ÙØ´Ù„ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù…: {serve_err}", exc_info=True)


# ---------------------- Main Loop and Check Function ----------------------
def main_loop() -> None:
    """Ø§Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù…Ø³Ø­ Ø§Ù„Ø³ÙˆÙ‚ ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª."""
    symbols_to_scan = get_crypto_symbols()
    if not symbols_to_scan: logger.critical("âŒ [Main] Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø±Ù…ÙˆØ² ØµØ§Ù„Ø­Ø©. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©."); return
    logger.info(f"âœ… [Main] ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(symbols_to_scan)} Ø±Ù…Ø²Ù‹Ø§ Ù„Ù„Ù…Ø³Ø­.")

    while True:
        try:
            scan_start_time = time.time()
            logger.info(f"ğŸ”„ [Main] Ø¨Ø¯Ø¡ Ø¯ÙˆØ±Ø© Ù…Ø³Ø­ Ø§Ù„Ø³ÙˆÙ‚ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            if not check_db_connection() or not conn:
                logger.error("âŒ [Main] ÙØ´Ù„ Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. ØªØ®Ø·ÙŠ Ø§Ù„Ù…Ø³Ø­.")
                time.sleep(60)
                continue

            open_count = 0
            try:
                 with conn.cursor() as cur_check:
                    cur_check.execute("SELECT COUNT(*) AS count FROM signals WHERE achieved_target = FALSE AND hit_stop_loss = FALSE;")
                    open_count = (cur_check.fetchone() or {}).get('count', 0)
            except psycopg2.Error as db_err:
                logger.error(f"âŒ [Main] Ø®Ø·Ø£ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙØªÙˆØ­Ø©: {db_err}. ØªØ®Ø·ÙŠ.")
                if conn:
                    conn.rollback()
                time.sleep(60)
                continue

            logger.info(f"â„¹ï¸ [Main] Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙØªÙˆØ­Ø©: {open_count} / {MAX_OPEN_TRADES}")
            if open_count >= MAX_OPEN_TRADES:
                logger.info(f"âš ï¸ [Main] ØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙØªÙˆØ­Ø©. Ø§Ù†ØªØ¸Ø§Ø±...")
                time.sleep(get_interval_minutes(SIGNAL_GENERATION_TIMEFRAME) * 60) # Wait for the signal timeframe duration
                continue

            processed_in_loop = 0; signals_generated_in_loop = 0; slots_available = MAX_OPEN_TRADES - open_count
            logger.info(f"â„¹ï¸ [Main] ÙØªØ­Ø§Øª Ù…ØªØ§Ø­Ø© Ù„Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©: {slots_available}")

            for symbol in symbols_to_scan:
                 if slots_available <= 0:
                     logger.info(f"â„¹ï¸ [Main] ØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³Ø­. Ø§Ù„ØªÙˆÙ‚Ù.")
                     break
                 processed_in_loop += 1
                 logger.debug(f"ğŸ” [Main] Ù…Ø³Ø­ Ø§Ù„Ø±Ù…Ø² {symbol} ({processed_in_loop}/{len(symbols_to_scan)})...")
                 try:
                    with conn.cursor() as symbol_cur: # Check for existing open signal for symbol
                        symbol_cur.execute("SELECT 1 FROM signals WHERE symbol = %s AND achieved_target = FALSE AND hit_stop_loss = FALSE LIMIT 1;", (symbol,))
                        if symbol_cur.fetchone():
                            logger.debug(f"â„¹ï¸ [Main] ØªÙˆØ¬Ø¯ Ø¥Ø´Ø§Ø±Ø© Ù…ÙØªÙˆØ­Ø© Ø¨Ø§Ù„ÙØ¹Ù„ Ù„Ù€ {symbol}. ØªØ®Ø·ÙŠ.")
                            continue
                    logger.debug(f"â„¹ï¸ [Main] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¥Ø´Ø§Ø±Ø© Ù…ÙØªÙˆØ­Ø© Ù„Ù€ {symbol}. Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø¨Ø§Ù„ØªØ­Ù„ÙŠÙ„.")

                    # Fetch and process data for the signal timeframe (15m)
                    df_hist_signal_tf = fetch_historical_data(symbol, interval=SIGNAL_GENERATION_TIMEFRAME, days=SIGNAL_GENERATION_LOOKBACK_DAYS)
                    if df_hist_signal_tf is None or df_hist_signal_tf.empty:
                        logger.debug(f"âš ï¸ [Main] ØªØ¹Ø°Ø± Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø·Ø§Ø± {SIGNAL_GENERATION_TIMEFRAME} Ù„Ù€ {symbol}. ØªØ®Ø·ÙŠ.")
                        continue

                    strategy = ScalpingTradingStrategy(symbol)
                    # Populate indicators for the signal timeframe (15m)
                    df_indicators_signal_tf = strategy.populate_indicators(df_hist_signal_tf, SIGNAL_GENERATION_TIMEFRAME)
                    if df_indicators_signal_tf is None:
                        logger.debug(f"âš ï¸ [Main] ØªØ¹Ø°Ø± Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø±Ø§Øª Ø¥Ø·Ø§Ø± {SIGNAL_GENERATION_TIMEFRAME} Ù„Ù€ {symbol}. ØªØ®Ø·ÙŠ.")
                        continue

                    # Generate potential signal (which now includes the multi-TF and entry point checks internally)
                    potential_signal = strategy.generate_buy_signal(df_indicators_signal_tf)

                    if potential_signal:
                        logger.info(f"âœ¨ [Main] Ø¥Ø´Ø§Ø±Ø© Ù…Ø­ØªÙ…Ù„Ø© Ù„Ù€ {symbol}! (Ø§Ù„Ù†Ù‚Ø§Ø·: {potential_signal.get('r2_score', 0):.2f})")
                        with conn.cursor() as final_check_cur: # Final check on open slots before inserting
                             final_check_cur.execute("SELECT COUNT(*) AS count FROM signals WHERE achieved_target = FALSE AND hit_stop_loss = FALSE;")
                             final_open_count = (final_check_cur.fetchone() or {}).get('count', 0)
                             if final_open_count < MAX_OPEN_TRADES:
                                 logger.info(f"â„¹ï¸ [Main] ÙØªØ­Ø© Ù…ØªØ§Ø­Ø© ({final_open_count}/{MAX_OPEN_TRADES}). Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ù€ {symbol}.")
                                 if insert_signal_into_db(potential_signal):
                                     send_telegram_alert(potential_signal, SIGNAL_GENERATION_TIMEFRAME)
                                     signals_generated_in_loop += 1
                                     slots_available -= 1
                                     logger.info(f"âœ… [Main] ØªÙ… Ø¥Ø¯Ø±Ø§Ø¬ ÙˆØ¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø§Ø±Ø© Ù„Ù€ {symbol}. Ø§Ù„ÙØªØ­Ø§Øª Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ©: {slots_available}.")
                                     time.sleep(2) # Small delay after sending alert
                                 else:
                                     logger.error(f"âŒ [Main] ÙØ´Ù„ Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ù€ {symbol}.")
                             else:
                                 logger.warning(f"âš ï¸ [Main] ØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù‚Ø¨Ù„ Ø¥Ø¯Ø±Ø§Ø¬ {symbol}. ØªÙ… ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©.")
                                 break # Stop scanning if max open trades is reached during the loop
                    else:
                        logger.debug(f"â„¹ï¸ [Main] Ù„Ù… ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø¥Ø´Ø§Ø±Ø© Ù„Ù€ {symbol} ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ±Ø©.")

                 except psycopg2.Error as db_loop_err:
                     logger.error(f"âŒ [Main] Ø®Ø·Ø£ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {symbol}: {db_loop_err}. Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ù„Ù„Ø±Ù…Ø² Ø§Ù„ØªØ§Ù„ÙŠ...")
                     if conn:
                         conn.rollback()
                     continue
                 except Exception as symbol_proc_err:
                     logger.error(f"âŒ [Main] Ø®Ø·Ø£ Ø¹Ø§Ù… Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ù…Ø² {symbol}: {symbol_proc_err}", exc_info=True)
                     continue
                 time.sleep(0.1) # Small delay between processing symbols

            scan_duration = time.time() - scan_start_time
            logger.info(f"ğŸ [Main] Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„Ù…Ø³Ø­. Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©: {signals_generated_in_loop}. Ø§Ù„Ù…Ø¯Ø©: {scan_duration:.2f} Ø«Ø§Ù†ÙŠØ©.")
            frame_minutes = get_interval_minutes(SIGNAL_GENERATION_TIMEFRAME)
            # Wait until the next candle close for the signal timeframe, or at least 2 minutes
            wait_time = max(frame_minutes * 60 - (scan_duration % (frame_minutes * 60)), 120 - scan_duration) if scan_duration < 120 else frame_minutes * 60
            logger.info(f"â³ [Main] Ø§Ù†ØªØ¸Ø§Ø± {wait_time:.1f} Ø«Ø§Ù†ÙŠØ© Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©...")
            time.sleep(wait_time)
        except KeyboardInterrupt:
            logger.info("ğŸ›‘ [Main] Ø·Ù„Ø¨ Ø¥ÙŠÙ‚Ø§Ù. Ø¥ØºÙ„Ø§Ù‚...")
            break
        except psycopg2.Error as db_main_err:
            logger.error(f"âŒ [Main] Ø®Ø·Ø£ ÙØ§Ø¯Ø­ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {db_main_err}. Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§ØªØµØ§Ù„...")
            if conn:
                conn.rollback()
            time.sleep(60)
            try:
                init_db()
            except Exception as recon_err:
                logger.critical(f"âŒ [Main] ÙØ´Ù„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {recon_err}. Ø§Ù„Ø®Ø±ÙˆØ¬...")
                break
        except Exception as main_err:
            logger.error(f"âŒ [Main] Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ Ø§Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {main_err}", exc_info=True)
            logger.info("â„¹ï¸ [Main] Ø§Ù†ØªØ¸Ø§Ø± 120 Ø«Ø§Ù†ÙŠØ© Ù‚Ø¨Ù„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©...")
            time.sleep(120)


def cleanup_resources() -> None:
    """ÙŠÙ†Ø¸Ù Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ù‚Ø¨Ù„ Ø§Ù„Ø®Ø±ÙˆØ¬."""
    global conn
    logger.info("â„¹ï¸ [Cleanup] Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯...")
    if conn:
        try:
            conn.close()
            logger.info("âœ… [DB] ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        except Exception as close_err:
            logger.error(f"âš ï¸ [DB] Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥ØºÙ„Ø§Ù‚ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {close_err}")
    logger.info("âœ… [Cleanup] Ø§ÙƒØªÙ…Ù„ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯.")

# ---------------------- Main Entry Point ----------------------
if __name__ == "__main__":
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø¨ÙˆØª Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„ØªØ¯Ø§ÙˆÙ„ (Ø¥ØµØ¯Ø§Ø± Ø¨Ø¯ÙˆÙ† ÙˆÙ‚Ù Ø®Ø³Ø§Ø±Ø©)...")
    logger.info(f"Ù…Ø­Ù„ÙŠ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | UTC: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}")
    ws_thread: Optional[Thread] = None
    tracker_thread: Optional[Thread] = None
    flask_thread: Optional[Thread] = None
    try:
        init_db()
        ws_thread = Thread(target=run_ticker_socket_manager, daemon=True, name="WebSocketThread")
        ws_thread.start()
        logger.info("âœ… [Main] ØªÙ… Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø®ÙŠØ· WebSocket Ticker. Ø§Ù†ØªØ¸Ø§Ø± 5 Ø«ÙˆØ§Ù†Ù Ù„Ù„ØªÙ‡ÙŠØ¦Ø©...")
        time.sleep(5)
        if not ticker_data:
            logger.warning("âš ï¸ [Main] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙˆÙ„ÙŠØ© Ù…Ù† WebSocket Ø¨Ø¹Ø¯ 5 Ø«ÙˆØ§Ù†Ù.")
        else:
            logger.info(f"âœ… [Main] Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙˆÙ„ÙŠØ© Ù…Ù† WebSocket Ù„Ù€ {len(ticker_data)} Ø±Ù…Ø²Ù‹Ø§.")
        tracker_thread = Thread(target=track_signals, daemon=True, name="TrackerThread")
        tracker_thread.start()
        logger.info("âœ… [Main] ØªÙ… Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø®ÙŠØ· Signal Tracker.")
        if WEBHOOK_URL:
            flask_thread = Thread(target=run_flask, daemon=True, name="FlaskThread")
            flask_thread.start()
            logger.info("âœ… [Main] ØªÙ… Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø®ÙŠØ· Flask Webhook.")
        else:
            logger.info("â„¹ï¸ [Main] Ø¹Ù†ÙˆØ§Ù† Webhook ØºÙŠØ± Ù…ÙƒÙˆÙ‘Ù†ØŒ Ù„Ù† ÙŠØªÙ… ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… Flask.")
        main_loop()
    except Exception as startup_err:
        logger.critical(f"âŒ [Main] Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„/Ø§Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {startup_err}", exc_info=True)
    finally:
        logger.info("ğŸ›‘ [Main] Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬...")
        cleanup_resources()
        logger.info("ğŸ‘‹ [Main] ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø¨ÙˆØª Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„ØªØ¯Ø§ÙˆÙ„.")
        os._exit(0) # Force exit if threads are stuck
