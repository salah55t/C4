# --- ÿßŸÑÿßÿ≥ÿ™Ÿäÿ±ÿßÿØÿßÿ™ ŸàÿßŸÑÿ•ÿπÿØÿßÿØ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿä ---
import time
import os
import json
import logging
import requests
import numpy as np
import pandas as pd
import psycopg2
import pickle
import redis
from urllib.parse import urlparse
from psycopg2 import sql, OperationalError, InterfaceError
from psycopg2.extras import RealDictCursor
from binance.client import Client
from binance import ThreadedWebsocketManager
from binance.exceptions import BinanceAPIException
from flask import Flask, request, Response, jsonify, render_template_string
from flask_cors import CORS
from threading import Thread, Lock
from datetime import datetime, timedelta, timezone
from decouple import config
from typing import List, Dict, Optional, Any, Set
from sklearn.preprocessing import StandardScaler
from collections import deque
import warnings
import gc

# ÿ™ÿ¨ÿßŸáŸÑ ÿßŸÑÿ™ÿ≠ÿ∞Ÿäÿ±ÿßÿ™ ÿ∫Ÿäÿ± ÿßŸÑŸáÿßŸÖÿ©
warnings.simplefilter(action='ignore', category=FutureWarning)

# --- ÿ•ÿπÿØÿßÿØ ÿßŸÑÿ™ÿ≥ÿ¨ŸäŸÑ ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crypto_bot_v7_with_ichimoku.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('CryptoBotV7_With_Ichimoku')

# --- ÿ™ÿ≠ŸÖŸäŸÑ ŸÖÿ™ÿ∫Ÿäÿ±ÿßÿ™ ÿßŸÑÿ®Ÿäÿ¶ÿ© ---
try:
    API_KEY: str = config('BINANCE_API_KEY')
    API_SECRET: str = config('BINANCE_API_SECRET')
    TELEGRAM_TOKEN: str = config('TELEGRAM_BOT_TOKEN')
    CHAT_ID: str = config('TELEGRAM_CHAT_ID')
    DB_URL: str = config('DATABASE_URL')
    WEBHOOK_URL: Optional[str] = config('WEBHOOK_URL', default=None)
    REDIS_URL: str = config('REDIS_URL', default='redis://localhost:6379/0')
except Exception as e:
    logger.critical(f"‚ùå ŸÅÿ¥ŸÑ ÿ≠ÿßÿ≥ŸÖ ŸÅŸä ÿ™ÿ≠ŸÖŸäŸÑ ŸÖÿ™ÿ∫Ÿäÿ±ÿßÿ™ ÿßŸÑÿ®Ÿäÿ¶ÿ© ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©: {e}")
    exit(1)

# --- ÿßŸÑÿ´Ÿàÿßÿ®ÿ™ ŸàÿßŸÑŸÖÿ™ÿ∫Ÿäÿ±ÿßÿ™ ÿßŸÑÿπÿßŸÖÿ© ---
BASE_ML_MODEL_NAME: str = 'LightGBM_Scalping_V7_With_Ichimoku'
MODEL_FOLDER: str = 'V7'
SIGNAL_GENERATION_TIMEFRAME: str = '15m'
HIGHER_TIMEFRAME: str = '4h'
SIGNAL_GENERATION_LOOKBACK_DAYS: int = 30
REDIS_PRICES_HASH_NAME: str = "crypto_bot_current_prices"
MODEL_BATCH_SIZE: int = 5
DIRECT_API_CHECK_INTERVAL: int = 10
MEMORY_CLEANUP_INTERVAL: int = 3600  # ÿ™ŸÜÿ∏ŸäŸÅ ŸÉŸÑ ÿ≥ÿßÿπÿ©

# --- ÿ´Ÿàÿßÿ®ÿ™ ÿßŸÑŸÖÿ§ÿ¥ÿ±ÿßÿ™ ÿßŸÑŸÅŸÜŸäÿ© ---
ADX_PERIOD: int = 14
BBANDS_PERIOD: int = 20
RSI_PERIOD: int = 14
MACD_FAST, MACD_SLOW, MACD_SIGNAL = 12, 26, 9
ATR_PERIOD: int = 14
EMA_SLOW_PERIOD: int = 200
EMA_FAST_PERIOD: int = 50
BTC_CORR_PERIOD: int = 30
STOCH_RSI_PERIOD: int = 14
STOCH_K: int = 3
STOCH_D: int = 3
REL_VOL_PERIOD: int = 30
RSI_OVERBOUGHT: int = 70
RSI_OVERSOLD: int = 30
STOCH_RSI_OVERBOUGHT: int = 80
STOCH_RSI_OVERSOLD: int = 20
MODEL_CONFIDENCE_THRESHOLD = 0.70
MAX_OPEN_TRADES: int = 10
TRADE_AMOUNT_USDT: float = 10.0
USE_DYNAMIC_SL_TP = True
ATR_SL_MULTIPLIER = 1.5
ATR_TP_MULTIPLIER = 2.0
USE_BTC_TREND_FILTER = True
BTC_SYMBOL = 'BTCUSDT'
BTC_TREND_TIMEFRAME = '4h'
BTC_TREND_EMA_PERIOD = 50
MIN_PROFIT_PERCENTAGE_FILTER: float = 1.0

# --- ÿ´Ÿàÿßÿ®ÿ™ ŸÅŸÑÿ™ÿ± ÿßŸÑÿ≥ÿ±ÿπÿ© ŸàÿßŸÑÿ™ŸàŸÇŸäÿ™ ---
USE_SPEED_FILTER: bool = True
SPEED_FILTER_ADX_THRESHOLD: float = 20.0
SPEED_FILTER_REL_VOL_THRESHOLD: float = 1.0
SPEED_FILTER_RSI_MIN: float = 30.0
SPEED_FILTER_RSI_MAX: float = 70.0

# --- ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ±ÿßÿ™ ÿßŸÑÿπÿßŸÖÿ© ŸàŸÇŸÅŸÑ ÿßŸÑÿπŸÖŸÑŸäÿßÿ™ ---
conn: Optional[psycopg2.extensions.connection] = None
client: Optional[Client] = None
redis_client: Optional[redis.Redis] = None
ml_models_cache: Dict[str, Any] = {}
validated_symbols_to_scan: List[str] = []
open_signals_cache: Dict[str, Dict] = {}
signal_cache_lock = Lock()
notifications_cache = deque(maxlen=50)
notifications_lock = Lock()
signals_pending_closure: Set[int] = set()
closure_lock = Lock()
last_api_check_time = time.time()
last_memory_cleanup = time.time()
# --- ÿØŸàÿßŸÑ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ---
def init_db(retries: int = 5, delay: int = 5) -> None:
    global conn
    logger.info("[ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™] ÿ®ÿØÿ° ÿ™ŸáŸäÿ¶ÿ© ÿßŸÑÿßÿ™ÿµÿßŸÑ...")
    for attempt in range(retries):
        try:
            conn = psycopg2.connect(DB_URL, connect_timeout=10, cursor_factory=RealDictCursor)
            conn.autocommit = False
            with conn.cursor() as cur:
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS signals (
                        id SERIAL PRIMARY KEY, 
                        symbol TEXT NOT NULL,
                        entry_price DOUBLE PRECISION NOT NULL,
                        target_price DOUBLE PRECISION NOT NULL,
                        stop_loss DOUBLE PRECISION NOT NULL,
                        status TEXT DEFAULT 'open',
                        closing_price DOUBLE PRECISION,
                        closed_at TIMESTAMP,
                        profit_percentage DOUBLE PRECISION,
                        strategy_name TEXT,
                        signal_details JSONB,
                        created_at TIMESTAMP DEFAULT NOW()
                    );
                """)
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS notifications (
                        id SERIAL PRIMARY KEY,
                        timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                        type TEXT NOT NULL,
                        message TEXT NOT NULL,
                        is_read BOOLEAN DEFAULT FALSE,
                        severity TEXT DEFAULT 'info'
                    );
                """)
                cur.execute("""
                    CREATE INDEX IF NOT EXISTS idx_signals_status ON signals(status);
                    CREATE INDEX IF NOT EXISTS idx_signals_symbol ON signals(symbol);
                    CREATE INDEX IF NOT EXISTS idx_notifications_timestamp ON notifications(timestamp);
                """)
            conn.commit()
            logger.info("‚úÖ [ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™] ÿ™ŸÖ ÿ™ŸáŸäÿ¶ÿ© ÿ¨ÿØÿßŸàŸÑ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ®ŸÜÿ¨ÿßÿ≠.")
            return
        except Exception as e:
            logger.error(f"‚ùå [ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™] ÿÆÿ∑ÿ£ ŸÅŸä ÿßŸÑÿßÿ™ÿµÿßŸÑ (ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ© {attempt + 1}): {e}")
            if conn: conn.rollback()
            if attempt < retries - 1: time.sleep(delay)
            else: 
                logger.critical("‚ùå [ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™] ŸÅÿ¥ŸÑ ÿßŸÑÿßÿ™ÿµÿßŸÑ ÿ®ÿπÿØ ÿπÿØÿ© ŸÖÿ≠ÿßŸàŸÑÿßÿ™.")
                exit(1)

def check_db_connection() -> bool:
    global conn
    if conn is None or conn.closed != 0:
        logger.warning("[ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™] ÿßŸÑÿßÿ™ÿµÿßŸÑ ŸÖÿ∫ŸÑŸÇÿå ŸÖÿ≠ÿßŸàŸÑÿ© ÿ•ÿπÿßÿØÿ© ÿßŸÑÿßÿ™ÿµÿßŸÑ...")
        init_db()
    try:
        if conn: 
            conn.cursor().execute("SELECT 1;")
            return True
        return False
    except (OperationalError, InterfaceError) as e:
        logger.error(f"‚ùå [ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™] ŸÅŸÇÿØÿßŸÜ ÿßŸÑÿßÿ™ÿµÿßŸÑ: {e}. ŸÖÿ≠ÿßŸàŸÑÿ© ÿ•ÿπÿßÿØÿ© ÿßŸÑÿßÿ™ÿµÿßŸÑ...")
        try: 
            init_db()
            return conn is not None and conn.closed == 0
        except Exception as retry_e: 
            logger.error(f"‚ùå [ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™] ŸÅÿ¥ŸÑ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿßÿ™ÿµÿßŸÑ: {retry_e}")
            return False

def log_and_notify(level: str, message: str, notification_type: str):
    log_methods = {
        'info': logger.info,
        'warning': logger.warning,
        'error': logger.error,
        'critical': logger.critical
    }
    log_methods.get(level.lower(), logger.info)(message)
    
    if not check_db_connection() or not conn:
        return
        
    try:
        new_notification = {
            "timestamp": datetime.now().isoformat(),
            "type": notification_type,
            "message": message,
            "severity": level
        }
        
        with notifications_lock:
            notifications_cache.appendleft(new_notification)
            
        with conn.cursor() as cur:
            cur.execute(
                "INSERT INTO notifications (type, message, severity) VALUES (%s, %s, %s);",
                (notification_type, message, level)
            )
        conn.commit()
        
    except Exception as e:
        logger.error(f"‚ùå [Notify DB] ŸÅÿ¥ŸÑ ÿ≠ŸÅÿ∏ ÿßŸÑÿ™ŸÜÿ®ŸäŸá ŸÅŸä ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™: {e}")
        if conn: conn.rollback()

def init_redis() -> None:
    global redis_client
    logger.info("[Redis] ÿ®ÿØÿ° ÿ™ŸáŸäÿ¶ÿ© ÿßŸÑÿßÿ™ÿµÿßŸÑ...")
    try:
        redis_client = redis.from_url(REDIS_URL, decode_responses=True)
        redis_client.ping()
        logger.info("‚úÖ [Redis] ÿ™ŸÖ ÿßŸÑÿßÿ™ÿµÿßŸÑ ÿ®ŸÜÿ¨ÿßÿ≠ ÿ®ÿÆÿßÿØŸÖ Redis.")
    except redis.exceptions.ConnectionError as e:
        logger.critical(f"‚ùå [Redis] ŸÅÿ¥ŸÑ ÿßŸÑÿßÿ™ÿµÿßŸÑ ÿ®ŸÄ Redis ÿπŸÑŸâ {REDIS_URL}. ÿßŸÑÿÆÿ∑ÿ£: {e}")
        exit(1)
    except Exception as e:
        logger.critical(f"‚ùå [Redis] ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£ ÿ∫Ÿäÿ± ŸÖÿ™ŸàŸÇÿπ ÿ£ÿ´ŸÜÿßÿ° ÿ™ŸáŸäÿ¶ÿ© Redis: {e}")
        exit(1)

def recover_cache_state():
    """ÿßÿ≥ÿ™ÿ±ÿØÿßÿØ ÿ≠ÿßŸÑÿ© ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© ÿßŸÑŸÖÿ§ŸÇÿ™ÿ© ŸÅŸä ÿ≠ÿßŸÑÿ© ÿßŸÑÿ£ÿÆÿ∑ÿßÿ°"""
    logger.info("üîÑ [ÿßÿ≥ÿ™ÿ±ÿØÿßÿØ] ÿ®ÿØÿ° ÿßÿ≥ÿ™ÿ±ÿØÿßÿØ ÿ≠ÿßŸÑÿ© ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© ÿßŸÑŸÖÿ§ŸÇÿ™ÿ©...")
    try:
        # ÿßÿ≥ÿ™ÿ±ÿØÿßÿØ ÿßŸÑÿ•ÿ¥ÿßÿ±ÿßÿ™ ÿßŸÑŸÖŸÅÿ™Ÿàÿ≠ÿ©
        load_open_signals_to_cache()
        
        # ÿßÿ≥ÿ™ÿ±ÿØÿßÿØ ÿßŸÑÿ™ŸÜÿ®ŸäŸáÿßÿ™
        load_notifications_to_cache()
        
        # ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿ•ÿ¥ÿßÿ±ÿßÿ™ ÿßŸÑÿπÿßŸÑŸÇÿ©
        with closure_lock:
            signals_pending_closure.clear()
            
        # ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑŸÖÿÆÿ≤ŸÜÿ© ŸÖÿ§ŸÇÿ™ÿßŸã
        ml_models_cache.clear()
        
        # ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ©
        gc.collect()
            
        logger.info("‚úÖ [ÿßÿ≥ÿ™ÿ±ÿØÿßÿØ] ÿ™ŸÖ ÿßÿ≥ÿ™ÿ±ÿØÿßÿØ ÿ≠ÿßŸÑÿ© ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© ÿßŸÑŸÖÿ§ŸÇÿ™ÿ© ÿ®ŸÜÿ¨ÿßÿ≠")
    except Exception as e:
        logger.error(f"‚ùå [ÿßÿ≥ÿ™ÿ±ÿØÿßÿØ] ŸÅÿ¥ŸÑ ÿßÿ≥ÿ™ÿ±ÿØÿßÿØ ÿ≠ÿßŸÑÿ© ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© ÿßŸÑŸÖÿ§ŸÇÿ™ÿ©: {e}")

def cleanup_memory():
    """ÿ™ŸÜÿ∏ŸäŸÅ ÿØŸàÿ±Ÿä ŸÑŸÑÿ∞ÿßŸÉÿ±ÿ©"""
    global last_memory_cleanup
    
    current_time = time.time()
    if current_time - last_memory_cleanup > MEMORY_CLEANUP_INTERVAL:
        logger.info("üßπ [ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ©] ÿ®ÿØÿ° ÿßŸÑÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿØŸàÿ±Ÿä...")
        
        # ÿ™ŸÜÿ∏ŸäŸÅ ÿ∞ÿßŸÉÿ±ÿ© ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨
        ml_models_cache.clear()
        
        # ÿ™ÿ¥ÿ∫ŸäŸÑ ÿ¨ÿßŸÖÿπ ÿßŸÑŸÜŸÅÿßŸäÿßÿ™
        gc.collect()
        
        last_memory_cleanup = current_time
        logger.info("‚úÖ [ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ©] ÿßŸÉÿ™ŸÖŸÑ ÿßŸÑÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿØŸàÿ±Ÿä")
        # --- ÿØŸàÿßŸÑ Binance ŸàÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ---
def get_validated_symbols(filename: str = 'crypto_list.txt') -> List[str]:
    logger.info(f"‚ÑπÔ∏è [ÿßŸÑÿ™ÿ≠ŸÇŸÇ] ŸÇÿ±ÿßÿ°ÿ© ÿßŸÑÿ±ŸÖŸàÿ≤ ŸÖŸÜ '{filename}' ŸàÿßŸÑÿ™ÿ≠ŸÇŸÇ ŸÖŸÜŸáÿß ŸÖÿπ Binance...")
    if not client: 
        logger.error("‚ùå [ÿßŸÑÿ™ÿ≠ŸÇŸÇ] ŸÉÿßÿ¶ŸÜ Binance client ÿ∫Ÿäÿ± ŸÖŸáŸäÿ£.")
        return []
        
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, filename)
        
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_symbols = {line.strip().upper() for line in f if line.strip() and not line.startswith('#')}
            
        formatted = {f"{s}USDT" if not s.endswith('USDT') else s for s in raw_symbols}
        
        try:
            exchange_info = client.get_exchange_info()
            active = {s['symbol'] for s in exchange_info['symbols'] 
                     if s.get('quoteAsset') == 'USDT' and s.get('status') == 'TRADING'}
        except BinanceAPIException as e:
            logger.error(f"‚ùå [Binance API] ŸÅÿ¥ŸÑ ÿ¨ŸÑÿ® ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑÿ™ÿØÿßŸàŸÑ: {e}")
            return []
            
        validated = sorted(list(formatted.intersection(active)))
        logger.info(f"‚úÖ [ÿßŸÑÿ™ÿ≠ŸÇŸÇ] ÿ≥ŸäŸÇŸàŸÖ ÿßŸÑÿ®Ÿàÿ™ ÿ®ŸÖÿ±ÿßŸÇÿ®ÿ© {len(validated)} ÿπŸÖŸÑÿ© ŸÖÿπÿ™ŸÖÿØÿ©.")
        return validated
        
    except Exception as e:
        logger.error(f"‚ùå [ÿßŸÑÿ™ÿ≠ŸÇŸÇ] ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£ ÿ£ÿ´ŸÜÿßÿ° ÿßŸÑÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ÿßŸÑÿ±ŸÖŸàÿ≤: {e}", exc_info=True)
        return []

def fetch_historical_data(symbol: str, interval: str, days: int) -> Optional[pd.DataFrame]:
    if not client:
        return None
        
    try:
        start_dt = datetime.now(timezone.utc) - timedelta(days=days)
        start_str = start_dt.strftime("%Y-%m-%d %H:%M:%S")
        
        klines = client.get_historical_klines(symbol, interval, start_str)
        if not klines:
            return None
            
        df = pd.DataFrame(klines, columns=['timestamp', 'open', 'high', 'low', 'close', 
                                         'volume', 'close_time', 'quote_volume', 'trades',
                                         'taker_buy_base', 'taker_buy_quote', 'ignore'])
                                         
        df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]
        
        # ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑÿ£ÿπŸÖÿØÿ© ÿ•ŸÑŸâ ŸÜŸàÿπ float32 ŸÑÿ™ŸàŸÅŸäÿ± ÿßŸÑÿ∞ÿßŸÉÿ±ÿ©
        numeric_cols = {
            'open': 'float32',
            'high': 'float32',
            'low': 'float32', 
            'close': 'float32',
            'volume': 'float32'
        }
        df = df.astype(numeric_cols)
        
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)
        df.set_index('timestamp', inplace=True)
        
        return df.dropna()
        
    except BinanceAPIException as e:
        logger.warning(f"‚ö†Ô∏è [API Binance] ÿÆÿ∑ÿ£ ŸÅŸä ÿ¨ŸÑÿ® ÿ®ŸäÿßŸÜÿßÿ™ {symbol}: {e}")
        return None
    except Exception as e:
        logger.error(f"‚ùå [ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™] ÿÆÿ∑ÿ£ ÿ£ÿ´ŸÜÿßÿ° ÿ¨ŸÑÿ® ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ™ÿßÿ±ŸäÿÆŸäÿ© ŸÑŸÄ {symbol}: {e}")
        return None
    finally:
        gc.collect()  # ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© ÿ®ÿπÿØ ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™

def fetch_sr_levels_from_db(symbol: str) -> pd.DataFrame:
    if not check_db_connection() or not conn:
        return pd.DataFrame()
        
    query = """
        SELECT level_price, level_type, score 
        FROM support_resistance_levels 
        WHERE symbol = %s
        AND updated_at > NOW() - INTERVAL '24 HOURS';
    """
    
    try:
        with conn.cursor() as cur:
            cur.execute(query, (symbol,))
            levels = cur.fetchall()
            
            if not levels:
                return pd.DataFrame()
                
            return pd.DataFrame(levels)
            
    except Exception as e:
        logger.error(f"‚ùå [S/R Fetch Bot] Could not fetch S/R levels for {symbol}: {e}")
        if conn: conn.rollback()
        return pd.DataFrame()
    finally:
        gc.collect()

def fetch_ichimoku_features_from_db(symbol: str, timeframe: str) -> pd.DataFrame:
    if not check_db_connection() or not conn:
        return pd.DataFrame()
        
    query = """
        SELECT timestamp, tenkan_sen, kijun_sen, senkou_span_a, senkou_span_b, chikou_span
        FROM ichimoku_features
        WHERE symbol = %s 
        AND timeframe = %s
        AND timestamp > NOW() - INTERVAL '24 HOURS'
        ORDER BY timestamp;
    """
    
    try:
        with conn.cursor() as cur:
            cur.execute(query, (symbol, timeframe))
            features = cur.fetchall()
            
            if not features:
                return pd.DataFrame()
                
            df_ichimoku = pd.DataFrame(features)
            df_ichimoku['timestamp'] = pd.to_datetime(df_ichimoku['timestamp'], utc=True)
            df_ichimoku.set_index('timestamp', inplace=True)
            
            return df_ichimoku
            
    except Exception as e:
        logger.error(f"‚ùå [Ichimoku Fetch Bot] Could not fetch Ichimoku features for {symbol}: {e}")
        if conn: conn.rollback()
        return pd.DataFrame()
    finally:
        gc.collect()
        def calculate_ichimoku_based_features(df: pd.DataFrame) -> pd.DataFrame:
    try:
        # ÿ≠ÿ≥ÿßÿ® ÿßŸÑŸÖÿ§ÿ¥ÿ±ÿßÿ™ ÿßŸÑŸÜÿ≥ÿ®Ÿäÿ©
        df['price_vs_tenkan'] = (df['close'] - df['tenkan_sen']) / df['tenkan_sen']
        df['price_vs_kijun'] = (df['close'] - df['kijun_sen']) / df['kijun_sen']
        df['tenkan_vs_kijun'] = (df['tenkan_sen'] - df['kijun_sen']) / df['kijun_sen']
        df['price_vs_kumo_a'] = (df['close'] - df['senkou_span_a']) / df['senkou_span_a']
        df['price_vs_kumo_b'] = (df['close'] - df['senkou_span_b']) / df['senkou_span_b']
        df['kumo_thickness'] = (df['senkou_span_a'] - df['senkou_span_b']).abs() / df['close']

        # ÿ™ÿ≠ÿØŸäÿØ ŸÖŸàŸÇÿπ ÿßŸÑÿ≥ÿπÿ± ÿ®ÿßŸÑŸÜÿ≥ÿ®ÿ© ŸÑŸÑÿ≥ÿ≠ÿßÿ®ÿ©
        kumo_high = df[['senkou_span_a', 'senkou_span_b']].max(axis=1)
        kumo_low = df[['senkou_span_a', 'senkou_span_b']].min(axis=1)
        
        df['price_above_kumo'] = (df['close'] > kumo_high).astype(int)
        df['price_below_kumo'] = (df['close'] < kumo_low).astype(int)
        df['price_in_kumo'] = ((df['close'] >= kumo_low) & (df['close'] <= kumo_high)).astype(int)
        
        # ŸÖÿ§ÿ¥ÿ±ÿßÿ™ Chikou
        df['chikou_above_kumo'] = (df['chikou_span'] > kumo_high).astype(int)
        df['chikou_below_kumo'] = (df['chikou_span'] < kumo_low).astype(int)
        
        # ÿ™ŸÇÿßÿ∑ÿπÿßÿ™ Tenkan/Kijun
        df['tenkan_kijun_cross'] = 0
        cross_up = (df['tenkan_sen'].shift(1) < df['kijun_sen'].shift(1)) & (df['tenkan_sen'] > df['kijun_sen'])
        cross_down = (df['tenkan_sen'].shift(1) > df['kijun_sen'].shift(1)) & (df['tenkan_sen'] < df['kijun_sen'])
        df.loc[cross_up, 'tenkan_kijun_cross'] = 1
        df.loc[cross_down, 'tenkan_kijun_cross'] = -1
        
        return df
        
    except Exception as e:
        logger.error(f"‚ùå [Ichimoku] ÿÆÿ∑ÿ£ ŸÅŸä ÿ≠ÿ≥ÿßÿ® ŸÖÿ§ÿ¥ÿ±ÿßÿ™ Ichimoku: {e}")
        return df
    finally:
        gc.collect()

def calculate_candlestick_patterns(df: pd.DataFrame) -> pd.DataFrame:
    try:
        df_patterns = df.copy()
        op, hi, lo, cl = df_patterns['open'], df_patterns['high'], df_patterns['low'], df_patterns['close']
        
        # ÿ≠ÿ≥ÿßÿ® ÿÆÿµÿßÿ¶ÿµ ÿßŸÑÿ¥ŸÖŸàÿπ
        body = abs(cl - op)
        candle_range = hi - lo
        candle_range[candle_range == 0] = 1e-9  # ÿ™ÿ¨ŸÜÿ® ÿßŸÑŸÇÿ≥ŸÖÿ© ÿπŸÑŸâ ÿµŸÅÿ±
        
        upper_wick = hi - pd.concat([op, cl], axis=1).max(axis=1)
        lower_wick = pd.concat([op, cl], axis=1).min(axis=1) - lo
        
        df_patterns['candlestick_pattern'] = 0
        
        # ÿ™ÿ≠ÿØŸäÿØ ÿ£ŸÜŸÖÿßÿ∑ ÿßŸÑÿ¥ŸÖŸàÿπ
        is_bullish_marubozu = (cl > op) & (body / candle_range > 0.95) & (upper_wick < body * 0.1) & (lower_wick < body * 0.1)
        is_bearish_marubozu = (op > cl) & (body / candle_range > 0.95) & (upper_wick < body * 0.1) & (lower_wick < body * 0.1)
        
        is_bullish_engulfing = (cl.shift(1) < op.shift(1)) & (cl > op) & (cl >= op.shift(1)) & (op <= cl.shift(1)) & (body > body.shift(1))
        is_bearish_engulfing = (cl.shift(1) > op.shift(1)) & (cl < op) & (op >= cl.shift(1)) & (cl <= op.shift(1)) & (body > body.shift(1))
        
        is_hammer = (body > candle_range * 0.1) & (lower_wick >= body * 2) & (upper_wick < body)
        is_shooting_star = (body > candle_range * 0.1) & (upper_wick >= body * 2) & (lower_wick < body)
        
        is_doji = (body / candle_range) < 0.05
        
        # ÿ™ÿπŸäŸäŸÜ ŸÇŸäŸÖ ÿßŸÑÿ£ŸÜŸÖÿßÿ∑
        df_patterns.loc[is_doji, 'candlestick_pattern'] = 3
        df_patterns.loc[is_hammer, 'candlestick_pattern'] = 2
        df_patterns.loc[is_shooting_star, 'candlestick_pattern'] = -2
        df_patterns.loc[is_bullish_engulfing, 'candlestick_pattern'] = 1
        df_patterns.loc[is_bearish_engulfing, 'candlestick_pattern'] = -1
        df_patterns.loc[is_bullish_marubozu, 'candlestick_pattern'] = 4
        df_patterns.loc[is_bearish_marubozu, 'candlestick_pattern'] = -4
        
        return df_patterns
        
    except Exception as e:
        logger.error(f"‚ùå [Patterns] ÿÆÿ∑ÿ£ ŸÅŸä ÿ™ÿ≠ŸÑŸäŸÑ ÿ£ŸÜŸÖÿßÿ∑ ÿßŸÑÿ¥ŸÖŸàÿπ: {e}")
        return df
    finally:
        gc.collect()

def calculate_sr_features(df: pd.DataFrame, sr_levels_df: pd.DataFrame) -> pd.DataFrame:
    if sr_levels_df.empty:
        df['dist_to_support'] = 0.0
        df['dist_to_resistance'] = 0.0
        df['score_of_support'] = 0.0
        df['score_of_resistance'] = 0.0
        return df
        
    try:
        # ÿ™ÿ≠ÿØŸäÿØ ŸÖÿ≥ÿ™ŸàŸäÿßÿ™ ÿßŸÑÿØÿπŸÖ ŸàÿßŸÑŸÖŸÇÿßŸàŸÖÿ©
        supports = sr_levels_df[sr_levels_df['level_type'].str.contains('support|poc|confluence', case=False)]['level_price'].sort_values().to_numpy()
        resistances = sr_levels_df[sr_levels_df['level_type'].str.contains('resistance|poc|confluence', case=False)]['level_price'].sort_values().to_numpy()
        
        # ÿ™ÿ≠ŸàŸäŸÑ ÿØÿ±ÿ¨ÿßÿ™ ÿßŸÑŸÇŸàÿ© ÿ•ŸÑŸâ ŸÇŸàÿßŸÖŸäÿ≥ ŸÑŸÑŸàÿµŸàŸÑ ÿßŸÑÿ≥ÿ±Ÿäÿπ
        support_scores = sr_levels_df[sr_levels_df['level_type'].str.contains('support|poc|confluence', case=False)].set_index('level_price')['score'].to_dict()
        resistance_scores = sr_levels_df[sr_levels_df['level_type'].str.contains('resistance|poc|confluence', case=False)].set_index('level_price')['score'].to_dict()

        def get_sr_info(price):
            dist_support, score_support = 1.0, 0.0
            dist_resistance, score_resistance = 1.0, 0.0
            
            # ÿ≠ÿ≥ÿßÿ® ÿ£ŸÇÿ±ÿ® ŸÖÿ≥ÿ™ŸàŸâ ÿØÿπŸÖ
            if supports.size > 0:
                idx = np.searchsorted(supports, price, side='right') - 1
                if idx >= 0:
                    nearest_support_price = supports[idx]
                    dist_support = (price - nearest_support_price) / price if price > 0 else 0
                    score_support = support_scores.get(nearest_support_price, 0)
                    
            # ÿ≠ÿ≥ÿßÿ® ÿ£ŸÇÿ±ÿ® ŸÖÿ≥ÿ™ŸàŸâ ŸÖŸÇÿßŸàŸÖÿ©
            if resistances.size > 0:
                idx = np.searchsorted(resistances, price, side='left')
                if idx < len(resistances):
                    nearest_resistance_price = resistances[idx]
                    dist_resistance = (nearest_resistance_price - price) / price if price > 0 else 0
                    score_resistance = resistance_scores.get(nearest_resistance_price, 0)
                    
            return dist_support, score_support, dist_resistance, score_resistance

        # ÿ™ÿ∑ÿ®ŸäŸÇ ÿßŸÑÿ≠ÿ≥ÿßÿ®ÿßÿ™ ÿπŸÑŸâ ŸÉŸÑ ÿ≥ÿπÿ±
        results = df['close'].apply(get_sr_info)
        df[['dist_to_support', 'score_of_support', 'dist_to_resistance', 'score_of_resistance']] = pd.DataFrame(results.tolist(), index=df.index)
        
        return df
        
    except Exception as e:
        logger.error(f"‚ùå [S/R Features] ÿÆÿ∑ÿ£ ŸÅŸä ÿ≠ÿ≥ÿßÿ® ÿÆÿµÿßÿ¶ÿµ ÿßŸÑÿØÿπŸÖ ŸàÿßŸÑŸÖŸÇÿßŸàŸÖÿ©: {e}")
        return df
    finally:
        gc.collect()
        def calculate_features(df: pd.DataFrame, btc_df: pd.DataFrame) -> pd.DataFrame:
    try:
        df_calc = df.copy()
        
        # ÿ≠ÿ≥ÿßÿ® ATR
        high_low = df_calc['high'] - df_calc['low']
        high_close = (df_calc['high'] - df_calc['close'].shift()).abs()
        low_close = (df_calc['low'] - df_calc['close'].shift()).abs()
        tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        df_calc['atr'] = tr.ewm(span=ATR_PERIOD, adjust=False).mean()
        
        # ÿ≠ÿ≥ÿßÿ® ADX
        up_move = df_calc['high'].diff()
        down_move = -df_calc['low'].diff()
        plus_dm = pd.Series(np.where((up_move > down_move) & (up_move > 0), up_move, 0.0), index=df_calc.index)
        minus_dm = pd.Series(np.where((down_move > up_move) & (down_move > 0), down_move, 0.0), index=df_calc.index)
        plus_di = 100 * plus_dm.ewm(span=ADX_PERIOD, adjust=False).mean() / df_calc['atr']
        minus_di = 100 * minus_dm.ewm(span=ADX_PERIOD, adjust=False).mean() / df_calc['atr']
        dx = 100 * (abs(plus_di - minus_di) / (plus_di + minus_di).replace(0, 1e-9))
        df_calc['adx'] = dx.ewm(span=ADX_PERIOD, adjust=False).mean()
        
        # ÿ≠ÿ≥ÿßÿ® RSI
        delta = df_calc['close'].diff()
        gain = delta.clip(lower=0).ewm(com=RSI_PERIOD - 1, adjust=False).mean()
        loss = -delta.clip(upper=0).ewm(com=RSI_PERIOD - 1, adjust=False).mean()
        df_calc['rsi'] = 100 - (100 / (1 + (gain / loss.replace(0, 1e-9))))
        
        # ÿ≠ÿ≥ÿßÿ® MACD
        ema_fast = df_calc['close'].ewm(span=MACD_FAST, adjust=False).mean()
        ema_slow = df_calc['close'].ewm(span=MACD_SLOW, adjust=False).mean()
        macd_line = ema_fast - ema_slow
        signal_line = macd_line.ewm(span=MACD_SIGNAL, adjust=False).mean()
        df_calc['macd_hist'] = macd_line - signal_line
        
        # ÿ™ŸÇÿßÿ∑ÿπÿßÿ™ MACD
        df_calc['macd_cross'] = 0
        df_calc.loc[(df_calc['macd_hist'].shift(1) < 0) & (df_calc['macd_hist'] >= 0), 'macd_cross'] = 1
        df_calc.loc[(df_calc['macd_hist'].shift(1) > 0) & (df_calc['macd_hist'] <= 0), 'macd_cross'] = -1
        
        # ÿ≠ÿ≥ÿßÿ® Bollinger Bands
        sma = df_calc['close'].rolling(window=BBANDS_PERIOD).mean()
        std_dev = df_calc['close'].rolling(window=BBANDS_PERIOD).std()
        upper_band = sma + (std_dev * 2)
        lower_band = sma - (std_dev * 2)
        df_calc['bb_width'] = (upper_band - lower_band) / (sma + 1e-9)
        
        # ÿ≠ÿ≥ÿßÿ® Stochastic RSI
        rsi = df_calc['rsi']
        min_rsi = rsi.rolling(window=STOCH_RSI_PERIOD).min()
        max_rsi = rsi.rolling(window=STOCH_RSI_PERIOD).max()
        stoch_rsi_val = (rsi - min_rsi) / (max_rsi - min_rsi).replace(0, 1e-9)
        df_calc['stoch_rsi_k'] = stoch_rsi_val.rolling(window=STOCH_K).mean() * 100
        df_calc['stoch_rsi_d'] = df_calc['stoch_rsi_k'].rolling(window=STOCH_D).mean()
        
        # ÿ≠ÿ¨ŸÖ ÿßŸÑÿ™ÿØÿßŸàŸÑ ÿßŸÑŸÜÿ≥ÿ®Ÿä
        df_calc['relative_volume'] = df_calc['volume'] / (df_calc['volume'].rolling(window=REL_VOL_PERIOD, min_periods=1).mean() + 1e-9)
        
        # ÿ≠ÿßŸÑÿ© ÿßŸÑÿ≥ŸàŸÇ
        df_calc['market_condition'] = 0
        df_calc.loc[(df_calc['rsi'] > RSI_OVERBOUGHT) | (df_calc['stoch_rsi_k'] > STOCH_RSI_OVERBOUGHT), 'market_condition'] = 1
        df_calc.loc[(df_calc['rsi'] < RSI_OVERSOLD) | (df_calc['stoch_rsi_k'] < STOCH_RSI_OVERSOLD), 'market_condition'] = -1
        
        # ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ∑ÿßÿ™ ÿßŸÑŸÖÿ™ÿ≠ÿ±ŸÉÿ© ÿßŸÑÿ£ÿ≥Ÿäÿ©
        ema_fast_trend = df_calc['close'].ewm(span=EMA_FAST_PERIOD, adjust=False).mean()
        ema_slow_trend = df_calc['close'].ewm(span=EMA_SLOW_PERIOD, adjust=False).mean()
        df_calc['price_vs_ema50'] = (df_calc['close'] / ema_fast_trend) - 1
        df_calc['price_vs_ema200'] = (df_calc['close'] / ema_slow_trend) - 1
        
        # ÿßŸÑÿπŸàÿßÿ¶ÿØ ŸàÿßŸÑÿßÿ±ÿ™ÿ®ÿßÿ∑ ŸÖÿπ ÿßŸÑÿ®Ÿäÿ™ŸÉŸàŸäŸÜ
        df_calc['returns'] = df_calc['close'].pct_change()
        merged_df = pd.merge(df_calc, btc_df[['btc_returns']], left_index=True, right_index=True, how='left').fillna(0)
        df_calc['btc_correlation'] = merged_df['returns'].rolling(window=BTC_CORR_PERIOD).corr(merged_df['btc_returns'])
        
        # ÿ≥ÿßÿπÿ© ÿßŸÑŸäŸàŸÖ
        df_calc['hour_of_day'] = df_calc.index.hour
        
        # ÿ£ŸÜŸÖÿßÿ∑ ÿßŸÑÿ¥ŸÖŸàÿπ
        df_calc = calculate_candlestick_patterns(df_calc)
        
        return df_calc.astype('float32', errors='ignore')
        
    except Exception as e:
        logger.error(f"‚ùå [Features] ÿÆÿ∑ÿ£ ŸÅŸä ÿ≠ÿ≥ÿßÿ® ÿßŸÑŸÖÿ§ÿ¥ÿ±ÿßÿ™ ÿßŸÑŸÅŸÜŸäÿ©: {e}")
        return df_calc
    finally:
        gc.collect()
        def load_ml_model_bundle_from_folder(symbol: str) -> Optional[Dict[str, Any]]:
    global ml_models_cache
    
    model_name = f"{BASE_ML_MODEL_NAME}_{symbol}"
    
    # ÿßŸÑÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ Ÿàÿ¨ŸàÿØ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸÅŸä ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© ÿßŸÑŸÖÿ§ŸÇÿ™ÿ©
    if model_name in ml_models_cache:
        logger.debug(f"‚úÖ [ML Model] ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ '{model_name}' ŸÖŸÜ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© ÿßŸÑŸÖÿ§ŸÇÿ™ÿ©.")
        return ml_models_cache[model_name]

    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        model_path = os.path.join(script_dir, MODEL_FOLDER, f"{model_name}.pkl")
        
        if not os.path.exists(model_path):
            logger.warning(f"‚ö†Ô∏è [ML Model] ŸÖŸÑŸÅ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ '{model_path}' ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØ ŸÑŸÑÿπŸÖŸÑÿ© {symbol}.")
            return None

        with open(model_path, 'rb') as f:
            model_bundle = pickle.load(f)
            
        if all(key in model_bundle for key in ['model', 'scaler', 'feature_names']):
            ml_models_cache[model_name] = model_bundle
            logger.info(f"‚úÖ [ML Model] ÿ™ŸÖ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ '{model_name}' ÿ®ŸÜÿ¨ÿßÿ≠ ŸÖŸÜ ÿßŸÑŸÖŸÑŸÅ ÿßŸÑŸÖÿ≠ŸÑŸä.")
            return model_bundle
        else:
            logger.error(f"‚ùå [ML Model] ÿ≠ÿ≤ŸÖÿ© ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸÅŸä '{model_path}' ÿ∫Ÿäÿ± ŸÖŸÉÿ™ŸÖŸÑÿ©.")
            return None
            
    except Exception as e:
        logger.error(f"‚ùå [ML Model] ÿÆÿ∑ÿ£ ŸÅŸä ÿ™ÿ≠ŸÖŸäŸÑ ÿ≠ÿ≤ŸÖÿ© ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸÖŸÜ ÿßŸÑŸÖŸÑŸÅ ŸÑŸÑÿπŸÖŸÑÿ© {symbol}: {e}", exc_info=True)
        return None
    finally:
        gc.collect()

def handle_price_update_message(msg: List[Dict[str, Any]]) -> None:
    global redis_client
    
    try:
        if not isinstance(msg, list):
            logger.warning(f"‚ö†Ô∏è [WebSocket] ÿ™ŸÖ ÿßÿ≥ÿ™ŸÑÿßŸÖ ÿ±ÿ≥ÿßŸÑÿ© ÿ®ÿ™ŸÜÿ≥ŸäŸÇ ÿ∫Ÿäÿ± ŸÖÿ™ŸàŸÇÿπ: {type(msg)}")
            return
            
        if not redis_client:
            logger.error("‚ùå [WebSocket] ŸÉÿßÿ¶ŸÜ Redis ÿ∫Ÿäÿ± ŸÖŸáŸäÿ£. ŸÑÿß ŸäŸÖŸÉŸÜ ÿ≠ŸÅÿ∏ ÿßŸÑÿ£ÿ≥ÿπÿßÿ±.")
            return

        # ÿ™ÿ¨ŸÖŸäÿπ ÿ™ÿ≠ÿØŸäÿ´ÿßÿ™ ÿßŸÑÿ£ÿ≥ÿπÿßÿ± ŸÅŸä ŸÇÿßŸÖŸàÿ≥ Ÿàÿßÿ≠ÿØ
        price_updates = {
            item.get('s'): float(item.get('c', 0)) 
            for item in msg 
            if item.get('s') and item.get('c')
        }
        
        if price_updates:
            # ÿ™ÿ≠ÿØŸäÿ´ Redis ŸÅŸä ÿπŸÖŸÑŸäÿ© Ÿàÿßÿ≠ÿØÿ©
            redis_client.hset(REDIS_PRICES_HASH_NAME, mapping=price_updates)
            
    except Exception as e:
        logger.error(f"‚ùå [WebSocket Price Updater] ÿÆÿ∑ÿ£ ŸÅŸä ŸÖÿπÿßŸÑÿ¨ÿ© ÿ±ÿ≥ÿßŸÑÿ© ÿßŸÑÿ≥ÿπÿ±: {e}", exc_info=True)
    finally:
        gc.collect()

def initiate_signal_closure(symbol: str, signal_to_close: Dict, status: str, closing_price: float):
    signal_id = signal_to_close.get('id')
    
    # ÿßŸÑÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ÿµÿ≠ÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™
    if not all([signal_id, symbol, status, closing_price]):
        logger.error(f"‚ùå [CLOSURE] ÿ®ŸäÿßŸÜÿßÿ™ ÿ∫Ÿäÿ± ÿµÿßŸÑÿ≠ÿ© ŸÑÿ•ÿ∫ŸÑÿßŸÇ ÿßŸÑÿ•ÿ¥ÿßÿ±ÿ©: ID={signal_id}, Symbol={symbol}")
        return
    
    with closure_lock:
        if signal_id in signals_pending_closure:
            logger.warning(f"‚ö†Ô∏è [CLOSURE] ÿßŸÑÿ•ÿ¥ÿßÿ±ÿ© {signal_id} ŸÇŸäÿØ ÿßŸÑÿ•ÿ∫ŸÑÿßŸÇ ÿ®ÿßŸÑŸÅÿπŸÑ")
            return
        signals_pending_closure.add(signal_id)
    
    try:
        with signal_cache_lock:
            signal_data = open_signals_cache.pop(symbol, None)
            
        if signal_data:
            logger.info(f"‚ö° [CLOSURE] ÿ™ŸÖ ÿ•ÿ≤ÿßŸÑÿ© ÿßŸÑÿ•ÿ¥ÿßÿ±ÿ© {signal_id} ŸÖŸÜ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© ÿßŸÑŸÖÿ§ŸÇÿ™ÿ©. ÿ®ÿØÿ° ÿπŸÖŸÑŸäÿ© ÿßŸÑÿ•ÿ∫ŸÑÿßŸÇ...")
            Thread(target=close_signal, args=(signal_data, status, closing_price, "auto_monitor")).start()
        else:
            logger.warning(f"‚ö†Ô∏è [CLOSURE] ŸÑŸÖ Ÿäÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ÿßŸÑÿ•ÿ¥ÿßÿ±ÿ© {signal_id} ŸÅŸä ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© ÿßŸÑŸÖÿ§ŸÇÿ™ÿ©")
            with closure_lock:
                signals_pending_closure.discard(signal_id)
                
    except Exception as e:
        logger.error(f"‚ùå [CLOSURE] ÿÆÿ∑ÿ£ ŸÅŸä ÿ®ÿØÿ° ÿ•ÿ∫ŸÑÿßŸÇ ÿßŸÑÿ•ÿ¥ÿßÿ±ÿ© {signal_id}: {e}")
        with closure_lock:
            signals_pending_closure.discard(signal_id)
    finally:
        gc.collect()
        def trade_monitoring_loop():
    global last_api_check_time
    
    # ÿ•ÿ∂ÿßŸÅÿ© ŸÖÿ§ŸÇÿ™ ŸÑŸÑÿ™ÿ≠ŸÉŸÖ ŸÅŸä ÿπÿØÿØ ŸÖÿ≠ÿßŸàŸÑÿßÿ™ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿßÿ™ÿµÿßŸÑ
    MAX_RECONNECT_ATTEMPTS = 3
    reconnect_attempts = 0
    
    # ÿ•ÿ∂ÿßŸÅÿ© ŸÖÿ§ŸÇÿ™ ŸÑŸÑÿ™ÿ≠ŸÉŸÖ ŸÅŸä ÿπŸÖŸÑŸäÿßÿ™ ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ©
    last_memory_cleanup = time.time()
    
    logger.info("‚úÖ [Trade Monitor] ÿ®ÿØÿ° ÿÆŸäÿ∑ ÿßŸÑŸÖÿ±ÿßŸÇÿ®ÿ©")
    
    while True:
        try:
            # ÿ™ŸÜÿ∏ŸäŸÅ ÿØŸàÿ±Ÿä ŸÑŸÑÿ∞ÿßŸÉÿ±ÿ©
            current_time = time.time()
            if current_time - last_memory_cleanup > MEMORY_CLEANUP_INTERVAL:
                logger.info("üßπ [Trade Monitor] ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ©...")
                gc.collect()
                last_memory_cleanup = current_time

            with signal_cache_lock:
                signals_to_check = dict(open_signals_cache)

            if not signals_to_check:
                time.sleep(1)
                continue
                
            if not redis_client or not client:
                reconnect_attempts += 1
                if reconnect_attempts > MAX_RECONNECT_ATTEMPTS:
                    logger.error("‚ùå [Trade Monitor] ŸÅÿ¥ŸÑ ÿßŸÑÿßÿ™ÿµÿßŸÑ ÿ®ÿßŸÑÿÆÿØŸÖÿßÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ© ÿ®ÿπÿØ ÿπÿØÿ© ŸÖÿ≠ÿßŸàŸÑÿßÿ™")
                    time.sleep(60)
                    reconnect_attempts = 0
                continue
                
            reconnect_attempts = 0  # ÿ•ÿπÿßÿØÿ© ÿ™ÿπŸäŸäŸÜ ÿßŸÑÿπÿØÿßÿØ ÿπŸÜÿØ ŸÜÿ¨ÿßÿ≠ ÿßŸÑÿßÿ™ÿµÿßŸÑ

            perform_direct_api_check = (time.time() - last_api_check_time) > DIRECT_API_CHECK_INTERVAL
            if perform_direct_api_check:
                logger.debug(f"üîÑ [Direct API Check] ÿ≠ÿßŸÜ ŸàŸÇÿ™ ÿßŸÑŸÅÿ≠ÿµ ÿßŸÑŸÖÿ®ÿßÿ¥ÿ± ŸÖŸÜ API (ŸÉŸÑ {DIRECT_API_CHECK_INTERVAL} ÿ´ÿßŸÜŸäÿ©).")
                last_api_check_time = time.time()

            symbols_to_fetch = list(signals_to_check.keys())
            redis_prices_list = redis_client.hmget(REDIS_PRICES_HASH_NAME, symbols_to_fetch)
            redis_prices = {symbol: price for symbol, price in zip(symbols_to_fetch, redis_prices_list)}

            for symbol, signal in signals_to_check.items():
                signal_id = signal.get('id')
                
                with closure_lock:
                    if signal_id in signals_pending_closure:
                        continue

                price = None
                price_source = "None"

                if perform_direct_api_check:
                    try:
                        ticker = client.get_symbol_ticker(symbol=symbol)
                        price = float(ticker['price'])
                        price_source = "Direct API"
                    except Exception as e:
                        logger.error(f"‚ùå [Direct API Check] ŸÅÿ¥ŸÑ ÿ¨ŸÑÿ® ÿßŸÑÿ≥ÿπÿ± ŸÑŸÄ {symbol}: {e}")
                        if redis_prices.get(symbol):
                            price = float(redis_prices[symbol])
                            price_source = "Redis (Fallback)"
                else:
                    if redis_prices.get(symbol):
                        price = float(redis_prices[symbol])
                        price_source = "Redis"

                target_price = float(signal.get('target_price', 0))
                stop_loss_price = float(signal.get('stop_loss', 0))

                logger.debug(f"[MONITOR] ID:{signal_id} | {symbol} | Price: {price} ({price_source}) | TP: {target_price} | SL: {stop_loss_price}")
                
                if not all([price, target_price > 0, stop_loss_price > 0]):
                    logger.warning(f"  -> [SKIP] ÿ®ŸäÿßŸÜÿßÿ™ ÿ∫Ÿäÿ± ÿµÿßŸÑÿ≠ÿ© ÿ£Ÿà ÿ≥ÿπÿ± ÿ∫Ÿäÿ± ŸÖÿ™ŸàŸÅÿ± ŸÑŸÄ {symbol} (ID: {signal_id}).")
                    continue
                
                status_to_set = None
                if price >= target_price:
                    status_to_set = 'target_hit'
                elif price <= stop_loss_price:
                    status_to_set = 'stop_loss_hit'

                if status_to_set:
                    logger.info(f"‚úÖ [TRIGGER] ID:{signal_id} | {symbol} | ÿ™ÿ≠ŸÇŸÇ ÿ¥ÿ±ÿ∑ '{status_to_set}'. ÿßŸÑÿ≥ÿπÿ± {price} ({price_source}) ÿ™ÿ¨ÿßŸàÿ≤ ÿßŸÑŸÖÿ≥ÿ™ŸàŸâ (TP: {target_price}, SL: {stop_loss_price}).")
                    initiate_signal_closure(symbol, signal, status_to_set, price)

            time.sleep(0.2)

        except Exception as e:
            logger.error(f"‚ùå [Trade Monitor] ÿÆÿ∑ÿ£ ŸÅŸä ÿ≠ŸÑŸÇÿ© ÿßŸÑŸÖÿ±ÿßŸÇÿ®ÿ©: {e}", exc_info=True)
            time.sleep(5)
        finally:
            gc.collect()
            def run_websocket_manager() -> None:
    logger.info("‚ÑπÔ∏è [WebSocket] ÿ®ÿØÿ° ŸÖÿØŸäÿ± WebSocket...")
    
    MAX_RECONNECT_ATTEMPTS = 5
    reconnect_delay = 10
    attempt = 0
    
    while attempt < MAX_RECONNECT_ATTEMPTS:
        try:
            twm = ThreadedWebsocketManager(api_key=API_KEY, api_secret=API_SECRET)
            twm.start()
            
            # ÿ™ÿ≥ÿ¨ŸäŸÑ Ÿàÿ∏ŸäŸÅÿ© ŸÖÿπÿßŸÑÿ¨ÿ© ÿ™ÿ≠ÿØŸäÿ´ÿßÿ™ ÿßŸÑÿ£ÿ≥ÿπÿßÿ±
            twm.start_miniticker_socket(callback=handle_price_update_message)
            
            logger.info("‚úÖ [WebSocket] ÿ™ŸÖ ÿßŸÑÿßÿ™ÿµÿßŸÑ ŸàÿßŸÑÿßÿ≥ÿ™ŸÖÿßÿπ ÿ•ŸÑŸâ 'All Market Mini Tickers' ÿ®ŸÜÿ¨ÿßÿ≠.")
            twm.join()
            break
            
        except Exception as e:
            attempt += 1
            logger.error(f"‚ùå [WebSocket] ŸÅÿ¥ŸÑ ÿßŸÑÿßÿ™ÿµÿßŸÑ (ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ© {attempt}/{MAX_RECONNECT_ATTEMPTS}): {e}")
            
            if attempt < MAX_RECONNECT_ATTEMPTS:
                time.sleep(reconnect_delay)
                reconnect_delay *= 2  # ÿ≤ŸäÿßÿØÿ© ŸàŸÇÿ™ ÿßŸÑÿßŸÜÿ™ÿ∏ÿßÿ± ÿ™ÿØÿ±Ÿäÿ¨ŸäÿßŸã
            else:
                logger.critical("‚ùå [WebSocket] ŸÅÿ¥ŸÑ ÿßŸÑÿßÿ™ÿµÿßŸÑ ÿ®ÿπÿØ ÿπÿØÿ© ŸÖÿ≠ÿßŸàŸÑÿßÿ™. ÿ•ŸäŸÇÿßŸÅ ÿßŸÑÿ®Ÿàÿ™.")
                os._exit(1)

class TradingStrategy:
    def __init__(self, symbol: str):
        self.symbol = symbol
        model_bundle = load_ml_model_bundle_from_folder(symbol)
        self.ml_model = model_bundle.get('model') if model_bundle else None
        self.scaler = model_bundle.get('scaler') if model_bundle else None
        self.feature_names = model_bundle.get('feature_names') if model_bundle else None

    def get_features(self, df_15m: pd.DataFrame, df_4h: pd.DataFrame, btc_df: pd.DataFrame, 
                    sr_levels_df: pd.DataFrame, ichimoku_df: pd.DataFrame) -> Optional[pd.DataFrame]:
        try:
            # ÿ•ŸÜÿ¥ÿßÿ° ŸÜÿ≥ÿÆ ŸÖÿ≠ŸÑŸäÿ© ŸÑÿ™ÿ¨ŸÜÿ® ÿ™ÿπÿØŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ£ÿµŸÑŸäÿ©
            df_featured = df_15m.copy()
            df_4h_local = df_4h.copy()
            
            # ÿ≠ÿ≥ÿßÿ® ÿßŸÑŸÖÿ§ÿ¥ÿ±ÿßÿ™
            df_featured = calculate_features(df_featured, btc_df)
            df_featured = calculate_sr_features(df_featured, sr_levels_df)
            
            if not ichimoku_df.empty:
                df_featured = df_featured.join(ichimoku_df, how='left')
                df_featured = calculate_ichimoku_based_features(df_featured)
                
            # ÿ•ÿ∂ÿßŸÅÿ© ŸÖÿ§ÿ¥ÿ±ÿßÿ™ ÿßŸÑÿ•ÿ∑ÿßÿ± ÿßŸÑÿ≤ŸÖŸÜŸä ÿßŸÑÿ£ÿπŸÑŸâ
            delta_4h = df_4h_local['close'].diff()
            gain_4h = delta_4h.clip(lower=0).ewm(com=RSI_PERIOD - 1, adjust=False).mean()
            loss_4h = -delta_4h.clip(upper=0).ewm(com=RSI_PERIOD - 1, adjust=False).mean()
            df_4h_local['rsi_4h'] = 100 - (100 / (1 + (gain_4h / loss_4h.replace(0, 1e-9))))
            
            ema_fast_4h = df_4h_local['close'].ewm(span=EMA_FAST_PERIOD, adjust=False).mean()
            df_4h_local['price_vs_ema50_4h'] = (df_4h_local['close'] / ema_fast_4h) - 1
            
            mtf_features = df_4h_local[['rsi_4h', 'price_vs_ema50_4h']]
            df_featured = df_featured.join(mtf_features)
            df_featured[['rsi_4h', 'price_vs_ema50_4h']] = df_featured[['rsi_4h', 'price_vs_ema50_4h']].fillna(method='ffill')
            
            # ÿßŸÑÿ™ÿ£ŸÉÿØ ŸÖŸÜ Ÿàÿ¨ŸàÿØ ÿ¨ŸÖŸäÿπ ÿßŸÑÿÆÿµÿßÿ¶ÿµ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©
            for col in self.feature_names:
                if col not in df_featured.columns:
                    df_featured[col] = 0.0
            
            # ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑŸÇŸäŸÖ ÿ∫Ÿäÿ± ÿßŸÑŸÖÿ≠ÿØŸàÿØÿ©
            df_featured.replace([np.inf, -np.inf], np.nan, inplace=True)
            
            return df_featured[self.feature_names].dropna()
            
        except Exception as e:
            logger.error(f"‚ùå [{self.symbol}] ŸÅÿ¥ŸÑ ŸáŸÜÿØÿ≥ÿ© ÿßŸÑŸÖŸäÿ≤ÿßÿ™: {e}", exc_info=True)
            return None
        finally:
            # ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ©
            del df_15m, df_4h, sr_levels_df, ichimoku_df
            gc.collect()

    def generate_signal(self, df_features: pd.DataFrame) -> Optional[Dict[str, Any]]:
        if not all([self.ml_model, self.scaler, self.feature_names]): 
            return None
            
        if df_features.empty:
            return None
            
        try:
            last_row_df = df_features.iloc[[-1]]
            features_scaled = self.scaler.transform(last_row_df)
            features_scaled_df = pd.DataFrame(features_scaled, columns=self.feature_names)
            
            prediction = self.ml_model.predict(features_scaled_df)[0]
            prediction_proba = self.ml_model.predict_proba(features_scaled_df)[0]
            
            try:
                class_1_index = list(self.ml_model.classes_).index(1)
            except ValueError:
                return None
                
            prob_for_class_1 = prediction_proba[class_1_index]
            
            if prediction == 1 and prob_for_class_1 >= MODEL_CONFIDENCE_THRESHOLD:
                logger.info(f"‚úÖ [ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ÿ•ÿ¥ÿßÿ±ÿ©] {self.symbol}: ÿ™ŸÜÿ®ÿ£ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ 'ÿ¥ÿ±ÿßÿ°' (1) ÿ®ÿ´ŸÇÿ© {prob_for_class_1:.2%}")
                return {
                    'symbol': self.symbol,
                    'strategy_name': BASE_ML_MODEL_NAME,
                    'signal_details': {
                        'ML_Probability_Buy': f"{prob_for_class_1:.2%}"
                    }
                }
            return None
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [ÿ™ŸàŸÑŸäÿØ ÿ•ÿ¥ÿßÿ±ÿ©] {self.symbol}: ÿÆÿ∑ÿ£ ÿ£ÿ´ŸÜÿßÿ° ÿßŸÑÿ™ŸàŸÑŸäÿØ: {e}", exc_info=True)
            return None
        finally:
            gc.collect()
            def main_loop():
    logger.info("[ÿßŸÑÿ≠ŸÑŸÇÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ©] ÿßŸÜÿ™ÿ∏ÿßÿ± ÿßŸÉÿ™ŸÖÿßŸÑ ÿßŸÑÿ™ŸáŸäÿ¶ÿ© ÿßŸÑÿ£ŸàŸÑŸäÿ©...")
    time.sleep(15)
    
    # ÿ•ÿ∂ÿßŸÅÿ© ŸÖÿ§ŸÇÿ™ ŸÑÿ™ÿ™ÿ®ÿπ ÿ¢ÿÆÿ± ŸÖÿ±ÿ© ÿ™ŸÖ ŸÅŸäŸáÿß ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ©
    last_memory_cleanup = time.time()
    MEMORY_CLEANUP_INTERVAL = 3600  # ÿ™ŸÜÿ∏ŸäŸÅ ŸÉŸÑ ÿ≥ÿßÿπÿ©
    
    if not validated_symbols_to_scan:
        log_and_notify("critical", "ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ±ŸÖŸàÿ≤ ŸÖÿπÿ™ŸÖÿØÿ© ŸÑŸÑŸÖÿ≥ÿ≠. ŸÑŸÜ Ÿäÿ≥ÿ™ŸÖÿ± ÿßŸÑÿ®Ÿàÿ™ ŸÅŸä ÿßŸÑÿπŸÖŸÑ.", "SYSTEM")
        return
        
    log_and_notify("info", f"ÿ®ÿØÿ° ÿ≠ŸÑŸÇÿ© ÿßŸÑŸÖÿ≥ÿ≠ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ŸÑŸÄ {len(validated_symbols_to_scan)} ÿπŸÖŸÑÿ©.", "SYSTEM")
    
    all_symbols = list(validated_symbols_to_scan)
    symbol_chunks = [all_symbols[i:i + MODEL_BATCH_SIZE] for i in range(0, len(all_symbols), MODEL_BATCH_SIZE)]
    strategies = {symbol: TradingStrategy(symbol) for symbol in all_symbols}

    while True:
        try:
            start_time = time.time()
            
            # ÿ™ŸÜÿ∏ŸäŸÅ ÿØŸàÿ±Ÿä ŸÑŸÑÿ∞ÿßŸÉÿ±ÿ©
            if start_time - last_memory_cleanup > MEMORY_CLEANUP_INTERVAL:
                logger.info("üßπ [ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ©] ÿ®ÿØÿ° ÿßŸÑÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿØŸàÿ±Ÿä...")
                ml_models_cache.clear()
                gc.collect()
                last_memory_cleanup = start_time
                logger.info("‚úÖ [ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ©] ÿßŸÉÿ™ŸÖŸÑ ÿßŸÑÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿØŸàÿ±Ÿä")

            for symbol_batch in symbol_chunks:
                batch_signals = []
                
                for symbol in symbol_batch:
                    try:
                        # ÿ¨ŸÑÿ® ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ™ÿßÿ±ŸäÿÆŸäÿ©
                        df_15m = fetch_historical_data(symbol, SIGNAL_GENERATION_TIMEFRAME, SIGNAL_GENERATION_LOOKBACK_DAYS)
                        if df_15m is None or df_15m.empty:
                            logger.warning(f"‚ö†Ô∏è [{symbol}] ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ®ŸäÿßŸÜÿßÿ™ 15 ÿØŸÇŸäŸÇÿ© ÿµÿßŸÑÿ≠ÿ©.")
                            continue
                            
                        df_4h = fetch_historical_data(symbol, HIGHER_TIMEFRAME, SIGNAL_GENERATION_LOOKBACK_DAYS)
                        if df_4h is None or df_4h.empty:
                            logger.warning(f"‚ö†Ô∏è [{symbol}] ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ®ŸäÿßŸÜÿßÿ™ 4 ÿ≥ÿßÿπÿßÿ™ ÿµÿßŸÑÿ≠ÿ©.")
                            continue

                        btc_df = fetch_historical_data(BTC_SYMBOL, SIGNAL_GENERATION_TIMEFRAME, SIGNAL_GENERATION_LOOKBACK_DAYS)
                        if btc_df is None or btc_df.empty:
                            logger.warning(f"‚ö†Ô∏è [{symbol}] ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ®ŸäÿßŸÜÿßÿ™ BTC ÿµÿßŸÑÿ≠ÿ©.")
                            continue
                            
                        btc_df['btc_returns'] = btc_df['close'].pct_change()

                        # ÿ¨ŸÑÿ® ŸÖÿ≥ÿ™ŸàŸäÿßÿ™ ÿßŸÑÿØÿπŸÖ ŸàÿßŸÑŸÖŸÇÿßŸàŸÖÿ©
                        sr_levels_df = fetch_sr_levels_from_db(symbol)
                        
                        # ÿ¨ŸÑÿ® ŸÖÿ§ÿ¥ÿ±ÿßÿ™ Ichimoku
                        ichimoku_df = fetch_ichimoku_features_from_db(symbol, SIGNAL_GENERATION_TIMEFRAME)

                        # ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© ÿßŸÑÿ™ÿØÿßŸàŸÑ
                        strategy = strategies.get(symbol)
                        if not strategy:
                            logger.warning(f"‚ö†Ô∏è [{symbol}] ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© ŸÖŸáŸäÿ£ÿ©.")
                            continue

                        # ÿ≠ÿ≥ÿßÿ® ÿßŸÑŸÖÿ§ÿ¥ÿ±ÿßÿ™ Ÿàÿ™ŸàŸÑŸäÿØ ÿßŸÑÿ•ÿ¥ÿßÿ±ÿ©
                        df_features = strategy.get_features(df_15m, df_4h, btc_df, sr_levels_df, ichimoku_df)
                        if df_features is None:
                            continue

                        signal = strategy.generate_signal(df_features)
                        if signal:
                            batch_signals.append(signal)

                    except Exception as e:
                        logger.error(f"‚ùå [{symbol}] ÿÆÿ∑ÿ£ ŸÅŸä ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿπŸÖŸÑÿ©: {e}", exc_info=True)
                    finally:
                        # ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© ÿ®ÿπÿØ ŸÉŸÑ ÿπŸÖŸÑÿ©
                        gc.collect()

                # ŸÖÿπÿßŸÑÿ¨ÿ© ÿ•ÿ¥ÿßÿ±ÿßÿ™ ÿßŸÑŸÖÿ¨ŸÖŸàÿπÿ©
                if batch_signals:
                    logger.info(f"üéØ [Batch] ÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ {len(batch_signals)} ÿ•ÿ¥ÿßÿ±ÿ© ŸÅŸä Ÿáÿ∞Ÿá ÿßŸÑŸÖÿ¨ŸÖŸàÿπÿ©.")
                    for signal in batch_signals:
                        try:
                            # ŸáŸÜÿß ŸäŸÖŸÉŸÜŸÉ ÿ•ÿ∂ÿßŸÅÿ© ŸÖŸÜÿ∑ŸÇ ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿ•ÿ¥ÿßÿ±ÿßÿ™
                            pass
                        except Exception as e:
                            logger.error(f"‚ùå [ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿ•ÿ¥ÿßÿ±ÿ©] ÿÆÿ∑ÿ£ ŸÅŸä ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿ•ÿ¥ÿßÿ±ÿ©: {e}", exc_info=True)

            # ÿ≠ÿ≥ÿßÿ® ŸàŸÇÿ™ ÿßŸÑŸÜŸàŸÖ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®
            execution_time = time.time() - start_time
            sleep_time = max(1, 60 - execution_time)  # ÿπŸÑŸâ ÿßŸÑÿ£ŸÇŸÑ ÿ´ÿßŸÜŸäÿ© Ÿàÿßÿ≠ÿØÿ©
            logger.debug(f"üí§ [ÿßŸÑÿ≠ŸÑŸÇÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ©] ÿßŸÉÿ™ŸÖŸÑ ÿßŸÑŸÖÿ≥ÿ≠ ŸÅŸä {execution_time:.2f} ÿ´ÿßŸÜŸäÿ©. ÿßŸÑŸÜŸàŸÖ ŸÑŸÖÿØÿ© {sleep_time:.2f} ÿ´ÿßŸÜŸäÿ©.")
            time.sleep(sleep_time)

        except Exception as e:
            logger.error(f"‚ùå [ÿßŸÑÿ≠ŸÑŸÇÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ©] ÿÆÿ∑ÿ£ ÿ∫Ÿäÿ± ŸÖÿ™ŸàŸÇÿπ: {e}", exc_info=True)
            time.sleep(60)
        finally:
            gc.collect()
            # --- ÿ™ŸáŸäÿ¶ÿ© Flask Ÿàÿ™ÿπÿ±ŸäŸÅ ÿßŸÑŸÖÿ≥ÿßÿ±ÿßÿ™ ---
app = Flask(__name__)
CORS(app)

@app.route('/health')
def health_check():
    try:
        checks = {
            'redis': bool(redis_client and redis_client.ping()),
            'database': check_db_connection(),
            'binance': bool(client and client.ping()),
            'symbols_loaded': len(validated_symbols_to_scan) > 0,
            'models_loaded': len(ml_models_cache) > 0,
            'memory_usage': f"{psutil.Process().memory_info().rss / 1024 / 1024:.1f}MB"
        }
        
        status = 'healthy' if all(v for k, v in checks.items() if k != 'memory_usage') else 'degraded'
        return jsonify({
            'status': status,
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'checks': checks
        }), 200 if status == 'healthy' else 503
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now(timezone.utc).isoformat()
        }), 500

@app.route('/notifications', methods=['GET'])
def get_notifications():
    try:
        limit = min(int(request.args.get('limit', 50)), 100)
        with notifications_lock:
            recent_notifications = list(notifications_cache)[:limit]
        return jsonify(recent_notifications)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/signals', methods=['GET'])
def get_signals():
    try:
        with signal_cache_lock:
            signals = list(open_signals_cache.values())
        return jsonify(signals)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/metrics')
def metrics():
    try:
        process = psutil.Process()
        memory_info = process.memory_info()
        
        metrics_data = {
            'system': {
                'cpu_percent': psutil.cpu_percent(),
                'memory_usage_mb': memory_info.rss / 1024 / 1024,
                'memory_percent': process.memory_percent(),
                'threads_count': process.num_threads(),
                'open_files': len(process.open_files()),
                'uptime_seconds': time.time() - process.create_time()
            },
            'application': {
                'open_signals_count': len(open_signals_cache),
                'cached_models_count': len(ml_models_cache),
                'monitored_symbols': len(validated_symbols_to_scan),
                'notifications_cached': len(notifications_cache)
            },
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
        
        return jsonify(metrics_data)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

def init_services():
    """ÿ™ŸáŸäÿ¶ÿ© ÿ¨ŸÖŸäÿπ ÿßŸÑÿÆÿØŸÖÿßÿ™ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©"""
    try:
        global client, validated_symbols_to_scan
        
        # ÿ™ŸáŸäÿ¶ÿ© ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™
        init_db()
        
        # ÿ™ŸáŸäÿ¶ÿ© Redis
        init_redis()
        
        # ÿ™ŸáŸäÿ¶ÿ© ÿπŸÖŸäŸÑ Binance
        client = Client(API_KEY, API_SECRET)
        
        # ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑÿ±ŸÖŸàÿ≤ ÿßŸÑŸÖÿπÿ™ŸÖÿØÿ©
        validated_symbols_to_scan = get_validated_symbols()
        
        # ÿßÿ≥ÿ™ÿ±ÿØÿßÿØ ÿ≠ÿßŸÑÿ© ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© ÿßŸÑŸÖÿ§ŸÇÿ™ÿ©
        recover_cache_state()
        
        return True
        
    except Exception as e:
        logger.critical(f"‚ùå [ÿ™ŸáŸäÿ¶ÿ©] ŸÅÿ¥ŸÑ ŸÅŸä ÿ™ŸáŸäÿ¶ÿ© ÿßŸÑÿÆÿØŸÖÿßÿ™: {e}", exc_info=True)
        return False

def start_background_tasks():
    """ÿ®ÿØÿ° ÿßŸÑŸÖŸáÿßŸÖ ÿßŸÑÿÆŸÑŸÅŸäÿ©"""
    try:
        # ÿ®ÿØÿ° ÿÆŸäÿ∑ ŸÖÿ±ÿßŸÇÿ®ÿ© ÿßŸÑÿ™ÿØÿßŸàŸÑ
        Thread(target=trade_monitoring_loop, daemon=True, name="TradeMonitor").start()
        
        # ÿ®ÿØÿ° ŸÖÿØŸäÿ± WebSocket
        Thread(target=run_websocket_manager, daemon=True, name="WebSocketManager").start()
        
        # ÿ®ÿØÿ° ÿßŸÑÿ≠ŸÑŸÇÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ©
        Thread(target=main_loop, daemon=True, name="MainLoop").start()
        
        return True
        
    except Exception as e:
        logger.critical(f"‚ùå [ÿßŸÑŸÖŸáÿßŸÖ ÿßŸÑÿÆŸÑŸÅŸäÿ©] ŸÅÿ¥ŸÑ ŸÅŸä ÿ®ÿØÿ° ÿßŸÑŸÖŸáÿßŸÖ ÿßŸÑÿÆŸÑŸÅŸäÿ©: {e}", exc_info=True)
        return False

if __name__ == '__main__':
    try:
        # ÿ™ŸáŸäÿ¶ÿ© ÿßŸÑÿÆÿØŸÖÿßÿ™
        if not init_services():
            logger.critical("‚ùå ŸÅÿ¥ŸÑ ŸÅŸä ÿ™ŸáŸäÿ¶ÿ© ÿßŸÑÿÆÿØŸÖÿßÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©. ÿ•ŸäŸÇÿßŸÅ ÿßŸÑÿ®Ÿàÿ™.")
            sys.exit(1)
            
        # ÿ®ÿØÿ° ÿßŸÑŸÖŸáÿßŸÖ ÿßŸÑÿÆŸÑŸÅŸäÿ©
        if not start_background_tasks():
            logger.critical("‚ùå ŸÅÿ¥ŸÑ ŸÅŸä ÿ®ÿØÿ° ÿßŸÑŸÖŸáÿßŸÖ ÿßŸÑÿÆŸÑŸÅŸäÿ©. ÿ•ŸäŸÇÿßŸÅ ÿßŸÑÿ®Ÿàÿ™.")
            sys.exit(1)
            
        # ÿ™ÿ¥ÿ∫ŸäŸÑ ÿÆÿßÿØŸÖ Flask
        port = int(os.environ.get('PORT', 5000))
        app.run(host='0.0.0.0', port=port, debug=False)
        
    except Exception as e:
        logger.critical(f"‚ùå [ÿßŸÑÿ™ÿ¥ÿ∫ŸäŸÑ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿä] ÿÆÿ∑ÿ£ ÿ≠ÿ±ÿ¨: {e}", exc_info=True)
        sys.exit(1)